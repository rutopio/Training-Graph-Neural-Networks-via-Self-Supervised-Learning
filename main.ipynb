{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Graph Neural Networks via Self-Supervised Learning: Experiments and Analysis\n",
    "\n",
    "Latest Update: June 6th, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HvkSZad2C_x6"
   },
   "source": [
    "## Install Pytorch Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Cgqq1b3iuDYX"
   },
   "outputs": [],
   "source": [
    "# !pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
    "# !pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
    "# !pip install -q torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TMTCi-o3R4lp"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import networkx as nx\n",
    "import shutil\n",
    "from itertools import repeat\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pagJZef_C-xT",
    "outputId": "d68493b1-22f8-4c7e-893f-ae7f3797434d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Version: 1.9.1+cu111\n",
      "CUDA Available? True\n",
      "CUDA Version: 11.1\n",
      "+-----------------------------------------------------------------------------+\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2021 NVIDIA Corporation\n",
      "Built on Sun_Feb_14_21:12:58_PST_2021\n",
      "Cuda compilation tools, release 11.2, V11.2.152\n",
      "Build cuda_11.2.r11.2/compiler.29618528_0\n",
      "+-----------------------------------------------------------------------------+\n",
      "Fri Dec 17 15:30:41 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.74       Driver Version: 470.74       CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:53:00.0 Off |                  N/A |\n",
      "| 30%   54C    P0   112W / 320W |      2MiB / 10015MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:8D:00.0 Off |                  N/A |\n",
      "| 32%   57C    P2    93W / 320W |   6279MiB / 10018MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "print(\"Pytorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available?\", torch.cuda.is_available())\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"+-----------------------------------------------------------------------------+\")\n",
    "!nvcc --version\n",
    "print(\"+-----------------------------------------------------------------------------+\")\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oojjg6wicGKg",
    "outputId": "09e7098e-54fd-4791-eede-ee9aa35342d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Datasets/\n"
     ]
    }
   ],
   "source": [
    "path = \"./Datasets/\"\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = \"MUTAG\"\n",
    "# dataset_name = \"PROTEINS\"\n",
    "# dataset_name = \"NCI1\"\n",
    "\n",
    "dataset_name = \"DD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWdyHUHUBXPV",
    "outputId": "0f7f0e75-993e-46d3-e52f-e1d957248da5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Logs09/logs_DD_20211217-153041/\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "dt = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_path = \"./Logs09/logs_\"+dataset_name+\"_\"+dt+\"/\"\n",
    "os.mkdir(log_path)\n",
    "print(log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqNWxHvbT1-y"
   },
   "source": [
    "## Seed & Plot Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5NIlOSvZLEvz"
   },
   "outputs": [],
   "source": [
    "def my_seed(seed):\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "my_seed(46)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Yf50L4NrTRm3"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.utils.convert import to_networkx\n",
    "\n",
    "def draw_graph(pygeo_graph):\n",
    "    G = to_networkx(pygeo_graph)\n",
    "    pos = nx.circular_layout(G)\n",
    "    nx.draw(G, pos, with_labels=True, node_color=\"pink\", node_size=200)\n",
    "    plt.show()\n",
    "\n",
    "def plot_data(path, data, title_str, save_file=False):\n",
    "    now = datetime.now()\n",
    "    dt = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, len(data)+1), data)\n",
    "    plt.title(title_str) # title\n",
    "    plt.xlabel(\"Epochs\") # x label\n",
    "    plt.xticks(range(0, len(data)+1, int(len(data)/10)))\n",
    "    if save_file == True:\n",
    "      plt.savefig(path+dt+\"_\"+title_str+\".png\")\n",
    "      print(\"Save!\")\n",
    "    plt.show()\n",
    "\n",
    "def record__to_txt(path, data, filename):\n",
    "    now = datetime.now()\n",
    "    dt = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    with open(path+dt+\"_\"+filename+\".txt\", \"w\") as f:\n",
    "        for item in data:\n",
    "            f.write(\"%s\\n\" % item)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "tz = timezone(timedelta(hours=+8))\n",
    "now = datetime.now(tz)\n",
    "dt = now.strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLbK3nQaU6LP"
   },
   "source": [
    "## Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4qiJ9tsPsgp"
   },
   "source": [
    "### TUDataset\n",
    "\n",
    "- TU Datasets: https://chrsmrrs.github.io/datasets/docs/datasets/\n",
    "- Ref: https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/datasets/tu_dataset.html\n",
    "- Open Graph Benchmark: https://ogb.stanford.edu/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                           Version\n",
      "--------------------------------- ---------------\n",
      "absl-py                           0.13.0\n",
      "argon2-cffi                       20.1.0\n",
      "asn1crypto                        0.24.0\n",
      "astunparse                        1.6.3\n",
      "async-generator                   1.10\n",
      "attrs                             21.2.0\n",
      "backcall                          0.2.0\n",
      "backports.entry-points-selectable 1.1.0\n",
      "bleach                            4.0.0\n",
      "cached-property                   1.5.2\n",
      "cachetools                        4.2.2\n",
      "certifi                           2021.5.30\n",
      "cffi                              1.14.6\n",
      "charset-normalizer                2.0.4\n",
      "clang                             5.0\n",
      "cryptography                      2.1.4\n",
      "cycler                            0.10.0\n",
      "dataclasses                       0.8\n",
      "decorator                         4.4.2\n",
      "defusedxml                        0.7.1\n",
      "distlib                           0.3.3\n",
      "entrypoints                       0.3\n",
      "filelock                          3.3.1\n",
      "flatbuffers                       1.12\n",
      "gast                              0.4.0\n",
      "google-auth                       1.34.0\n",
      "google-auth-oauthlib              0.4.5\n",
      "google-pasta                      0.2.0\n",
      "googledrivedownloader             0.4\n",
      "grpcio                            1.39.0\n",
      "h5py                              3.1.0\n",
      "idna                              2.6\n",
      "importlib-metadata                4.6.3\n",
      "importlib-resources               5.2.3\n",
      "ipykernel                         5.1.1\n",
      "ipython                           7.16.1\n",
      "ipython-genutils                  0.2.0\n",
      "ipywidgets                        7.6.3\n",
      "isodate                           0.6.0\n",
      "jedi                              0.18.0\n",
      "Jinja2                            3.0.1\n",
      "joblib                            1.1.0\n",
      "jsonschema                        3.2.0\n",
      "jupyter                           1.0.0\n",
      "jupyter-client                    6.1.12\n",
      "jupyter-console                   6.4.0\n",
      "jupyter-core                      4.7.1\n",
      "jupyter-http-over-ws              0.0.8\n",
      "jupyterlab-pygments               0.1.2\n",
      "jupyterlab-widgets                1.0.0\n",
      "keras                             2.6.0\n",
      "Keras-Preprocessing               1.1.2\n",
      "keyring                           10.6.0\n",
      "keyrings.alt                      3.0\n",
      "kiwisolver                        1.3.1\n",
      "Markdown                          3.3.4\n",
      "MarkupSafe                        2.0.1\n",
      "matplotlib                        3.3.4\n",
      "mistune                           0.8.4\n",
      "nbclient                          0.5.3\n",
      "nbconvert                         6.0.7\n",
      "nbformat                          4.4.0\n",
      "nest-asyncio                      1.5.1\n",
      "networkx                          2.5.1\n",
      "notebook                          6.4.3\n",
      "numpy                             1.19.5\n",
      "oauthlib                          3.1.1\n",
      "opt-einsum                        3.3.0\n",
      "packaging                         21.0\n",
      "pandas                            1.1.5\n",
      "pandocfilters                     1.4.3\n",
      "parso                             0.8.2\n",
      "pexpect                           4.8.0\n",
      "pickleshare                       0.7.5\n",
      "Pillow                            8.3.1\n",
      "pip                               21.3\n",
      "platformdirs                      2.4.0\n",
      "prometheus-client                 0.11.0\n",
      "prompt-toolkit                    3.0.19\n",
      "protobuf                          3.17.3\n",
      "ptyprocess                        0.7.0\n",
      "pyasn1                            0.4.8\n",
      "pyasn1-modules                    0.2.8\n",
      "pycparser                         2.20\n",
      "pycrypto                          2.6.1\n",
      "Pygments                          2.9.0\n",
      "PyGObject                         3.26.1\n",
      "pyparsing                         2.4.7\n",
      "pyrsistent                        0.18.0\n",
      "python-apt                        1.6.5+ubuntu0.6\n",
      "python-dateutil                   2.8.2\n",
      "pytz                              2021.3\n",
      "pyxdg                             0.25\n",
      "PyYAML                            6.0\n",
      "pyzmq                             22.2.1\n",
      "qtconsole                         5.1.1\n",
      "QtPy                              1.9.0\n",
      "rdflib                            5.0.0\n",
      "requests                          2.26.0\n",
      "requests-oauthlib                 1.3.0\n",
      "rsa                               4.7.2\n",
      "scikit-learn                      0.24.2\n",
      "scipy                             1.5.4\n",
      "SecretStorage                     2.3.1\n",
      "Send2Trash                        1.8.0\n",
      "setuptools                        57.4.0\n",
      "six                               1.15.0\n",
      "sklearn                           0.0\n",
      "tensorboard                       2.6.0\n",
      "tensorboard-data-server           0.6.1\n",
      "tensorboard-plugin-wit            1.8.0\n",
      "tensorflow                        2.6.0\n",
      "tensorflow-estimator              2.6.0\n",
      "termcolor                         1.1.0\n",
      "terminado                         0.10.1\n",
      "testpath                          0.5.0\n",
      "threadpoolctl                     3.0.0\n",
      "torch                             1.9.1+cu111\n",
      "torch-cluster                     1.5.9\n",
      "torch-geometric                   2.0.1\n",
      "torch-scatter                     2.0.8\n",
      "torch-sparse                      0.6.12\n",
      "torch-spline-conv                 1.2.1\n",
      "torchaudio                        0.9.1\n",
      "torchvision                       0.10.1+cu111\n",
      "tornado                           6.1\n",
      "tqdm                              4.62.3\n",
      "traitlets                         4.3.3\n",
      "typing-extensions                 3.7.4.3\n",
      "urllib3                           1.26.6\n",
      "virtualenv                        20.8.1\n",
      "wcwidth                           0.2.5\n",
      "webencodings                      0.5.1\n",
      "Werkzeug                          2.0.1\n",
      "wheel                             0.37.0\n",
      "widgetsnbextension                3.5.1\n",
      "wrapt                             1.12.1\n",
      "yacs                              0.1.8\n",
      "zipp                              3.5.0\n",
      "\u001b[33mWARNING: You are using pip version 21.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "diyLDgn4LHCU"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import InMemoryDataset, download_url, extract_zip, DataLoader, Data\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.io import read_tu_data\n",
    "\n",
    "from typing import Optional, Callable, List\n",
    "import copy\n",
    "\n",
    "from torch_geometric.data.collate import collate\n",
    "from torch_geometric.data.separate import separate\n",
    "from torch_geometric.data.dataset import Dataset, IndexType\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "class TUDataset_AUG(InMemoryDataset):\n",
    "    ## Ref: https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/datasets/tu_dataset.html\n",
    "    ## Ref: https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/data/in_memory_dataset.html\n",
    "\n",
    "    url = 'https://www.chrsmrrs.com/graphkerneldatasets'\n",
    "    cleaned_url = ('https://raw.githubusercontent.com/nd7141/'\n",
    "                    'graph_datasets/master/datasets')\n",
    "\n",
    "    def __init__(self, root, name, transform=None, pre_transform=None,\n",
    "            pre_filter=None, use_node_attr=False, use_edge_attr=False,\n",
    "            cleaned=False, aug=None, proportion=[0.2, 0.2]):\n",
    "        self.name = name\n",
    "        self.cleaned = cleaned\n",
    "        super(TUDataset_AUG, self).__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        self.aug = aug\n",
    "        self.proportion = proportion\n",
    "\n",
    "        if self.data.x is not None and not use_node_attr:\n",
    "            num_node_attributes = self.num_node_attributes\n",
    "            self.data.x = self.data.x[:, num_node_attributes:]\n",
    "        if self.data.edge_attr is not None and not use_edge_attr:\n",
    "            num_edge_attributes = self.num_edge_attributes\n",
    "            self.data.edge_attr = self.data.edge_attr[:, num_edge_attributes:]\n",
    "\n",
    "        # for those dataset does not have node attribute\n",
    "        if not (self.name == \"MUTAG\" or self.name == \"PTC_MR\" or self.name == \"DD\" or self.name == \"PROTEINS\" or self.name == \"NCI1\" or self.name == \"NCI109\"):\n",
    "            print(\"No Node Feature...處理中...\")\n",
    "            edge_index = self.data.edge_index[0, :].numpy()\n",
    "            num_edge = self.data.edge_index.size()[1]\n",
    "            nlist = [edge_index[n] + 1 for n in range(num_edge - 1) if edge_index[n] > edge_index[n + 1]]\n",
    "            nlist.append(edge_index[-1] + 1)\n",
    "\n",
    "            num_node = np.array(nlist).sum()          \n",
    "            # if the node does note have attribute, it can fill with 0 or 1.\n",
    "            # self.data.x = torch.zeros((num_node, 1))\n",
    "            self.data.x = torch.ones((num_node, 1))\n",
    "\n",
    "            edge_slice = [0]\n",
    "            k = 0\n",
    "            for n in nlist:\n",
    "                k = k + n\n",
    "                edge_slice.append(k)\n",
    "            self.slices[\"x\"] = torch.tensor(edge_slice)\n",
    "\n",
    "    @property\n",
    "    def raw_dir(self) -> str:\n",
    "        name = f'raw{\"_cleaned\" if self.cleaned else \"\"}'\n",
    "        return osp.join(self.root, self.name, name)\n",
    "\n",
    "    @property\n",
    "    def processed_dir(self) -> str:\n",
    "        name = f'processed{\"_cleaned\" if self.cleaned else \"\"}'\n",
    "        return osp.join(self.root, self.name, name)\n",
    "\n",
    "    @property\n",
    "    def num_node_labels(self):\n",
    "        if self.data.x is None:\n",
    "            return 0\n",
    "        for i in range(self.data.x.size(1)):\n",
    "            if self.data.x[:, i:].sum().item() == self.data.x.size(0):\n",
    "                return self.data.x.size(1) - i\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def num_node_attributes(self):\n",
    "        if self.data.x is None:\n",
    "            return 0\n",
    "        return self.data.x.size(1) - self.num_node_labels\n",
    "\n",
    "    @property\n",
    "    def num_edge_labels(self) -> int:\n",
    "        if self.data.edge_attr is None:\n",
    "            return 0\n",
    "        for i in range(self.data.edge_attr.size(1)):\n",
    "            if self.data.edge_attr[:, i:].sum() == self.data.edge_attr.size(0):\n",
    "                return self.data.edge_attr.size(1) - i\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def num_edge_attributes(self) -> int:\n",
    "        if self.data.edge_attr is None:\n",
    "            return 0\n",
    "        return self.data.edge_attr.size(1) - self.num_edge_labels\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        names = ['A', 'graph_indicator']\n",
    "        return ['{}_{}.txt'.format(self.name, name) for name in names]\n",
    "\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return 'data.pt'\n",
    "\n",
    "    @property\n",
    "    def num_node_features(self):\n",
    "        r\"\"\"Returns the number of features per node in the dataset.\"\"\"\n",
    "        return self[0][0].num_node_features\n",
    "\n",
    "    def download(self):\n",
    "        url = self.cleaned_url if self.cleaned else self.url\n",
    "        folder = osp.join(self.root, self.name)\n",
    "        path = download_url(f'{url}/{self.name}.zip', folder)\n",
    "        extract_zip(path, folder)\n",
    "        os.unlink(path)\n",
    "        shutil.rmtree(self.raw_dir)\n",
    "        os.rename(osp.join(folder, self.name), self.raw_dir)\n",
    "\n",
    "    def process(self):\n",
    "        self.data, self.slices = read_tu_data(self.raw_dir, self.name)\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [self.get(idx) for idx in range(len(self))]\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "            self.data, self.slices = self.collate(data_list)\n",
    "\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.get(idx) for idx in range(len(self))]\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "            self.data, self.slices = self.collate(data_list)\n",
    "\n",
    "        torch.save((self.data, self.slices), self.processed_paths[0])\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.name}({len(self)})'\n",
    "\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = self.data.__class__()\n",
    "\n",
    "\n",
    "        if hasattr(self.data, '__num_nodes__'):\n",
    "            data.num_nodes = self.data.__num_nodes__[idx]\n",
    "\n",
    "        for key in self.data.keys:\n",
    "            item, slices = self.data[key], self.slices[key]\n",
    "            if torch.is_tensor(item):\n",
    "                s = list(repeat(slice(None), item.dim()))\n",
    "                s[self.data.__cat_dim__(key, item)] = slice(slices[idx], slices[idx + 1])\n",
    "            else:\n",
    "                s = slice(slices[idx], slices[idx + 1])\n",
    "            data[key] = item[s]\n",
    "\n",
    "\n",
    "\n",
    "        node_num = data.edge_index.max()\n",
    "        sl = torch.tensor([[n,n] for n in range(node_num)]).transpose(0, 1)\n",
    "        data.edge_index = torch.cat((data.edge_index, sl), dim=1)\n",
    "\n",
    "        # print(self.proportion)\n",
    "        # print(self.aug)\n",
    "        \n",
    "        prop1 = self.proportion[0]\n",
    "        prop2 = self.proportion[1]\n",
    "\n",
    "        aug1 = self.aug[0]\n",
    "        aug2 = self.aug[1]\n",
    "\n",
    "        ## AUGMENTATION ##\n",
    "        if aug1 == \"node_dropping\":\n",
    "            data_aug1 = Augmantations.node_dropping(deepcopy(data), prop1)\n",
    "        elif aug1 == \"edge_perturbation\":\n",
    "            data_aug1 = Augmantations.edge_perturbation(deepcopy(data), prop1)\n",
    "        elif aug1 == \"subgraph\":\n",
    "            data_aug1 = Augmantations.subgraph(deepcopy(data), prop1)\n",
    "        elif aug1 == \"attribute_masking\":\n",
    "            data_aug1 = Augmantations.attribute_masking(deepcopy(data), prop1)\n",
    "\n",
    "        ## DO NOTHING ##\n",
    "        elif aug1 == \"raw\":\n",
    "            data_aug1 = deepcopy(data)\n",
    "\n",
    "        else:\n",
    "            print(\"Augmentation Error!\")\n",
    "            assert False\n",
    "\n",
    "        ## AUGMENTATION ##\n",
    "        if aug2 == \"node_dropping\":\n",
    "            data_aug2 = Augmantations.node_dropping(deepcopy(data), prop2)\n",
    "        elif aug2 == \"edge_perturbation\":\n",
    "            data_aug2 = Augmantations.edge_perturbation(deepcopy(data), prop2)\n",
    "        elif aug2 == \"subgraph\":\n",
    "            data_aug2 = Augmantations.subgraph(deepcopy(data), prop2)\n",
    "        elif aug2 == \"attribute_masking\":\n",
    "            data_aug2 = Augmantations.attribute_masking(deepcopy(data), prop2)\n",
    "\n",
    "        ## DO NOTHING ##\n",
    "        elif aug2 == \"raw\":\n",
    "            data_aug2 = deepcopy(data)\n",
    "\n",
    "        else:\n",
    "            print(\"Augmentation Error!\")\n",
    "            assert False\n",
    "\n",
    "        return data_aug1, data_aug2\n",
    "        # return: (original raw data, augmentation data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3N8kCAVbrIZH"
   },
   "source": [
    "## Augmentation\n",
    "\n",
    "- Ref: You, Y., Chen, T., Sui, Y., Chen, T., Wang, Z., & Shen, Y. (2020). Graph contrastive learning with augmentations. Advances in Neural Information Processing Systems, 33, 5812-5823.\n",
    "- https://arxiv.org/pdf/2010.13902.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kZOXu6Xc6TA"
   },
   "source": [
    "### Node dropping \n",
    "\n",
    "Given the graph $\\mathcal{G}$, node dropping will randomly discard certain portion of vertices along with their connections. The underlying prior enforced by it is that missing part of vertices does not affect the semantic meaning of $\\mathcal{G}$. Each node’s dropping probability follows a default i.i.d.\n",
    "uniform distribution (or any other distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odzpR1gidftk"
   },
   "source": [
    "### Edge perturbation\n",
    "\n",
    "It will perturb the connectivities in $\\mathcal{G}$ through randomly adding or dropping certain ratio of edges. It implies that the semantic meaning of $\\mathcal{G}$ has certain robustness to the edge connectivity pattern variances. We also follow an i.i.d. uniform distribution to add/drop each edge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SogIFdfAd6DC"
   },
   "source": [
    "### Subgraph\n",
    "\n",
    "This one samples a subgraph from $\\mathcal{G}$ using random walk (the algorithm is summarized in Appendix A). It assumes that the semantics of $\\mathcal{G}$ can be much preserved in its (partial) local structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTNUrn6ReDFS"
   },
   "source": [
    "### Attribute masking\n",
    "\n",
    "Attribute masking prompts models to recover masked vertex attributes using their context information, i.e., the remaining attributes. The underlying assumption is that missing partial vertex attributes does not affect the model predictions much.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "iwbEe6UvSn3X"
   },
   "outputs": [],
   "source": [
    "class Augmantations():\n",
    "    def __init__(self, data, proportion=0.2):\n",
    "        self.data = data\n",
    "        self.proportion = proportion\n",
    "\n",
    "    def node_dropping(data, proportion=0.2):\n",
    "        node_num = data.x.size()[0]\n",
    "        edge_num = data.edge_index.size()[1]\n",
    "        drop_num = int(node_num * proportion) # how much to drop\n",
    "        # print(node_num, edge_num, drop_num)\n",
    "        idx_drop = np.random.choice(node_num, drop_num, replace=False) # dropped node\n",
    "        edge_index = data.edge_index.numpy()\n",
    "        adj = torch.zeros((node_num, node_num)) # adjacency matrix\n",
    "        adj[edge_index[0], edge_index[1]] = 1\n",
    "        adj[idx_drop, :] = adj[:, idx_drop] = 0 # dropped node = no edge link\n",
    "        edge_index = adj.nonzero().transpose(0, 1) \n",
    "        data.edge_index = edge_index\n",
    "        return data\n",
    "\n",
    "    def edge_perturbation(data, proportion=0.2):\n",
    "        node_num = data.x.size()[0]\n",
    "        edge_num = data.edge_index.size()[1]\n",
    "        drop_num = int(edge_num * proportion) # how much to perturbation\n",
    "        # print(node_num, edge_num, drop_num)\n",
    "        edge_index = data.edge_index.transpose(0, 1).numpy() # edge list, [start, end]\n",
    "        edge_index = edge_index[np.random.choice(edge_num, edge_num-drop_num, replace=False)]\n",
    "        data.edge_index = torch.tensor(edge_index).transpose(0, 1)\n",
    "        return data \n",
    "\n",
    "    def subgraph(data, proportion=0.2):\n",
    "        node_num = data.x.size()[0]\n",
    "        edge_num = data.edge_index.size()[1]\n",
    "        sub_num = int(node_num * proportion) \n",
    "        # print(node_num, edge_num, sub_num)\n",
    "        edge_index = data.edge_index.numpy()\n",
    "        idx_sub = [np.random.randint(node_num, size=1)[0]]\n",
    "        idx_neighbor = set([n for n in edge_index[1][edge_index[0]==idx_sub[0]]])\n",
    "        # print(idx_sub, idx_neighbor)\n",
    "        count = 0\n",
    "        while len(idx_sub) <= sub_num:\n",
    "            count = count + 1\n",
    "            if count > node_num or len(idx_neighbor) == 0:\n",
    "                break # neighborhoods\n",
    "            sample_node = np.random.choice(list(idx_neighbor))\n",
    "            if sample_node in idx_sub:\n",
    "                continue\n",
    "            idx_sub.append(sample_node)\n",
    "            idx_neighbor.union(set([n for n in edge_index[1][edge_index[0]==idx_sub[-1]]]))\n",
    "        idx_drop = [n for n in range(node_num) if not n in idx_sub] # do not need\n",
    "        idx_nondrop = idx_sub # keep in subgraph\n",
    "        idx_dict = {idx_nondrop[n]:n for n in list(range(len(idx_nondrop)))}\n",
    "        # print(idx_drop, idx_nondrop, idx_dict)\n",
    "        edge_index = data.edge_index.numpy()\n",
    "        adj = torch.zeros((node_num, node_num)) # adjacency matrix\n",
    "        adj[edge_index[0], edge_index[1]] = 1\n",
    "        adj[idx_drop, :] = adj[:, idx_drop] = 0 # dropped node = no edge link\n",
    "        edge_index = adj.nonzero().transpose(0, 1) \n",
    "        data.edge_index = edge_index\n",
    "        return data\n",
    "\n",
    "\n",
    "    def attribute_masking(data, proportion=0.2):\n",
    "        node_num, feat_dim = data.x.size()\n",
    "        mask_num = int(node_num * proportion)\n",
    "        # print(node_num, feat_dim, mask_num)\n",
    "        idx_mask = np.random.choice(node_num, mask_num, replace=False)\n",
    "        data.x[idx_mask] = torch.tensor(np.random.normal(loc=0.5, scale=0.5, size=(mask_num, feat_dim)), dtype=torch.float32)\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VoelSjioZZjl"
   },
   "source": [
    "## Models\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Gd7J2IIqLArG"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T \n",
    "from torch_geometric.nn import GINConv, global_add_pool\n",
    "\n",
    "def D(p, z): \n",
    "    # negative cosine similarity\n",
    "    # z = z.detach() # stop gradient\n",
    "    # p = F.normalize(p, dim=1) # l2-normalize \n",
    "    # z = F.normalize(z, dim=1) # l2-normalize \n",
    "    # return -(p*z).sum(dim=1).mean()\n",
    "    return -F.cosine_similarity(p, z.detach(), dim=-1).mean()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTo6rMb_w4b1"
   },
   "source": [
    "### SimCLR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "-2vABJnea0dm"
   },
   "outputs": [],
   "source": [
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, hidden_dim, enc_layer, proj_layer, dataset_num_features):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = hidden_dim * enc_layer\n",
    "        self.dataset_num_features = dataset_num_features\n",
    "        self.encoder = Encoder(dataset_num_features, hidden_dim, enc_layer)\n",
    "        # self.proj_layer = proj_layer\n",
    "        ## Projection Head ##           \n",
    "        self.projector = Projection_MLP(self.embedding_dim, self.embedding_dim, self.embedding_dim, proj_layer)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, num_graphs):\n",
    "        y = self.encoder(x, edge_index, batch)        \n",
    "        y = self.projector(y)                        \n",
    "        return y        \n",
    "\n",
    "    def loss(self, x_raw, x_aug):\n",
    "        # NT-Xent Loss\n",
    "        T = 0.2 # temperature setting (tau)\n",
    "\n",
    "        batch_size = x_raw.size()[0]\n",
    "        sim_matrix = torch.einsum(\"ik,jk->ij\", x_raw, x_aug)\n",
    "\n",
    "        x_raw_abs = x_raw.norm(dim=1)\n",
    "        x_aug_abs = x_aug.norm(dim=1)\n",
    "\n",
    "        sim_matrix = sim_matrix / torch.einsum(\"i,j->ij\", x_raw_abs, x_aug_abs)\n",
    "        sim_matrix = torch.exp(sim_matrix / T)\n",
    "        pos_sim = torch.diagonal(sim_matrix)\n",
    "        \n",
    "        loss = pos_sim / (sim_matrix.sum(dim=1) - pos_sim)\n",
    "        loss = -torch.log(loss).mean()\n",
    "        # print(\"NT-Xent Loss:\", loss.item())\n",
    "        return loss        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DW63oz1LBjqz"
   },
   "source": [
    "### BarlowTwins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "sDuLvD1f3ede"
   },
   "outputs": [],
   "source": [
    "class BarlowTwins(nn.Module):\n",
    "    def __init__(self, hidden_dim, enc_layer, proj_layer, dataset_num_features):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = hidden_dim * enc_layer\n",
    "        self.dataset_num_features = dataset_num_features\n",
    "        # self.proj_layer = proj_layer\n",
    "        self.encoder = Encoder(dataset_num_features, hidden_dim, enc_layer)      \n",
    "        ## Projection Head ##           \n",
    "        self.projector = Projection_MLP(self.embedding_dim, self.embedding_dim, self.embedding_dim, proj_layer)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, num_graphs):\n",
    "        y = self.encoder(x, edge_index, batch)        \n",
    "        y = self.projector(y)                        \n",
    "        return y        \n",
    "\n",
    "    def loss(self, x_raw, x_aug):\n",
    "        ## Parameter ##\n",
    "        scale_loss = 1/32\n",
    "        lambd=3.9e-3\n",
    "\n",
    "        batch_size, metric_dim = x_raw.size()\n",
    "\n",
    "        ## Normalization \n",
    "        x_raw_norm = (x_raw - x_raw.mean(dim=0)) / x_raw.std(dim=0)  # [batch_size, metric_dim]\n",
    "        x_aug_norm = (x_aug - x_aug.mean(dim=0)) / x_aug.std(dim=0)  # [batch_size, metric_dim]\n",
    "        corr_matrix = (x_raw_norm.T @ x_aug_norm) / batch_size  # [metric_dim, metric_dim]\n",
    "\n",
    "        on_diag = torch.diagonal(corr_matrix).add_(-1).pow(2).sum().mul(scale_loss)\n",
    "        off_diag = corr_matrix.flatten()[:-1].view(metric_dim - 1, metric_dim + 1)[:, 1:].flatten()\n",
    "        off_diag = off_diag.pow(2).sum().mul(scale_loss)\n",
    "\n",
    "        loss = on_diag + (lambd * off_diag)\n",
    "        # print(\"BarlowTwins Loss:\", loss.item())\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7O8SD0KQBoeM"
   },
   "source": [
    "### SimSiam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "U53f87FtdSZu"
   },
   "outputs": [],
   "source": [
    "class Simsiam(nn.Module):\n",
    "    def __init__(self, hidden_dim, enc_layer, proj_layer, dataset_num_features):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = hidden_dim * enc_layer\n",
    "        self.dataset_num_features = dataset_num_features\n",
    "        # self.proj_layer = proj_layer\n",
    "        self.encoder = Encoder(dataset_num_features, hidden_dim, enc_layer)      \n",
    "        ## Projection Head ##           \n",
    "        self.projector = Projection_MLP(self.embedding_dim, self.embedding_dim, self.embedding_dim, proj_layer)\n",
    "        ## Predictor ##\n",
    "        self.predictor = Prediction_MLP(self.embedding_dim, self.embedding_dim, self.embedding_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, num_graphs):\n",
    "        y = self.encoder(x, edge_index, batch)        \n",
    "        y = self.projector(y)        \n",
    "        return y        \n",
    "    \n",
    "    def loss(self, x_raw, x_aug):\n",
    "        h = self.predictor\n",
    "\n",
    "        z1, z2 = x_raw, x_aug # embedding: encoder input\n",
    "        # print(x_raw.shape, x_aug.shape)\n",
    "\n",
    "        p1, p2 = h(z1), h(z2) # predictor\n",
    "        # print(p1.shape, p2.shape)\n",
    "\n",
    "        loss = D(p1, z2) / 2 + D(p2, z1) / 2\n",
    "        # print(\"Simsiam loss:\", loss.item())\n",
    "        return loss         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LMf2FZiVj0GQ"
   },
   "source": [
    "### Projection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "eKTj_O9s8Ow_"
   },
   "outputs": [],
   "source": [
    "class Projection_MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, proj_layer):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "            nn.BatchNorm1d(hidden_dim)\n",
    "        )\n",
    "        \n",
    "        self.unilayer = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True), \n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "        self.proj_layer = 3\n",
    "    \n",
    "    def set_layers(self, proj_layer):\n",
    "        self.proj_layer = proj_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.proj_layer == 3:\n",
    "            x = self.layer1(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.layer3(x)\n",
    "        elif self.proj_layer == 2:\n",
    "            x = self.layer1(x)\n",
    "            x = self.layer3(x)\n",
    "        elif self.proj_layer == 1:\n",
    "            x = self.unilayer(x)\n",
    "        else:\n",
    "            raise Exception\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpHjjObfkiuC"
   },
   "source": [
    "### Predictor\n",
    "\n",
    "Layer 1: Prediction MLP. The prediction MLP (h) has BN applied to its hidden fc layers. Its output fc does not have BN (ablation in Sec. 4.4) or ReLU. This MLP has 2 layers. The dimension of h’s input and output (z and p) is d = 2048, and h’s hidden layer’s dimension is 512, making h a bottleneck structure (ablation in supplement). \n",
    "\n",
    "Layer 2: Adding BN to the output of the prediction MLP h does not work well (Table 3d). We find that this is not about collapsing. The training is unstable and the loss oscillates.\n",
    "\n",
    "Ref: https://github.com/PatrickHua/SimSiam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "lFPxiIspcNvr"
   },
   "outputs": [],
   "source": [
    "class Prediction_MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim): # bottleneck structure\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True), # hidden layer\n",
    "            nn.Linear(hidden_dim, out_dim) # output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRhd6ErZM1O5"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jq7H2iYTiNKs"
   },
   "source": [
    "- GINConv\n",
    "- Ref: https://arxiv.org/abs/1810.00826\n",
    "- Xu et al.: How Powerful are Graph Neural Networks? (ICLR 2019) \n",
    "- https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GINConv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "JtbcSh-0Mx4O"
   },
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, num_features, dim, enc_layer):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_layer = enc_layer\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "\n",
    "        for i in range(enc_layer):     \n",
    "            if i == 0:\n",
    "                # initial layer\n",
    "                # input: (num_feature -> dim)\n",
    "                net = nn.Sequential(\n",
    "                    nn.Linear(num_features, dim), \n",
    "                    nn.ReLU(), \n",
    "                    nn.Linear(dim, dim)\n",
    "                    )     \n",
    "            else:\n",
    "                # connect layer after initail layer\n",
    "                # input: (dim -> dim)\n",
    "                net = nn.Sequential(\n",
    "                    nn.Linear(dim, dim), \n",
    "                    nn.ReLU(), \n",
    "                    nn.Linear(dim, dim)\n",
    "                    )              \n",
    "            \n",
    "            # conv_layers = GATConv()?                \n",
    "            # https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GINConv\n",
    "            # The graph isomorphism operator from the “How Powerful are Graph Neural Networks?” paper\n",
    "            conv_layers = GINConv(net)\n",
    "            batch_norm_layers = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "            self.convs.append(conv_layers)\n",
    "            self.bns.append(batch_norm_layers)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        if x is None:\n",
    "            # fill with 0 or 1?\n",
    "            x = torch.ones((batch.shape[0], 1)).to(device)\n",
    "            # x = torch.zeros((batch.shape[0], 1)).to(device)\n",
    "\n",
    "        x_ls = []\n",
    "        for i in range(self.enc_layer):\n",
    "            x = F.relu(self.convs[i](x, edge_index))\n",
    "            x = self.bns[i](x)\n",
    "            x_ls.append(x)\n",
    "\n",
    "        # https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.glob.global_add_pool\n",
    "        # Returns batch-wise graph-level-outputs by adding node features across the node dimension\n",
    "        x_pool = [global_add_pool(x, batch) for x in x_ls]\n",
    "\n",
    "        # concat embedding\n",
    "        x = torch.cat(x_pool, dim=1)\n",
    "        return x\n",
    "\n",
    "    def get_embeddings(self, loader):\n",
    "        print(\"Get Embedding...\")\n",
    "        emb_vector = []\n",
    "        y_vector = [] # graph classification (target)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in loader:\n",
    "                data_raw = data[0] # raw dara\n",
    "                data_raw = data_raw.to(device)\n",
    "                x, edge_index, batch = data_raw.x, data_raw.edge_index, data_raw.batch\n",
    "                if x is None:\n",
    "                    # fill with 0 or 1?\n",
    "                    x = torch.ones((batch.shape[0],1))\n",
    "                    # x = torch.zeros((batch.shape[0],1))\n",
    "                    x = x.to(device)\n",
    "                emb = self.forward(x, edge_index, batch)\n",
    "                # print(type(emb), type(y))\n",
    "                # print(type(emb), type(torch.from_numpy(y)))\n",
    "                # y = data_raw.y\n",
    "                # y = torch.from_numpy(y)\n",
    "                emb_vector.append(emb.cpu().numpy())\n",
    "                y_vector.append(data_raw.y.cpu().numpy())\n",
    "        # concat                \n",
    "        emb_vector = np.concatenate(emb_vector, axis=0)\n",
    "        # emb_vector shape: hidden_dim * layer number\n",
    "        y_vector = np.concatenate(y_vector, axis=0) \n",
    "        return emb_vector, y_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNyDb-WfNOcH"
   },
   "source": [
    "## Evaluate Embedding\n",
    "\n",
    "Ref: InfoGrpah, https://github.com/fanyun-sun/InfoGraph/blob/master/unsupervised/evaluate_embedding.py\n",
    "\n",
    "- SVC Classify\n",
    "- Logistic Classify\n",
    "- Random Forest Classify\n",
    "- Linearsvc Classify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eb8pRcXGaaFc",
    "outputId": "089b89c4-6bbd-4d46-9702-5286538a9f95"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "\n",
    "class LogReg(nn.Module):\n",
    "    def __init__(self, ft_in, nb_classes):\n",
    "        super(LogReg, self).__init__()\n",
    "        self.fc = nn.Linear(ft_in, nb_classes)\n",
    "        for m in self.modules():\n",
    "            self.weights_init(m)\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        ret = self.fc(seq)\n",
    "        return ret\n",
    "\n",
    "def logistic_classify(x, y):\n",
    "    nb_classes = np.unique(y).shape[0]\n",
    "    xent = nn.CrossEntropyLoss()\n",
    "    hid_units = x.shape[1]\n",
    "\n",
    "    accs = []\n",
    "    accs_val = [] #validation\n",
    "    kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=None)\n",
    "    for train_index, test_index in kf.split(x, y):\n",
    "        train_embs, test_embs = x[train_index], x[test_index]\n",
    "        train_lbls, test_lbls= y[train_index], y[test_index]\n",
    "\n",
    "        train_embs, train_lbls = torch.from_numpy(train_embs).cuda(), torch.from_numpy(train_lbls).cuda()\n",
    "        test_embs, test_lbls= torch.from_numpy(test_embs).cuda(), torch.from_numpy(test_lbls).cuda()\n",
    "\n",
    "        log = LogReg(hid_units, nb_classes)\n",
    "        log.cuda()\n",
    "        opt = torch.optim.Adam(log.parameters(), lr=0.01, weight_decay=0.0)\n",
    "\n",
    "        best_val = 0\n",
    "        test_acc = None\n",
    "        for it in range(100):\n",
    "            log.train()\n",
    "            opt.zero_grad()\n",
    "\n",
    "            logits = log(train_embs)\n",
    "            loss = xent(logits, train_lbls)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        logits = log(test_embs)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = torch.sum(preds == test_lbls).float() / test_lbls.shape[0]\n",
    "        accs.append(acc.item())\n",
    "\n",
    "        # Validation\n",
    "        val_size = len(test_index)\n",
    "        test_index = np.random.choice(test_index, val_size, replace=False).tolist()\n",
    "        train_index = [i for i in train_index if not i in test_index]\n",
    "\n",
    "        train_embs, test_embs = x[train_index], x[test_index]\n",
    "        train_lbls, test_lbls= y[train_index], y[test_index]\n",
    "\n",
    "        train_embs, train_lbls = torch.from_numpy(train_embs).cuda(), torch.from_numpy(train_lbls).cuda()\n",
    "        test_embs, test_lbls= torch.from_numpy(test_embs).cuda(), torch.from_numpy(test_lbls).cuda()\n",
    "\n",
    "\n",
    "        log = LogReg(hid_units, nb_classes)\n",
    "        log.cuda()\n",
    "        opt = torch.optim.Adam(log.parameters(), lr=0.01, weight_decay=0.0)\n",
    "\n",
    "        best_val = 0\n",
    "        test_acc = None\n",
    "        for it in range(100):\n",
    "            log.train()\n",
    "            opt.zero_grad()\n",
    "\n",
    "            logits = log(train_embs)\n",
    "            loss = xent(logits, train_lbls)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        logits = log(test_embs)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = torch.sum(preds == test_lbls).float() / test_lbls.shape[0]\n",
    "        accs_val.append(acc.item())\n",
    "    return np.mean(accs), np.mean(accs_val)\n",
    "\n",
    "def svc_classify(x, y, search):\n",
    "    kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=None)\n",
    "    accuracies = []\n",
    "    accuracies_val = []\n",
    "    for train_index, test_index in kf.split(x, y):\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1)\n",
    "        if search:\n",
    "            params = {'C':[0.001, 0.01,0.1,1,10,100,1000]}\n",
    "            classifier = GridSearchCV(SVC(), params, cv=5, scoring='accuracy', verbose=0)\n",
    "        else:\n",
    "            classifier = SVC(C=10)\n",
    "        classifier.fit(x_train, y_train)\n",
    "        accuracies.append(accuracy_score(y_test, classifier.predict(x_test)))\n",
    "\n",
    "        # validation\n",
    "        val_size = len(test_index)\n",
    "        test_index = np.random.choice(train_index, val_size, replace=False).tolist()\n",
    "        train_index = [i for i in train_index if not i in test_index]\n",
    "\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1)\n",
    "        if search:\n",
    "            params = {'C':[0.001, 0.01,0.1,1,10,100,1000]}\n",
    "            classifier = GridSearchCV(SVC(), params, cv=5, scoring='accuracy', verbose=0)\n",
    "        else:\n",
    "            classifier = SVC(C=10)\n",
    "        classifier.fit(x_train, y_train)\n",
    "        accuracies_val.append(accuracy_score(y_test, classifier.predict(x_test)))\n",
    "    return np.mean(accuracies), np.mean(accuracies_val)\n",
    "\n",
    "def randomforest_classify(x, y, search):\n",
    "    kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=None)\n",
    "    accuracies = []\n",
    "    accuracies_val = []\n",
    "    for train_index, test_index in kf.split(x, y):\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        if search:\n",
    "            params = {'n_estimators': [100, 200, 500, 1000]}\n",
    "            classifier = GridSearchCV(RandomForestClassifier(), params, cv=5, scoring='accuracy', verbose=0)\n",
    "        else:\n",
    "            classifier = RandomForestClassifier()\n",
    "        classifier.fit(x_train, y_train)\n",
    "        accuracies.append(accuracy_score(y_test, classifier.predict(x_test)))\n",
    "\n",
    "        # val\n",
    "        val_size = len(test_index)\n",
    "        test_index = np.random.choice(test_index, val_size, replace=False).tolist()\n",
    "        train_index = [i for i in train_index if not i in test_index]\n",
    "\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        if search:\n",
    "            params = {'n_estimators': [100, 200, 500, 1000]}\n",
    "            classifier = GridSearchCV(RandomForestClassifier(), params, cv=5, scoring='accuracy', verbose=0)\n",
    "        else:\n",
    "            classifier = RandomForestClassifier()\n",
    "        classifier.fit(x_train, y_train)\n",
    "        accuracies_val.append(accuracy_score(y_test, classifier.predict(x_test)))    \n",
    "    return np.mean(accuracies), np.mean(accuracies_val)\n",
    "\n",
    "def linearsvc_classify(x, y, search):\n",
    "    kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=None)\n",
    "    accuracies = []\n",
    "    accuracies_val = []\n",
    "    for train_index, test_index in kf.split(x, y):\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        if search:\n",
    "            params = {'C':[0.001, 0.01,0.1,1,10,100,1000]}\n",
    "            classifier = GridSearchCV(LinearSVC(), params, cv=5, scoring='accuracy', verbose=0)\n",
    "        else:\n",
    "            classifier = LinearSVC(C=10)\n",
    "        classifier.fit(x_train, y_train)\n",
    "        accuracies.append(accuracy_score(y_test, classifier.predict(x_test)))\n",
    "\n",
    "        # val\n",
    "        val_size = len(test_index)\n",
    "        test_index = np.random.choice(train_index, val_size, replace=False).tolist()\n",
    "        train_index = [i for i in train_index if not i in test_index]\n",
    "\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        if search:\n",
    "            params = {'C':[0.001, 0.01,0.1,1,10,100,1000]}\n",
    "            classifier = GridSearchCV(LinearSVC(), params, cv=5, scoring='accuracy', verbose=0)\n",
    "        else:\n",
    "            classifier = LinearSVC(C=10)\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            classifier.fit(x_train, y_train)\n",
    "        accuracies_val.append(accuracy_score(y_test, classifier.predict(x_test)))\n",
    "    return np.mean(accuracies), np.mean(accuracies_val)\n",
    "\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def evaluate_embedding(embeddings, labels, search=True):\n",
    "\n",
    "    labels = preprocessing.LabelEncoder().fit_transform(labels)\n",
    "    x, y = np.array(embeddings), np.array(labels)\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "    # print(\"+-----------------------------------------------------------------------------+\")\n",
    "    # print(\"[Acc, Validation Acc]\")\n",
    "    # logreg_acc, logreg_acc_val = logistic_classify(x, y)\n",
    "    # log_result = [np.mean(logreg_acc), np.mean(logreg_acc_val)]\n",
    "    # print('LogReg Accuracy:', log_result)\n",
    "\n",
    "    svc_acc, svc_acc_val = svc_classify(x, y, search)\n",
    "    scv_result = [np.mean(svc_acc), np.mean(svc_acc_val)]\n",
    "    print(\"SVC Accuracy:\", scv_result)\n",
    "\n",
    "    # linsvc_acc, linsvc_acc_val = linearsvc_classify(x, y, search)\n",
    "    # linsvc_result = [np.mean(linsvc_acc), np.mean(linsvc_acc_val)]\n",
    "    # print('LinearSVC Accuracy:', linsvc_result)\n",
    "\n",
    "    # rf_acc, rf_acc_val = randomforest_classify(x, y, search)\n",
    "    # rf_result = [np.mean(rf_acc), np.mean(rf_acc_val)]\n",
    "    # print('RandomForest Accuracy:', rf_result)\n",
    "    # print(\"+-----------------------------------------------------------------------------+\")\n",
    "\n",
    "    # return log_result, scv_result, linsvc_result, rf_result\n",
    "    return scv_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aF-02p_YGnd"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfYU-3a_-BBz"
   },
   "source": [
    "### Arguments \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "tdTRuSka6jVR"
   },
   "outputs": [],
   "source": [
    "model_ls = [\"SimCLR\", \"BarlowTwins\", \"Simsiam\"]\n",
    "aug_ls = [\"node_dropping\", \"edge_perturbation\", \"subgraph\", \"attribute_masking\"]\n",
    "batch_size_ls = [64, 128]\n",
    "hid_dim_ls = [64, 512]\n",
    "enc_layers = [1, 2, 3]\n",
    "proj_layer = [1, 2, 3]\n",
    "lr = 0.001\n",
    "epochs = 200\n",
    "data_prop = 0.9 # use 90\\% samples for self-learning training, 10 \\% for supervised validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohabCtinFjcq"
   },
   "source": [
    "### Training\n",
    "\n",
    "Loss:\n",
    "1. SimCLR (NX-Tent Loss)\n",
    "2. Barlow Twins\n",
    "3. Simsiam\n",
    "\n",
    "- Self-supervised: large, unlabeled, augmentation\n",
    "- supervised (eval): small, labeled, raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "nr2lZKmCHAvK"
   },
   "outputs": [],
   "source": [
    "with open(log_path+\"log_\"+dataset_name+\".txt\", mode=\"w+\") as f:  \n",
    "    f.write(\"{},{},{},{},{},{},{},{},{},{},{},{}\".format(\"idx\",\"data_prop\",\"model_name\", \"augmentation\", \"batch_size\", \"enc_layer\", \"hidden_dim\", \"acc\", \"val_acc\", \"cost_time(sec)\", \"record_time\", \"emb_time\"))\n",
    "    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x2c2Nxt79Z30",
    "outputId": "6c11d12a-c142-4640-8c5c-4ccf23c857bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1060\n",
      "[553, 217, 346, 850, 1139, 72, 280, 861, 916, 589, 50, 182, 307, 1067, 172, 583, 831, 791, 1014, 246, 18, 1013, 59, 1023, 984, 69, 755, 65, 71, 742, 1153, 256, 856, 266, 427, 956, 679, 826, 541, 147, 910, 0, 922, 683, 131, 103, 91, 1001, 166, 558, 796, 1078, 811, 852, 351, 1002, 383, 145, 197, 1072, 789, 17, 606, 9, 608, 1111, 664, 414, 154, 290, 954, 667, 959, 1097, 652, 339, 184, 368, 327, 506, 933, 370, 36, 768, 67, 1158, 93, 1123, 47, 624, 311, 883, 484, 702, 379, 316, 794, 822, 818, 55, 375, 1008, 605, 584, 920, 575, 986, 675, 82, 468, 1104, 202, 428, 566, 85, 550, 914, 1142, 545, 1145, 870, 1149, 332, 732, 185, 401, 359, 540, 714, 277, 785, 56, 45, 330, 1137, 564, 941, 703, 157, 1056, 1025, 325, 394, 360, 1077, 611, 1058, 1131, 830, 1081, 857, 1105, 364, 57, 233, 291, 1065, 926, 181, 231, 230, 808, 731, 678, 588, 929, 226, 537, 680, 38, 746, 595, 733, 244, 138, 1039, 441, 216, 762, 220, 284, 570, 1166, 1103, 117, 476, 261, 312, 232, 424, 149, 1110, 183, 651, 776, 522, 898, 381, 1100, 1088, 221, 1043, 596, 1084, 719, 669, 253, 715, 498, 35, 112, 495, 524, 548, 369, 735, 574, 580, 391, 167, 293, 942, 793, 273, 204, 1049, 474, 292, 973, 129, 1006, 402, 444, 975, 137, 1071, 647, 656, 1080, 1045, 333, 648, 555, 1127, 576, 486, 602, 989, 399, 3, 1030, 1016, 782, 153, 962, 421, 620, 1036, 264, 944, 848, 736, 717, 26, 1121, 978, 338, 398, 449, 46, 778, 877, 773, 633, 287, 465, 957, 828, 462, 800, 419, 902, 709, 953, 549, 997, 724, 78, 779, 1019, 2, 1050, 546, 1048, 195, 1061, 995, 1032, 10, 25, 450, 357, 1135, 254, 559, 612, 912, 965, 179, 94, 739, 470, 616, 298, 900, 224, 598, 301, 436, 1099, 234, 1060, 1053, 839, 815, 158, 804, 945, 89, 31, 210, 365, 993, 413, 225, 115, 618, 42, 567, 320, 1125, 644, 532, 337, 513, 168, 609, 243, 494, 186, 734, 1159, 193, 439, 305, 641, 628, 300, 521, 636, 948, 198, 994, 556, 96, 1130, 568, 22, 557, 1021, 132, 938, 707, 463, 723, 1152, 915, 711, 156, 631, 931, 457, 1066, 433, 666, 464, 459, 597, 816, 471, 725, 614, 1063, 397, 716, 684, 176, 227, 577, 1004, 801, 637, 34, 761, 302, 404, 1090, 980, 1028, 262, 161, 426, 751, 617, 1000, 39, 345, 504, 880, 907, 409, 173, 515, 249, 1069, 285, 1156, 730, 350, 260, 150, 281, 923, 361, 1062, 918, 1052, 904, 282, 1095, 219, 503, 738, 88, 668, 563, 710, 946, 207, 146, 60, 340, 760, 691, 106, 827, 265, 571, 15, 1132, 1015, 985, 825, 970, 758, 223, 406, 921, 1044, 772, 120, 247, 4, 542, 101, 859, 467, 1150, 12, 871, 268, 1012, 630, 1009, 336, 322, 621, 491, 1026, 187, 629, 432, 385, 992, 809, 1024, 756, 366, 62, 188, 53, 899, 950, 939, 1175, 988, 13, 105, 692, 659, 160, 960, 134, 875, 492, 107, 505, 757, 932, 653, 533, 175, 342, 314, 1165, 936, 1114, 252, 639, 8, 695, 838, 539, 824, 458, 408, 508, 248, 148, 352, 1162, 663, 911, 578, 318, 1116, 108, 1106, 235, 844, 54, 889, 645, 586, 1068, 239, 326, 763, 689, 104, 81, 323, 48, 1031, 908, 1041, 851, 1124, 452, 961, 122, 127, 1108, 388, 215, 155, 73, 887, 475, 77, 348, 485, 84, 1148, 650, 509, 75, 299, 331, 144, 241, 288, 727, 619, 775, 625, 206, 977, 455, 934, 697, 777, 865, 245, 835, 585, 133, 70, 655, 1161, 1092, 892, 190, 7, 32, 807, 110, 693, 1134, 1064, 295, 19, 872, 729, 1154, 289, 1176, 657, 142, 890, 445, 952, 275, 866, 587, 927, 603, 943, 211, 1118, 728, 593, 701, 477, 222, 196, 440, 205, 531, 981, 354, 478, 165, 242, 958, 1115, 1075, 1074, 1007, 24, 967, 913, 49, 744, 11, 554, 201, 321, 437, 517, 999, 814, 529, 681, 236, 180, 750, 780, 135, 868, 516, 879, 820, 315, 1128, 560, 218, 845, 297, 949, 841, 1059, 309, 1167, 1155, 665, 425, 1172, 461, 812, 79, 143, 622, 983, 937, 438, 353, 832, 6, 901, 795, 990, 759, 869, 1120, 276, 686, 810, 199, 371, 174, 677, 627, 116, 1133, 435, 519, 113, 496, 1029, 706, 255, 764, 130, 1005, 525, 500, 453, 367, 720, 171, 278, 483, 1122, 754, 698, 1051, 599, 1126, 74, 867, 726, 885, 250, 191, 1027, 229, 996, 310, 209, 884, 111, 456, 1082, 76, 358, 343, 66, 356, 213, 874, 1164, 362, 813, 1138, 5, 1055, 324, 888, 480, 849, 1033, 228, 267, 410, 786, 1035, 139, 873, 972, 740, 303, 951, 304, 1140, 543, 682, 382, 393, 896, 488, 718, 447, 1054, 417, 643, 43, 964, 14, 237, 189, 662, 29, 306, 1144, 119, 654, 86, 1107, 783, 511, 876, 1136, 696, 1146, 1086, 507, 341, 649, 771, 169, 274, 37, 376, 317, 279, 395, 840, 966, 251, 687, 878, 787, 159, 600, 1087, 905, 319, 1160, 378, 1094, 263, 52, 987, 344, 308, 363, 416, 396, 924, 769, 688, 33, 615, 460, 979, 906, 121, 895, 1171, 102, 579, 526, 882, 1168, 177, 523, 380, 1076, 864, 1085, 125, 674, 83, 781, 64, 124, 1047, 51, 20, 661, 767, 258, 151, 833, 638, 1109, 590, 635, 1169, 930, 372, 448, 982, 991, 846, 538, 374, 792, 446, 708, 528, 283, 819, 1089, 858, 1038, 806, 466, 493, 1174, 347, 114, 136, 128, 863, 935, 1093, 415, 269, 510, 1129, 834, 1, 389, 68, 749, 963, 552, 487, 430, 212, 971, 847, 745, 194, 162, 713, 296, 1011, 592, 582, 917, 594, 429, 853, 1073, 784, 520, 499, 469, 451, 1079, 572, 862, 753, 737, 27, 1117, 547, 947, 123, 894, 1112, 855, 384, 1034, 422, 490, 712, 843, 192, 1173, 203, 634, 514, 928, 694, 423, 141, 420, 329, 1017, 1119, 377, 1046, 527, 998, 349, 272, 454, 1057, 501, 178, 392, 405, 774, 472, 238, 1042, 534, 699, 482, 1018, 569, 842, 164, 660, 1113, 536, 817, 95, 240, 387, 854, 126, 561, 743, 431, 672, 802, 208, 766, 690, 390, 1101, 591, 604, 109, 909, 803, 28, 286, 765, 163, 601, 1163, 722, 90, 1177, 1037, 152, 1098, 805, 334, 294, 829, 700, 974, 21]\n"
     ]
    }
   ],
   "source": [
    "dataset_tmp = TUDataset_AUG(path, name=dataset_name, aug=[\"raw\", \"raw\"], proportion=[1,1])\n",
    "data_size = math.floor(len(dataset_tmp) * data_prop)\n",
    "train_idx = np.random.choice(range(len(dataset_tmp)), data_size, replace=False).tolist()\n",
    "print(data_size)\n",
    "print(train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_aug = TUDataset_AUG(path, name=dataset_name, aug=[\"raw\", \"subgraph\"], proportion=0.1)\n",
    "# dataset_eval = TUDataset_AUG(path, name=dataset_name, aug=[\"raw\", \"raw\"])\n",
    "\n",
    "# dataset_num_features = dataset_eval[0][0].num_node_features if dataset_eval[0][0].num_node_features is not None else 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_eval[0][0].num_node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "TEQrT65vNj_-"
   },
   "outputs": [],
   "source": [
    "def main_train(idx, train_idx, augmentation, batch_size, hidden_dim, enc_layer, proj_layer, model_name):    \n",
    "    aug_method = [\"raw\", augmentation]\n",
    "    proportion = [0.3, 0.3]\n",
    "  \n",
    "    dataset_aug = TUDataset_AUG(path, name=dataset_name, aug=aug_method, proportion=proportion)\n",
    "    dataset_eval = TUDataset_AUG(path, name=dataset_name, aug=[\"raw\", \"raw\"])\n",
    "    print(\"!!!\") # check features\n",
    "    dataset_num_features = dataset_eval[0][0].num_node_features if dataset_eval[0][0].num_node_features is not None else 1\n",
    "    print(dataset_num_features)\n",
    "    print(\"!!!\")\n",
    "    \n",
    "\n",
    "    test_idx = [i for i in range(len(dataset_aug)) if not i in train_idx]\n",
    "\n",
    "    dataset_train, dataset_test = dataset_aug[train_idx], dataset_eval[test_idx]\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=batch_size)  \n",
    "    dataloader_test = DataLoader(dataset_test, batch_size=batch_size)\n",
    "\n",
    "    print(train_idx)\n",
    "    print(\"Number of Graphs (dataset_train):\", len(dataset_train)) # 有幾張圖\n",
    "    print(\"Number of Graphs (dataset_test):\", len(dataset_test)) # 有幾張圖\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "    if model_name == \"SimCLR\":\n",
    "        model = SimCLR(hidden_dim, enc_layer, proj_layer, dataset_num_features).to(device)\n",
    "    elif model_name == \"BarlowTwins\":\n",
    "        model = BarlowTwins(hidden_dim, enc_layer, proj_layer, dataset_num_features).to(device)\n",
    "    elif model_name == \"Simsiam\":\n",
    "        model = Simsiam(hidden_dim, enc_layer, proj_layer, dataset_num_features).to(device)\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"+-----------------------------------------------------------------------------+\")\n",
    "    print(\"Learning Rate:\", lr)\n",
    "    print(\"Epochs:\", epochs)\n",
    "    print(\"Batch Size:\", batch_size)\n",
    "    print(\"Hidden Dimension\", hidden_dim)\n",
    "    print(\"Number of Layer in Encoder:\", enc_layer)\n",
    "    print(\"Opimizer:\", optimizer)\n",
    "    print(\"+-----------------------------------------------------------------------------+\")\n",
    "    print(\"Device:\", device)\n",
    "    print(\"Model:\")\n",
    "    print(model)\n",
    "    print(\"+-----------------------------------------------------------------------------+\")\n",
    "\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    start_time = time.time()\n",
    "    print(\"Start Training...\")\n",
    "    print(\"+-----------------------------------------------------------------------------+\")\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        loss_all = 0\n",
    "        model.train()\n",
    "        for data in dataloader_train:   \n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "            data_raw, data_aug = data\n",
    "            # raw data, augmentation data\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            data_raw = data_raw.to(device)\n",
    "            x_raw = model(data_raw.x, data_raw.edge_index, data_raw.batch, data_raw.num_graphs)\n",
    "\n",
    "            node_num = data_raw.x.size()[0] # number of nodes in this batch\n",
    "            edge_idx = data_aug.edge_index.numpy()\n",
    "            edge_num = edge_idx.shape[1] # number of edges in this batch\n",
    "            idx_not_missing = [n for n in range(node_num) if (n in edge_idx[0] or n in edge_idx[1])]\n",
    "            # via subgraph or node dropping, it might get isolated nodes.\n",
    "\n",
    "            node_num_aug = len(idx_not_missing)\n",
    "            data_aug.x = data_aug.x[idx_not_missing]      \n",
    "\n",
    "            data_aug.batch = data_raw.batch[idx_not_missing]\n",
    "            idx_dict = {idx_not_missing[n]:n for n in range(node_num_aug)}\n",
    "            edge_idx = [[idx_dict[edge_idx[0, n]], idx_dict[edge_idx[1, n]]] for n in range(edge_num) if not edge_idx[0, n] == edge_idx[1, n]]\n",
    "            data_aug.edge_index = torch.tensor(edge_idx).transpose(0, 1)\n",
    "            ##\n",
    "\n",
    "            data_aug = data_aug.to(device)\n",
    "            x_aug = model(data_aug.x, data_aug.edge_index, data_aug.batch, data_aug.num_graphs)\n",
    "            \n",
    "            # print(\"x_raw:\", x_raw.shape)\n",
    "            # print(\"x_aug:\", x_aug.shape)\n",
    "\n",
    "            loss = model.loss(x_raw, x_aug)\n",
    "            loss_all = loss_all + (loss.item() * data_raw.num_graphs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(\"Epoch\", epoch, \"| Loss =\", loss_all/len(dataloader_train))\n",
    "        losses.append(loss_all/len(dataloader_train))\n",
    "    end_time = time.time()       \n",
    "    ttl_time = end_time-start_time\n",
    "    print(\"+-----------------------------------------------------------------------------+\")\n",
    "    print(\"Training Time:\", ttl_time, \"seconds.\")\n",
    "\n",
    "    record__to_txt(log_path, losses, dataset_name+\"_Loss_\"+str(idx))\n",
    "\n",
    "    ## TEST\n",
    "\n",
    "    model.eval() \n",
    "\n",
    "    start_time = time.time()       \n",
    "    emb, y = model.encoder.get_embeddings(dataloader_test)\n",
    "    # print(emb, y)\n",
    "    # print(\"Embedding Shape:\", emb.shape, y.shape)\n",
    "    scv_result = evaluate_embedding(emb, y)\n",
    "\n",
    "    end_time = time.time()       \n",
    "    emb_time = end_time-start_time\n",
    "    print(\"+-----------------------------------------------------------------------------+\")\n",
    "    print(\"Embedding Time:\", emb_time, \"seconds.\")\n",
    "\n",
    "    return scv_result[0], scv_result[1], ttl_time, emb_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cksg088_--d0",
    "outputId": "8e5f961f-3264-4936-b457-3bf4ccdb82c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------+\n",
      "Experient 0\n",
      "!!!\n",
      "89\n",
      "!!!\n",
      "[553, 217, 346, 850, 1139, 72, 280, 861, 916, 589, 50, 182, 307, 1067, 172, 583, 831, 791, 1014, 246, 18, 1013, 59, 1023, 984, 69, 755, 65, 71, 742, 1153, 256, 856, 266, 427, 956, 679, 826, 541, 147, 910, 0, 922, 683, 131, 103, 91, 1001, 166, 558, 796, 1078, 811, 852, 351, 1002, 383, 145, 197, 1072, 789, 17, 606, 9, 608, 1111, 664, 414, 154, 290, 954, 667, 959, 1097, 652, 339, 184, 368, 327, 506, 933, 370, 36, 768, 67, 1158, 93, 1123, 47, 624, 311, 883, 484, 702, 379, 316, 794, 822, 818, 55, 375, 1008, 605, 584, 920, 575, 986, 675, 82, 468, 1104, 202, 428, 566, 85, 550, 914, 1142, 545, 1145, 870, 1149, 332, 732, 185, 401, 359, 540, 714, 277, 785, 56, 45, 330, 1137, 564, 941, 703, 157, 1056, 1025, 325, 394, 360, 1077, 611, 1058, 1131, 830, 1081, 857, 1105, 364, 57, 233, 291, 1065, 926, 181, 231, 230, 808, 731, 678, 588, 929, 226, 537, 680, 38, 746, 595, 733, 244, 138, 1039, 441, 216, 762, 220, 284, 570, 1166, 1103, 117, 476, 261, 312, 232, 424, 149, 1110, 183, 651, 776, 522, 898, 381, 1100, 1088, 221, 1043, 596, 1084, 719, 669, 253, 715, 498, 35, 112, 495, 524, 548, 369, 735, 574, 580, 391, 167, 293, 942, 793, 273, 204, 1049, 474, 292, 973, 129, 1006, 402, 444, 975, 137, 1071, 647, 656, 1080, 1045, 333, 648, 555, 1127, 576, 486, 602, 989, 399, 3, 1030, 1016, 782, 153, 962, 421, 620, 1036, 264, 944, 848, 736, 717, 26, 1121, 978, 338, 398, 449, 46, 778, 877, 773, 633, 287, 465, 957, 828, 462, 800, 419, 902, 709, 953, 549, 997, 724, 78, 779, 1019, 2, 1050, 546, 1048, 195, 1061, 995, 1032, 10, 25, 450, 357, 1135, 254, 559, 612, 912, 965, 179, 94, 739, 470, 616, 298, 900, 224, 598, 301, 436, 1099, 234, 1060, 1053, 839, 815, 158, 804, 945, 89, 31, 210, 365, 993, 413, 225, 115, 618, 42, 567, 320, 1125, 644, 532, 337, 513, 168, 609, 243, 494, 186, 734, 1159, 193, 439, 305, 641, 628, 300, 521, 636, 948, 198, 994, 556, 96, 1130, 568, 22, 557, 1021, 132, 938, 707, 463, 723, 1152, 915, 711, 156, 631, 931, 457, 1066, 433, 666, 464, 459, 597, 816, 471, 725, 614, 1063, 397, 716, 684, 176, 227, 577, 1004, 801, 637, 34, 761, 302, 404, 1090, 980, 1028, 262, 161, 426, 751, 617, 1000, 39, 345, 504, 880, 907, 409, 173, 515, 249, 1069, 285, 1156, 730, 350, 260, 150, 281, 923, 361, 1062, 918, 1052, 904, 282, 1095, 219, 503, 738, 88, 668, 563, 710, 946, 207, 146, 60, 340, 760, 691, 106, 827, 265, 571, 15, 1132, 1015, 985, 825, 970, 758, 223, 406, 921, 1044, 772, 120, 247, 4, 542, 101, 859, 467, 1150, 12, 871, 268, 1012, 630, 1009, 336, 322, 621, 491, 1026, 187, 629, 432, 385, 992, 809, 1024, 756, 366, 62, 188, 53, 899, 950, 939, 1175, 988, 13, 105, 692, 659, 160, 960, 134, 875, 492, 107, 505, 757, 932, 653, 533, 175, 342, 314, 1165, 936, 1114, 252, 639, 8, 695, 838, 539, 824, 458, 408, 508, 248, 148, 352, 1162, 663, 911, 578, 318, 1116, 108, 1106, 235, 844, 54, 889, 645, 586, 1068, 239, 326, 763, 689, 104, 81, 323, 48, 1031, 908, 1041, 851, 1124, 452, 961, 122, 127, 1108, 388, 215, 155, 73, 887, 475, 77, 348, 485, 84, 1148, 650, 509, 75, 299, 331, 144, 241, 288, 727, 619, 775, 625, 206, 977, 455, 934, 697, 777, 865, 245, 835, 585, 133, 70, 655, 1161, 1092, 892, 190, 7, 32, 807, 110, 693, 1134, 1064, 295, 19, 872, 729, 1154, 289, 1176, 657, 142, 890, 445, 952, 275, 866, 587, 927, 603, 943, 211, 1118, 728, 593, 701, 477, 222, 196, 440, 205, 531, 981, 354, 478, 165, 242, 958, 1115, 1075, 1074, 1007, 24, 967, 913, 49, 744, 11, 554, 201, 321, 437, 517, 999, 814, 529, 681, 236, 180, 750, 780, 135, 868, 516, 879, 820, 315, 1128, 560, 218, 845, 297, 949, 841, 1059, 309, 1167, 1155, 665, 425, 1172, 461, 812, 79, 143, 622, 983, 937, 438, 353, 832, 6, 901, 795, 990, 759, 869, 1120, 276, 686, 810, 199, 371, 174, 677, 627, 116, 1133, 435, 519, 113, 496, 1029, 706, 255, 764, 130, 1005, 525, 500, 453, 367, 720, 171, 278, 483, 1122, 754, 698, 1051, 599, 1126, 74, 867, 726, 885, 250, 191, 1027, 229, 996, 310, 209, 884, 111, 456, 1082, 76, 358, 343, 66, 356, 213, 874, 1164, 362, 813, 1138, 5, 1055, 324, 888, 480, 849, 1033, 228, 267, 410, 786, 1035, 139, 873, 972, 740, 303, 951, 304, 1140, 543, 682, 382, 393, 896, 488, 718, 447, 1054, 417, 643, 43, 964, 14, 237, 189, 662, 29, 306, 1144, 119, 654, 86, 1107, 783, 511, 876, 1136, 696, 1146, 1086, 507, 341, 649, 771, 169, 274, 37, 376, 317, 279, 395, 840, 966, 251, 687, 878, 787, 159, 600, 1087, 905, 319, 1160, 378, 1094, 263, 52, 987, 344, 308, 363, 416, 396, 924, 769, 688, 33, 615, 460, 979, 906, 121, 895, 1171, 102, 579, 526, 882, 1168, 177, 523, 380, 1076, 864, 1085, 125, 674, 83, 781, 64, 124, 1047, 51, 20, 661, 767, 258, 151, 833, 638, 1109, 590, 635, 1169, 930, 372, 448, 982, 991, 846, 538, 374, 792, 446, 708, 528, 283, 819, 1089, 858, 1038, 806, 466, 493, 1174, 347, 114, 136, 128, 863, 935, 1093, 415, 269, 510, 1129, 834, 1, 389, 68, 749, 963, 552, 487, 430, 212, 971, 847, 745, 194, 162, 713, 296, 1011, 592, 582, 917, 594, 429, 853, 1073, 784, 520, 499, 469, 451, 1079, 572, 862, 753, 737, 27, 1117, 547, 947, 123, 894, 1112, 855, 384, 1034, 422, 490, 712, 843, 192, 1173, 203, 634, 514, 928, 694, 423, 141, 420, 329, 1017, 1119, 377, 1046, 527, 998, 349, 272, 454, 1057, 501, 178, 392, 405, 774, 472, 238, 1042, 534, 699, 482, 1018, 569, 842, 164, 660, 1113, 536, 817, 95, 240, 387, 854, 126, 561, 743, 431, 672, 802, 208, 766, 690, 390, 1101, 591, 604, 109, 909, 803, 28, 286, 765, 163, 601, 1163, 722, 90, 1177, 1037, 152, 1098, 805, 334, 294, 829, 700, 974, 21]\n",
      "Number of Graphs (dataset_train): 1060\n",
      "Number of Graphs (dataset_test): 118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------+\n",
      "Learning Rate: 0.001\n",
      "Epochs: 200\n",
      "Batch Size: 64\n",
      "Hidden Dimension 64\n",
      "Number of Layer in Encoder: 1\n",
      "Opimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device: cuda\n",
      "Model:\n",
      "SimCLR(\n",
      "  (encoder): Encoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=89, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      ))\n",
      "    )\n",
      "    (bns): ModuleList(\n",
      "      (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Projection_MLP(\n",
      "    (layer1): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (unilayer): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Start Training...\n",
      "+-----------------------------------------------------------------------------+\n",
      "Epoch 1 | Loss = 60.857795750393585\n",
      "Epoch 2 | Loss = 43.43329719497877\n",
      "Epoch 3 | Loss = 34.644820493810315\n",
      "Epoch 4 | Loss = 29.59149449537782\n",
      "Epoch 5 | Loss = 24.938999863231885\n",
      "Epoch 6 | Loss = 22.43268390964059\n",
      "Epoch 7 | Loss = 21.202784124542685\n",
      "Epoch 8 | Loss = 18.538088167414944\n",
      "Epoch 9 | Loss = 16.665183123420267\n",
      "Epoch 10 | Loss = 14.802955739638385\n",
      "Epoch 11 | Loss = 12.206994884154376\n",
      "Epoch 12 | Loss = 12.657046149758731\n",
      "Epoch 13 | Loss = 11.833971121731926\n",
      "Epoch 14 | Loss = 9.939342162188362\n",
      "Epoch 15 | Loss = 8.36337410702425\n",
      "Epoch 16 | Loss = 7.876144661622889\n",
      "Epoch 17 | Loss = 8.209789696861716\n",
      "Epoch 18 | Loss = 7.07540963677799\n",
      "Epoch 19 | Loss = 5.6201408470378205\n",
      "Epoch 20 | Loss = 5.2854678700951965\n",
      "Epoch 21 | Loss = 5.152012025608736\n",
      "Epoch 22 | Loss = 4.547190161312328\n",
      "Epoch 23 | Loss = 2.6901187475989845\n",
      "Epoch 24 | Loss = 2.3799192169133354\n",
      "Epoch 25 | Loss = 1.6024399995803833\n",
      "Epoch 26 | Loss = 1.7295413683442509\n",
      "Epoch 27 | Loss = 1.8662340570898617\n",
      "Epoch 28 | Loss = 0.6623912979574764\n",
      "Epoch 29 | Loss = 1.8447892946355484\n",
      "Epoch 30 | Loss = -0.13964912120033712\n",
      "Epoch 31 | Loss = 0.4794123102636898\n",
      "Epoch 32 | Loss = 0.09813195116379682\n",
      "Epoch 33 | Loss = -1.321019915973439\n",
      "Epoch 34 | Loss = -1.9242832099690157\n",
      "Epoch 35 | Loss = -2.845842340413262\n",
      "Epoch 36 | Loss = -2.8364096809835995\n",
      "Epoch 37 | Loss = -2.319388417636647\n",
      "Epoch 38 | Loss = -3.344566022648531\n",
      "Epoch 39 | Loss = -3.4711527999709633\n",
      "Epoch 40 | Loss = -3.4847411934067223\n",
      "Epoch 41 | Loss = -4.20483416669509\n",
      "Epoch 42 | Loss = -4.062896737281014\n",
      "Epoch 43 | Loss = -3.6472324343288647\n",
      "Epoch 44 | Loss = -4.093609911554\n",
      "Epoch 45 | Loss = -5.230615044341368\n",
      "Epoch 46 | Loss = -4.976735577863805\n",
      "Epoch 47 | Loss = -4.563061156693627\n",
      "Epoch 48 | Loss = -5.465505624518675\n",
      "Epoch 49 | Loss = -6.421045029864592\n",
      "Epoch 50 | Loss = -6.556527768864351\n",
      "Epoch 51 | Loss = -6.504938395584331\n",
      "Epoch 52 | Loss = -5.9849578738212585\n",
      "Epoch 53 | Loss = -5.806068820111892\n",
      "Epoch 54 | Loss = -6.729418645886814\n",
      "Epoch 55 | Loss = -7.448591456693761\n",
      "Epoch 56 | Loss = -6.9954692546059105\n",
      "Epoch 57 | Loss = -7.642475815380321\n",
      "Epoch 58 | Loss = -7.392084949156818\n",
      "Epoch 59 | Loss = -6.973473818863139\n",
      "Epoch 60 | Loss = -8.383105428779826\n",
      "Epoch 61 | Loss = -8.163944938603569\n",
      "Epoch 62 | Loss = -7.48356182084364\n",
      "Epoch 64 | Loss = -8.765646748683032\n",
      "Epoch 65 | Loss = -8.387527325574089\n",
      "Epoch 66 | Loss = -9.145503184374641\n",
      "Epoch 67 | Loss = -8.44676236895954\n",
      "Epoch 68 | Loss = -8.572546979960274\n",
      "Epoch 69 | Loss = -9.145620598512537\n",
      "Epoch 70 | Loss = -9.384922977756052\n",
      "Epoch 71 | Loss = -8.968830024494844\n",
      "Epoch 72 | Loss = -9.751738912918988\n",
      "Epoch 73 | Loss = -9.629557244917926\n",
      "Epoch 74 | Loss = -10.77948260307312\n",
      "Epoch 75 | Loss = -9.246799111366272\n",
      "Epoch 76 | Loss = -11.063808833851533\n",
      "Epoch 77 | Loss = -10.698735573712517\n",
      "Epoch 78 | Loss = -10.778206460616168\n",
      "Epoch 79 | Loss = -10.325790994307575\n",
      "Epoch 80 | Loss = -10.884999892290901\n",
      "Epoch 81 | Loss = -9.683566258234137\n",
      "Epoch 82 | Loss = -10.610052739872652\n",
      "Epoch 83 | Loss = -10.575249068877277\n",
      "Epoch 84 | Loss = -11.826647337745218\n",
      "Epoch 85 | Loss = -10.17016308447894\n",
      "Epoch 86 | Loss = -11.261548925848569\n",
      "Epoch 87 | Loss = -11.593364168615903\n",
      "Epoch 88 | Loss = -11.80446964151719\n",
      "Epoch 89 | Loss = -12.162463721107034\n",
      "Epoch 90 | Loss = -10.535080531064201\n",
      "Epoch 91 | Loss = -11.932876053978415\n",
      "Epoch 92 | Loss = -11.412902313120226\n",
      "Epoch 93 | Loss = -11.773611713858212\n",
      "Epoch 94 | Loss = -11.66929451157065\n",
      "Epoch 95 | Loss = -11.767813598408418\n",
      "Epoch 96 | Loss = -12.284680871402516\n",
      "Epoch 97 | Loss = -12.55669506858377\n",
      "Epoch 98 | Loss = -12.510637914433199\n",
      "Epoch 99 | Loss = -12.085824349347282\n",
      "Epoch 100 | Loss = -12.782786355299109\n",
      "Epoch 101 | Loss = -13.232494508518892\n",
      "Epoch 102 | Loss = -13.008114955004524\n",
      "Epoch 103 | Loss = -12.697047177483054\n",
      "Epoch 104 | Loss = -13.130938193377327\n",
      "Epoch 105 | Loss = -13.318556701435762\n",
      "Epoch 106 | Loss = -13.347156188067268\n",
      "Epoch 107 | Loss = -12.563034338109633\n",
      "Epoch 108 | Loss = -13.386924968046301\n",
      "Epoch 109 | Loss = -13.19211311901317\n",
      "Epoch 110 | Loss = -13.431022980633903\n",
      "Epoch 111 | Loss = -13.924586183884564\n",
      "Epoch 112 | Loss = -13.248379118302289\n",
      "Epoch 113 | Loss = -14.211625435773064\n",
      "Epoch 114 | Loss = -14.504284073324765\n",
      "Epoch 115 | Loss = -14.63897388121661\n",
      "Epoch 116 | Loss = -14.221651441910687\n",
      "Epoch 117 | Loss = -14.803957041572122\n",
      "Epoch 118 | Loss = -14.0653657913208\n",
      "Epoch 119 | Loss = -14.077123024884392\n",
      "Epoch 120 | Loss = -13.945744598613066\n",
      "Epoch 121 | Loss = -13.70934259190279\n",
      "Epoch 122 | Loss = -15.328797705033246\n",
      "Epoch 123 | Loss = -15.140323218177347\n",
      "Epoch 124 | Loss = -14.819737490485696\n",
      "Epoch 125 | Loss = -14.746302324182848\n",
      "Epoch 126 | Loss = -14.364319745232077\n",
      "Epoch 127 | Loss = -14.60913604848525\n",
      "Epoch 128 | Loss = -14.447712281171013\n",
      "Epoch 129 | Loss = -15.355756030363196\n",
      "Epoch 130 | Loss = -13.821337223052979\n",
      "Epoch 131 | Loss = -13.931582843556123\n",
      "Epoch 132 | Loss = -15.37289681154139\n",
      "Epoch 133 | Loss = -15.07381150301765\n",
      "Epoch 134 | Loss = -15.882897713605095\n",
      "Epoch 135 | Loss = -16.04693022896262\n",
      "Epoch 136 | Loss = -14.307941633112291\n",
      "Epoch 137 | Loss = -15.08073195289163\n",
      "Epoch 138 | Loss = -15.338958123150993\n",
      "Epoch 139 | Loss = -15.2680083583383\n",
      "Epoch 140 | Loss = -14.854262716629925\n",
      "Epoch 141 | Loss = -15.072797354529886\n",
      "Epoch 142 | Loss = -15.528284774107092\n",
      "Epoch 143 | Loss = -15.146914678461412\n",
      "Epoch 144 | Loss = -15.383267641067505\n",
      "Epoch 145 | Loss = -15.493585726794075\n",
      "Epoch 146 | Loss = -15.62028144387638\n",
      "Epoch 147 | Loss = -16.10693827797385\n",
      "Epoch 148 | Loss = -16.068756692549762\n",
      "Epoch 149 | Loss = -15.294383904513191\n",
      "Epoch 150 | Loss = -16.700012739966898\n",
      "Epoch 151 | Loss = -15.476860747617835\n",
      "Epoch 152 | Loss = -15.88075980018167\n",
      "Epoch 153 | Loss = -16.695260272306555\n",
      "Epoch 154 | Loss = -16.212196350097656\n",
      "Epoch 155 | Loss = -16.87895326053395\n",
      "Epoch 156 | Loss = -16.381914194892435\n",
      "Epoch 157 | Loss = -16.228602605707504\n",
      "Epoch 158 | Loss = -16.438304957221536\n",
      "Epoch 159 | Loss = -15.974146057577695\n",
      "Epoch 160 | Loss = -16.403229068307315\n",
      "Epoch 161 | Loss = -16.816676195930032\n",
      "Epoch 162 | Loss = -17.02850355821497\n",
      "Epoch 163 | Loss = -17.70175246631398\n",
      "Epoch 164 | Loss = -16.498820052427405\n",
      "Epoch 165 | Loss = -16.590623322655173\n",
      "Epoch 166 | Loss = -16.10244100234088\n",
      "Epoch 167 | Loss = -17.447366518132828\n",
      "Epoch 168 | Loss = -16.8896985615001\n",
      "Epoch 169 | Loss = -17.222609463860007\n",
      "Epoch 170 | Loss = -17.29604934243595\n",
      "Epoch 171 | Loss = -17.465547786039465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172 | Loss = -16.751004191005933\n",
      "Epoch 173 | Loss = -17.85443364872652\n",
      "Epoch 174 | Loss = -16.79921214720782\n",
      "Epoch 175 | Loss = -17.387923268710864\n",
      "Epoch 176 | Loss = -17.270731617422665\n",
      "Epoch 177 | Loss = -17.680277964648077\n",
      "Epoch 178 | Loss = -17.7014180912691\n",
      "Epoch 179 | Loss = -17.693357804242304\n",
      "Epoch 180 | Loss = -17.262780834646787\n",
      "Epoch 181 | Loss = -17.9941086769104\n",
      "Epoch 182 | Loss = -17.821685875163357\n",
      "Epoch 183 | Loss = -17.265101376701804\n",
      "Epoch 184 | Loss = -17.396604481865378\n",
      "Epoch 185 | Loss = -17.971850843990552\n",
      "Epoch 186 | Loss = -18.96929754930384\n",
      "Epoch 187 | Loss = -18.3002451728372\n",
      "Epoch 188 | Loss = -18.193278733421774\n",
      "Epoch 189 | Loss = -17.97198480718276\n",
      "Epoch 190 | Loss = -18.47174302269431\n",
      "Epoch 191 | Loss = -17.39201800963458\n",
      "Epoch 192 | Loss = -18.82809075187234\n",
      "Epoch 193 | Loss = -19.35253564049216\n",
      "Epoch 194 | Loss = -18.337716467240277\n",
      "Epoch 195 | Loss = -19.225754737854004\n",
      "Epoch 196 | Loss = -18.659628251019647\n",
      "Epoch 197 | Loss = -18.22701658922083\n",
      "Epoch 198 | Loss = -18.62079864389756\n",
      "Epoch 199 | Loss = -18.7824868033914\n",
      "Epoch 200 | Loss = -18.31875442056095\n",
      "+-----------------------------------------------------------------------------+\n",
      "Training Time: 24037.620683431625 seconds.\n",
      "Get Embedding...\n",
      "(118, 64) (118,)\n",
      "SVC Accuracy: [0.7568181818181818, 0.7090909090909091]\n",
      "+-----------------------------------------------------------------------------+\n",
      "Embedding Time: 13.56998085975647 seconds.\n",
      "Experient 0\n",
      "20211217-221138 :Log the record!\n",
      "+-----------------------------------------------------------------------------+\n",
      "Experient 1\n",
      "!!!\n",
      "89\n",
      "!!!\n",
      "[553, 217, 346, 850, 1139, 72, 280, 861, 916, 589, 50, 182, 307, 1067, 172, 583, 831, 791, 1014, 246, 18, 1013, 59, 1023, 984, 69, 755, 65, 71, 742, 1153, 256, 856, 266, 427, 956, 679, 826, 541, 147, 910, 0, 922, 683, 131, 103, 91, 1001, 166, 558, 796, 1078, 811, 852, 351, 1002, 383, 145, 197, 1072, 789, 17, 606, 9, 608, 1111, 664, 414, 154, 290, 954, 667, 959, 1097, 652, 339, 184, 368, 327, 506, 933, 370, 36, 768, 67, 1158, 93, 1123, 47, 624, 311, 883, 484, 702, 379, 316, 794, 822, 818, 55, 375, 1008, 605, 584, 920, 575, 986, 675, 82, 468, 1104, 202, 428, 566, 85, 550, 914, 1142, 545, 1145, 870, 1149, 332, 732, 185, 401, 359, 540, 714, 277, 785, 56, 45, 330, 1137, 564, 941, 703, 157, 1056, 1025, 325, 394, 360, 1077, 611, 1058, 1131, 830, 1081, 857, 1105, 364, 57, 233, 291, 1065, 926, 181, 231, 230, 808, 731, 678, 588, 929, 226, 537, 680, 38, 746, 595, 733, 244, 138, 1039, 441, 216, 762, 220, 284, 570, 1166, 1103, 117, 476, 261, 312, 232, 424, 149, 1110, 183, 651, 776, 522, 898, 381, 1100, 1088, 221, 1043, 596, 1084, 719, 669, 253, 715, 498, 35, 112, 495, 524, 548, 369, 735, 574, 580, 391, 167, 293, 942, 793, 273, 204, 1049, 474, 292, 973, 129, 1006, 402, 444, 975, 137, 1071, 647, 656, 1080, 1045, 333, 648, 555, 1127, 576, 486, 602, 989, 399, 3, 1030, 1016, 782, 153, 962, 421, 620, 1036, 264, 944, 848, 736, 717, 26, 1121, 978, 338, 398, 449, 46, 778, 877, 773, 633, 287, 465, 957, 828, 462, 800, 419, 902, 709, 953, 549, 997, 724, 78, 779, 1019, 2, 1050, 546, 1048, 195, 1061, 995, 1032, 10, 25, 450, 357, 1135, 254, 559, 612, 912, 965, 179, 94, 739, 470, 616, 298, 900, 224, 598, 301, 436, 1099, 234, 1060, 1053, 839, 815, 158, 804, 945, 89, 31, 210, 365, 993, 413, 225, 115, 618, 42, 567, 320, 1125, 644, 532, 337, 513, 168, 609, 243, 494, 186, 734, 1159, 193, 439, 305, 641, 628, 300, 521, 636, 948, 198, 994, 556, 96, 1130, 568, 22, 557, 1021, 132, 938, 707, 463, 723, 1152, 915, 711, 156, 631, 931, 457, 1066, 433, 666, 464, 459, 597, 816, 471, 725, 614, 1063, 397, 716, 684, 176, 227, 577, 1004, 801, 637, 34, 761, 302, 404, 1090, 980, 1028, 262, 161, 426, 751, 617, 1000, 39, 345, 504, 880, 907, 409, 173, 515, 249, 1069, 285, 1156, 730, 350, 260, 150, 281, 923, 361, 1062, 918, 1052, 904, 282, 1095, 219, 503, 738, 88, 668, 563, 710, 946, 207, 146, 60, 340, 760, 691, 106, 827, 265, 571, 15, 1132, 1015, 985, 825, 970, 758, 223, 406, 921, 1044, 772, 120, 247, 4, 542, 101, 859, 467, 1150, 12, 871, 268, 1012, 630, 1009, 336, 322, 621, 491, 1026, 187, 629, 432, 385, 992, 809, 1024, 756, 366, 62, 188, 53, 899, 950, 939, 1175, 988, 13, 105, 692, 659, 160, 960, 134, 875, 492, 107, 505, 757, 932, 653, 533, 175, 342, 314, 1165, 936, 1114, 252, 639, 8, 695, 838, 539, 824, 458, 408, 508, 248, 148, 352, 1162, 663, 911, 578, 318, 1116, 108, 1106, 235, 844, 54, 889, 645, 586, 1068, 239, 326, 763, 689, 104, 81, 323, 48, 1031, 908, 1041, 851, 1124, 452, 961, 122, 127, 1108, 388, 215, 155, 73, 887, 475, 77, 348, 485, 84, 1148, 650, 509, 75, 299, 331, 144, 241, 288, 727, 619, 775, 625, 206, 977, 455, 934, 697, 777, 865, 245, 835, 585, 133, 70, 655, 1161, 1092, 892, 190, 7, 32, 807, 110, 693, 1134, 1064, 295, 19, 872, 729, 1154, 289, 1176, 657, 142, 890, 445, 952, 275, 866, 587, 927, 603, 943, 211, 1118, 728, 593, 701, 477, 222, 196, 440, 205, 531, 981, 354, 478, 165, 242, 958, 1115, 1075, 1074, 1007, 24, 967, 913, 49, 744, 11, 554, 201, 321, 437, 517, 999, 814, 529, 681, 236, 180, 750, 780, 135, 868, 516, 879, 820, 315, 1128, 560, 218, 845, 297, 949, 841, 1059, 309, 1167, 1155, 665, 425, 1172, 461, 812, 79, 143, 622, 983, 937, 438, 353, 832, 6, 901, 795, 990, 759, 869, 1120, 276, 686, 810, 199, 371, 174, 677, 627, 116, 1133, 435, 519, 113, 496, 1029, 706, 255, 764, 130, 1005, 525, 500, 453, 367, 720, 171, 278, 483, 1122, 754, 698, 1051, 599, 1126, 74, 867, 726, 885, 250, 191, 1027, 229, 996, 310, 209, 884, 111, 456, 1082, 76, 358, 343, 66, 356, 213, 874, 1164, 362, 813, 1138, 5, 1055, 324, 888, 480, 849, 1033, 228, 267, 410, 786, 1035, 139, 873, 972, 740, 303, 951, 304, 1140, 543, 682, 382, 393, 896, 488, 718, 447, 1054, 417, 643, 43, 964, 14, 237, 189, 662, 29, 306, 1144, 119, 654, 86, 1107, 783, 511, 876, 1136, 696, 1146, 1086, 507, 341, 649, 771, 169, 274, 37, 376, 317, 279, 395, 840, 966, 251, 687, 878, 787, 159, 600, 1087, 905, 319, 1160, 378, 1094, 263, 52, 987, 344, 308, 363, 416, 396, 924, 769, 688, 33, 615, 460, 979, 906, 121, 895, 1171, 102, 579, 526, 882, 1168, 177, 523, 380, 1076, 864, 1085, 125, 674, 83, 781, 64, 124, 1047, 51, 20, 661, 767, 258, 151, 833, 638, 1109, 590, 635, 1169, 930, 372, 448, 982, 991, 846, 538, 374, 792, 446, 708, 528, 283, 819, 1089, 858, 1038, 806, 466, 493, 1174, 347, 114, 136, 128, 863, 935, 1093, 415, 269, 510, 1129, 834, 1, 389, 68, 749, 963, 552, 487, 430, 212, 971, 847, 745, 194, 162, 713, 296, 1011, 592, 582, 917, 594, 429, 853, 1073, 784, 520, 499, 469, 451, 1079, 572, 862, 753, 737, 27, 1117, 547, 947, 123, 894, 1112, 855, 384, 1034, 422, 490, 712, 843, 192, 1173, 203, 634, 514, 928, 694, 423, 141, 420, 329, 1017, 1119, 377, 1046, 527, 998, 349, 272, 454, 1057, 501, 178, 392, 405, 774, 472, 238, 1042, 534, 699, 482, 1018, 569, 842, 164, 660, 1113, 536, 817, 95, 240, 387, 854, 126, 561, 743, 431, 672, 802, 208, 766, 690, 390, 1101, 591, 604, 109, 909, 803, 28, 286, 765, 163, 601, 1163, 722, 90, 1177, 1037, 152, 1098, 805, 334, 294, 829, 700, 974, 21]\n",
      "Number of Graphs (dataset_train): 1060\n",
      "Number of Graphs (dataset_test): 118\n",
      "+-----------------------------------------------------------------------------+\n",
      "Learning Rate: 0.001\n",
      "Epochs: 200\n",
      "Batch Size: 64\n",
      "Hidden Dimension 512\n",
      "Number of Layer in Encoder: 1\n",
      "Opimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device: cuda\n",
      "Model:\n",
      "SimCLR(\n",
      "  (encoder): Encoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=89, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      ))\n",
      "    )\n",
      "    (bns): ModuleList(\n",
      "      (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Projection_MLP(\n",
      "    (layer1): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (unilayer): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Start Training...\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss = 24.379525780677795\n",
      "Epoch 2 | Loss = 9.791505841647878\n",
      "Epoch 3 | Loss = 3.6425037804771874\n",
      "Epoch 4 | Loss = 0.39650869895430174\n",
      "Epoch 5 | Loss = -0.9712495838894564\n",
      "Epoch 6 | Loss = -4.182826589135563\n",
      "Epoch 7 | Loss = -6.062772666706758\n",
      "Epoch 8 | Loss = -6.606429787243114\n",
      "Epoch 9 | Loss = -8.355670897399678\n",
      "Epoch 10 | Loss = -9.270649082520428\n",
      "Epoch 11 | Loss = -9.983923337038826\n",
      "Epoch 12 | Loss = -11.840173609116498\n",
      "Epoch 13 | Loss = -13.400305074803969\n",
      "Epoch 14 | Loss = -12.78390890009263\n",
      "Epoch 15 | Loss = -14.611576304716223\n",
      "Epoch 16 | Loss = -15.611377463621253\n",
      "Epoch 17 | Loss = -16.746194811428296\n",
      "Epoch 18 | Loss = -17.014984775992\n",
      "Epoch 19 | Loss = -17.761953045340146\n",
      "Epoch 20 | Loss = -17.815295724307788\n",
      "Epoch 21 | Loss = -18.665145733777216\n",
      "Epoch 22 | Loss = -20.01780543607824\n",
      "Epoch 23 | Loss = -19.532046346103442\n",
      "Epoch 24 | Loss = -21.558541269863355\n",
      "Epoch 25 | Loss = -21.120568555944107\n",
      "Epoch 26 | Loss = -21.748138596029843\n",
      "Epoch 27 | Loss = -21.60185945735258\n",
      "Epoch 28 | Loss = -22.106422003577737\n",
      "Epoch 29 | Loss = -22.717484137591192\n",
      "Epoch 30 | Loss = -23.28280216104844\n",
      "Epoch 31 | Loss = -23.400833017685834\n",
      "Epoch 32 | Loss = -24.66710163565243\n",
      "Epoch 33 | Loss = -24.962860303766586\n",
      "Epoch 34 | Loss = -25.732900310965146\n",
      "Epoch 35 | Loss = -25.805386403027704\n",
      "Epoch 36 | Loss = -26.51174576142255\n",
      "Epoch 37 | Loss = -25.853949546813965\n",
      "Epoch 38 | Loss = -26.572444775525263\n",
      "Epoch 39 | Loss = -27.278733197380514\n",
      "Epoch 40 | Loss = -28.27372413523057\n",
      "Epoch 41 | Loss = -27.308926806730383\n",
      "Epoch 42 | Loss = -28.09492251452278\n",
      "Epoch 43 | Loss = -28.49567643333884\n",
      "Epoch 44 | Loss = -28.537781210506665\n",
      "Epoch 45 | Loss = -28.994732155519372\n",
      "Epoch 46 | Loss = -29.40940554001752\n",
      "Epoch 47 | Loss = -29.44898754007676\n",
      "Epoch 48 | Loss = -29.694204246296604\n",
      "Epoch 49 | Loss = -30.56280615750481\n",
      "Epoch 50 | Loss = -30.747978547040155\n",
      "Epoch 51 | Loss = -30.62854797699872\n",
      "Epoch 52 | Loss = -31.29291778452256\n",
      "Epoch 53 | Loss = -31.33386791453642\n",
      "Epoch 54 | Loss = -31.642952890957105\n",
      "Epoch 55 | Loss = -31.713661109699924\n",
      "Epoch 56 | Loss = -32.009026415207806\n",
      "Epoch 57 | Loss = -32.06917566411636\n",
      "Epoch 58 | Loss = -32.0911177186405\n",
      "Epoch 59 | Loss = -32.713861493503344\n",
      "Epoch 60 | Loss = -33.01253206589643\n",
      "Epoch 61 | Loss = -33.38052648656509\n",
      "Epoch 62 | Loss = -33.81173863130457\n",
      "Epoch 63 | Loss = -34.27091873393339\n",
      "Epoch 64 | Loss = -34.304130413953\n",
      "Epoch 65 | Loss = -33.894957318025476\n",
      "Epoch 66 | Loss = -34.13611773883595\n",
      "Epoch 67 | Loss = -34.7078545514275\n",
      "Epoch 68 | Loss = -34.76459918302648\n",
      "Epoch 69 | Loss = -34.80794957104851\n",
      "Epoch 70 | Loss = -34.78576691010419\n",
      "Epoch 71 | Loss = -35.51204902985517\n",
      "Epoch 72 | Loss = -35.70428497651044\n",
      "Epoch 73 | Loss = -36.0981624547173\n",
      "Epoch 74 | Loss = -35.61744659087237\n",
      "Epoch 75 | Loss = -35.71946382522583\n",
      "Epoch 76 | Loss = -36.88243335836074\n",
      "Epoch 77 | Loss = -36.14843160965864\n",
      "Epoch 78 | Loss = -36.37201067980598\n",
      "Epoch 79 | Loss = -35.85148084864897\n",
      "Epoch 80 | Loss = -38.185216174406165\n",
      "Epoch 81 | Loss = -37.15019812303431\n",
      "Epoch 82 | Loss = -37.53708065257353\n",
      "Epoch 83 | Loss = -37.57858385759241\n",
      "Epoch 84 | Loss = -36.71148518955006\n",
      "Epoch 85 | Loss = -38.27142973507152\n",
      "Epoch 86 | Loss = -37.856120979084686\n",
      "Epoch 87 | Loss = -38.05032716077917\n",
      "Epoch 88 | Loss = -38.366615155163935\n",
      "Epoch 89 | Loss = -38.20526698056389\n",
      "Epoch 90 | Loss = -38.38576846964219\n",
      "Epoch 91 | Loss = -39.541266188902014\n",
      "Epoch 92 | Loss = -39.11357795490938\n",
      "Epoch 93 | Loss = -39.722364622003894\n",
      "Epoch 94 | Loss = -39.58086319530712\n",
      "Epoch 95 | Loss = -39.3666036269244\n",
      "Epoch 96 | Loss = -39.58257554559147\n",
      "Epoch 97 | Loss = -39.764513548682714\n",
      "Epoch 98 | Loss = -39.568107324488025\n",
      "Epoch 99 | Loss = -39.946281881893384\n",
      "Epoch 100 | Loss = -39.97456090590533\n",
      "Epoch 101 | Loss = -39.8135676944957\n",
      "Epoch 102 | Loss = -41.12498286191155\n",
      "Epoch 103 | Loss = -40.89923580955057\n",
      "Epoch 104 | Loss = -41.15830432667452\n",
      "Epoch 105 | Loss = -41.109766371109906\n",
      "Epoch 106 | Loss = -41.502611609066236\n",
      "Epoch 107 | Loss = -41.08553398356718\n",
      "Epoch 108 | Loss = -41.464970364290124\n",
      "Epoch 109 | Loss = -41.00986104853013\n",
      "Epoch 110 | Loss = -41.663000779993396\n",
      "Epoch 111 | Loss = -42.19325318055994\n",
      "Epoch 112 | Loss = -42.378467587863696\n",
      "Epoch 113 | Loss = -42.10308453615974\n",
      "Epoch 114 | Loss = -41.82227636786068\n",
      "Epoch 115 | Loss = -41.433700785917395\n",
      "Epoch 116 | Loss = -42.609037427341235\n",
      "Epoch 117 | Loss = -42.28606978584738\n",
      "Epoch 118 | Loss = -42.63814081865198\n",
      "Epoch 119 | Loss = -42.81784425062292\n",
      "Epoch 120 | Loss = -43.14311622170841\n",
      "Epoch 121 | Loss = -43.133356262655816\n",
      "Epoch 122 | Loss = -43.468855885898364\n",
      "Epoch 123 | Loss = -43.44811111337998\n",
      "Epoch 124 | Loss = -42.943094477934\n",
      "Epoch 125 | Loss = -43.25184827692368\n",
      "Epoch 126 | Loss = -43.350159504834345\n",
      "Epoch 127 | Loss = -43.66644836874569\n",
      "Epoch 128 | Loss = -43.75168489007389\n",
      "Epoch 129 | Loss = -43.26211101868574\n",
      "Epoch 130 | Loss = -44.14220358343685\n",
      "Epoch 131 | Loss = -44.425323991214526\n",
      "Epoch 132 | Loss = -44.120917684891644\n",
      "Epoch 133 | Loss = -44.667965692632336\n",
      "Epoch 134 | Loss = -44.62886630787569\n",
      "Epoch 135 | Loss = -44.91928619496963\n",
      "Epoch 136 | Loss = -44.57909295138191\n",
      "Epoch 137 | Loss = -44.652760842267206\n",
      "Epoch 138 | Loss = -45.13882522022023\n",
      "Epoch 139 | Loss = -44.96408832774443\n",
      "Epoch 140 | Loss = -45.375873004688934\n",
      "Epoch 141 | Loss = -45.205236462985766\n",
      "Epoch 142 | Loss = -45.22979388517492\n",
      "Epoch 143 | Loss = -45.044598411111274\n",
      "Epoch 144 | Loss = -45.20363249498255\n",
      "Epoch 145 | Loss = -45.11852685142966\n",
      "Epoch 146 | Loss = -45.614503748276654\n",
      "Epoch 147 | Loss = -45.70581102371216\n",
      "Epoch 148 | Loss = -45.56395732655245\n",
      "Epoch 149 | Loss = -45.63607782476089\n",
      "Epoch 150 | Loss = -45.836127028745764\n",
      "Epoch 151 | Loss = -46.038898439968335\n",
      "Epoch 152 | Loss = -46.26080675686107\n",
      "Epoch 153 | Loss = -46.27946329116821\n",
      "Epoch 154 | Loss = -46.09501877953024\n",
      "Epoch 155 | Loss = -46.73869741664213\n",
      "Epoch 156 | Loss = -46.53790473937988\n",
      "Epoch 157 | Loss = -46.93002459582161\n",
      "Epoch 158 | Loss = -46.88191915960873\n",
      "Epoch 159 | Loss = -46.87053778592278\n",
      "Epoch 160 | Loss = -46.7830379710478\n",
      "Epoch 161 | Loss = -46.70803507636575\n",
      "Epoch 162 | Loss = -46.99395836100859\n",
      "Epoch 163 | Loss = -46.829475823570704\n",
      "Epoch 164 | Loss = -47.1593809969285\n",
      "Epoch 165 | Loss = -47.031030879301184\n",
      "Epoch 166 | Loss = -46.7292041498072\n",
      "Epoch 167 | Loss = -47.34765033160939\n",
      "Epoch 168 | Loss = -47.12493223302505\n",
      "Epoch 169 | Loss = -47.35928128747379\n",
      "Epoch 170 | Loss = -47.870074917288385\n",
      "Epoch 171 | Loss = -47.95663286657894\n",
      "Epoch 172 | Loss = -48.133464729084686\n",
      "Epoch 173 | Loss = -47.58028835408828\n",
      "Epoch 174 | Loss = -47.79413439245785\n",
      "Epoch 175 | Loss = -48.046595096588135\n",
      "Epoch 176 | Loss = -48.12378616893993\n",
      "Epoch 177 | Loss = -47.98957718119902\n",
      "Epoch 178 | Loss = -47.902379007900464\n",
      "Epoch 179 | Loss = -48.18945191888248\n",
      "Epoch 180 | Loss = -48.11498832702637\n",
      "Epoch 181 | Loss = -48.08407918144675\n",
      "Epoch 182 | Loss = -48.24998235702515\n",
      "Epoch 183 | Loss = -48.769974400015435\n",
      "Epoch 184 | Loss = -48.44709884419161\n",
      "Epoch 185 | Loss = -48.673303491929\n",
      "Epoch 186 | Loss = -48.80228715784409\n",
      "Epoch 187 | Loss = -48.27535783543306\n",
      "Epoch 188 | Loss = -48.62767603818108\n",
      "Epoch 189 | Loss = -48.80499977224014\n",
      "Epoch 190 | Loss = -49.07786433836993\n",
      "Epoch 191 | Loss = -48.841024959788605\n",
      "Epoch 192 | Loss = -48.92532458024866\n",
      "Epoch 193 | Loss = -49.10435140834135\n",
      "Epoch 194 | Loss = -49.32212094699635\n",
      "Epoch 195 | Loss = -49.518851420458624\n",
      "Epoch 196 | Loss = -49.297100712271295\n",
      "Epoch 197 | Loss = -49.217258369221405\n",
      "Epoch 198 | Loss = -49.51565318949082\n",
      "Epoch 199 | Loss = -49.07275530871223\n",
      "Epoch 200 | Loss = -49.529530216665826\n",
      "+-----------------------------------------------------------------------------+\n",
      "Training Time: 24083.271344423294 seconds.\n",
      "Get Embedding...\n",
      "(118, 512) (118,)\n",
      "SVC Accuracy: [0.7803030303030303, 0.7537878787878788]\n",
      "+-----------------------------------------------------------------------------+\n",
      "Embedding Time: 14.092910051345825 seconds.\n",
      "Experient 1\n",
      "20211218-045316 :Log the record!\n",
      "+-----------------------------------------------------------------------------+\n",
      "Experient 2\n",
      "!!!\n",
      "89\n",
      "!!!\n",
      "[553, 217, 346, 850, 1139, 72, 280, 861, 916, 589, 50, 182, 307, 1067, 172, 583, 831, 791, 1014, 246, 18, 1013, 59, 1023, 984, 69, 755, 65, 71, 742, 1153, 256, 856, 266, 427, 956, 679, 826, 541, 147, 910, 0, 922, 683, 131, 103, 91, 1001, 166, 558, 796, 1078, 811, 852, 351, 1002, 383, 145, 197, 1072, 789, 17, 606, 9, 608, 1111, 664, 414, 154, 290, 954, 667, 959, 1097, 652, 339, 184, 368, 327, 506, 933, 370, 36, 768, 67, 1158, 93, 1123, 47, 624, 311, 883, 484, 702, 379, 316, 794, 822, 818, 55, 375, 1008, 605, 584, 920, 575, 986, 675, 82, 468, 1104, 202, 428, 566, 85, 550, 914, 1142, 545, 1145, 870, 1149, 332, 732, 185, 401, 359, 540, 714, 277, 785, 56, 45, 330, 1137, 564, 941, 703, 157, 1056, 1025, 325, 394, 360, 1077, 611, 1058, 1131, 830, 1081, 857, 1105, 364, 57, 233, 291, 1065, 926, 181, 231, 230, 808, 731, 678, 588, 929, 226, 537, 680, 38, 746, 595, 733, 244, 138, 1039, 441, 216, 762, 220, 284, 570, 1166, 1103, 117, 476, 261, 312, 232, 424, 149, 1110, 183, 651, 776, 522, 898, 381, 1100, 1088, 221, 1043, 596, 1084, 719, 669, 253, 715, 498, 35, 112, 495, 524, 548, 369, 735, 574, 580, 391, 167, 293, 942, 793, 273, 204, 1049, 474, 292, 973, 129, 1006, 402, 444, 975, 137, 1071, 647, 656, 1080, 1045, 333, 648, 555, 1127, 576, 486, 602, 989, 399, 3, 1030, 1016, 782, 153, 962, 421, 620, 1036, 264, 944, 848, 736, 717, 26, 1121, 978, 338, 398, 449, 46, 778, 877, 773, 633, 287, 465, 957, 828, 462, 800, 419, 902, 709, 953, 549, 997, 724, 78, 779, 1019, 2, 1050, 546, 1048, 195, 1061, 995, 1032, 10, 25, 450, 357, 1135, 254, 559, 612, 912, 965, 179, 94, 739, 470, 616, 298, 900, 224, 598, 301, 436, 1099, 234, 1060, 1053, 839, 815, 158, 804, 945, 89, 31, 210, 365, 993, 413, 225, 115, 618, 42, 567, 320, 1125, 644, 532, 337, 513, 168, 609, 243, 494, 186, 734, 1159, 193, 439, 305, 641, 628, 300, 521, 636, 948, 198, 994, 556, 96, 1130, 568, 22, 557, 1021, 132, 938, 707, 463, 723, 1152, 915, 711, 156, 631, 931, 457, 1066, 433, 666, 464, 459, 597, 816, 471, 725, 614, 1063, 397, 716, 684, 176, 227, 577, 1004, 801, 637, 34, 761, 302, 404, 1090, 980, 1028, 262, 161, 426, 751, 617, 1000, 39, 345, 504, 880, 907, 409, 173, 515, 249, 1069, 285, 1156, 730, 350, 260, 150, 281, 923, 361, 1062, 918, 1052, 904, 282, 1095, 219, 503, 738, 88, 668, 563, 710, 946, 207, 146, 60, 340, 760, 691, 106, 827, 265, 571, 15, 1132, 1015, 985, 825, 970, 758, 223, 406, 921, 1044, 772, 120, 247, 4, 542, 101, 859, 467, 1150, 12, 871, 268, 1012, 630, 1009, 336, 322, 621, 491, 1026, 187, 629, 432, 385, 992, 809, 1024, 756, 366, 62, 188, 53, 899, 950, 939, 1175, 988, 13, 105, 692, 659, 160, 960, 134, 875, 492, 107, 505, 757, 932, 653, 533, 175, 342, 314, 1165, 936, 1114, 252, 639, 8, 695, 838, 539, 824, 458, 408, 508, 248, 148, 352, 1162, 663, 911, 578, 318, 1116, 108, 1106, 235, 844, 54, 889, 645, 586, 1068, 239, 326, 763, 689, 104, 81, 323, 48, 1031, 908, 1041, 851, 1124, 452, 961, 122, 127, 1108, 388, 215, 155, 73, 887, 475, 77, 348, 485, 84, 1148, 650, 509, 75, 299, 331, 144, 241, 288, 727, 619, 775, 625, 206, 977, 455, 934, 697, 777, 865, 245, 835, 585, 133, 70, 655, 1161, 1092, 892, 190, 7, 32, 807, 110, 693, 1134, 1064, 295, 19, 872, 729, 1154, 289, 1176, 657, 142, 890, 445, 952, 275, 866, 587, 927, 603, 943, 211, 1118, 728, 593, 701, 477, 222, 196, 440, 205, 531, 981, 354, 478, 165, 242, 958, 1115, 1075, 1074, 1007, 24, 967, 913, 49, 744, 11, 554, 201, 321, 437, 517, 999, 814, 529, 681, 236, 180, 750, 780, 135, 868, 516, 879, 820, 315, 1128, 560, 218, 845, 297, 949, 841, 1059, 309, 1167, 1155, 665, 425, 1172, 461, 812, 79, 143, 622, 983, 937, 438, 353, 832, 6, 901, 795, 990, 759, 869, 1120, 276, 686, 810, 199, 371, 174, 677, 627, 116, 1133, 435, 519, 113, 496, 1029, 706, 255, 764, 130, 1005, 525, 500, 453, 367, 720, 171, 278, 483, 1122, 754, 698, 1051, 599, 1126, 74, 867, 726, 885, 250, 191, 1027, 229, 996, 310, 209, 884, 111, 456, 1082, 76, 358, 343, 66, 356, 213, 874, 1164, 362, 813, 1138, 5, 1055, 324, 888, 480, 849, 1033, 228, 267, 410, 786, 1035, 139, 873, 972, 740, 303, 951, 304, 1140, 543, 682, 382, 393, 896, 488, 718, 447, 1054, 417, 643, 43, 964, 14, 237, 189, 662, 29, 306, 1144, 119, 654, 86, 1107, 783, 511, 876, 1136, 696, 1146, 1086, 507, 341, 649, 771, 169, 274, 37, 376, 317, 279, 395, 840, 966, 251, 687, 878, 787, 159, 600, 1087, 905, 319, 1160, 378, 1094, 263, 52, 987, 344, 308, 363, 416, 396, 924, 769, 688, 33, 615, 460, 979, 906, 121, 895, 1171, 102, 579, 526, 882, 1168, 177, 523, 380, 1076, 864, 1085, 125, 674, 83, 781, 64, 124, 1047, 51, 20, 661, 767, 258, 151, 833, 638, 1109, 590, 635, 1169, 930, 372, 448, 982, 991, 846, 538, 374, 792, 446, 708, 528, 283, 819, 1089, 858, 1038, 806, 466, 493, 1174, 347, 114, 136, 128, 863, 935, 1093, 415, 269, 510, 1129, 834, 1, 389, 68, 749, 963, 552, 487, 430, 212, 971, 847, 745, 194, 162, 713, 296, 1011, 592, 582, 917, 594, 429, 853, 1073, 784, 520, 499, 469, 451, 1079, 572, 862, 753, 737, 27, 1117, 547, 947, 123, 894, 1112, 855, 384, 1034, 422, 490, 712, 843, 192, 1173, 203, 634, 514, 928, 694, 423, 141, 420, 329, 1017, 1119, 377, 1046, 527, 998, 349, 272, 454, 1057, 501, 178, 392, 405, 774, 472, 238, 1042, 534, 699, 482, 1018, 569, 842, 164, 660, 1113, 536, 817, 95, 240, 387, 854, 126, 561, 743, 431, 672, 802, 208, 766, 690, 390, 1101, 591, 604, 109, 909, 803, 28, 286, 765, 163, 601, 1163, 722, 90, 1177, 1037, 152, 1098, 805, 334, 294, 829, 700, 974, 21]\n",
      "Number of Graphs (dataset_train): 1060\n",
      "Number of Graphs (dataset_test): 118\n",
      "+-----------------------------------------------------------------------------+\n",
      "Learning Rate: 0.001\n",
      "Epochs: 200\n",
      "Batch Size: 64\n",
      "Hidden Dimension 64\n",
      "Number of Layer in Encoder: 2\n",
      "Opimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device: cuda\n",
      "Model:\n",
      "SimCLR(\n",
      "  (encoder): Encoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=89, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      ))\n",
      "      (1): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      ))\n",
      "    )\n",
      "    (bns): ModuleList(\n",
      "      (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Projection_MLP(\n",
      "    (layer1): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (unilayer): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Start Training...\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss = 63.254477098584175\n",
      "Epoch 2 | Loss = 42.11401405930519\n",
      "Epoch 3 | Loss = 31.53990664201624\n",
      "Epoch 4 | Loss = 26.76424670570037\n",
      "Epoch 5 | Loss = 22.208787027527304\n",
      "Epoch 6 | Loss = 16.52180516018587\n",
      "Epoch 7 | Loss = 15.832263483720666\n",
      "Epoch 8 | Loss = 11.782349263920503\n",
      "Epoch 9 | Loss = 10.09569398094626\n",
      "Epoch 10 | Loss = 6.399367823320277\n",
      "Epoch 11 | Loss = 5.391173285596511\n",
      "Epoch 12 | Loss = 4.590899095815771\n",
      "Epoch 13 | Loss = 3.0036646723747253\n",
      "Epoch 14 | Loss = 2.445540224804598\n",
      "Epoch 15 | Loss = 0.9956510803278755\n",
      "Epoch 16 | Loss = 0.566311355899362\n",
      "Epoch 17 | Loss = -1.9842631536371567\n",
      "Epoch 18 | Loss = -2.3606169504277847\n",
      "Epoch 19 | Loss = -2.3248643734875847\n",
      "Epoch 20 | Loss = -2.920205940218533\n",
      "Epoch 21 | Loss = -4.33215676686343\n",
      "Epoch 22 | Loss = -4.9028659287620995\n",
      "Epoch 23 | Loss = -4.642125858980067\n",
      "Epoch 24 | Loss = -6.128605039680705\n",
      "Epoch 25 | Loss = -5.675639780128703\n",
      "Epoch 26 | Loss = -6.491321823176215\n",
      "Epoch 27 | Loss = -7.230420480756199\n",
      "Epoch 28 | Loss = -8.09342617147109\n",
      "Epoch 29 | Loss = -7.29973164025475\n",
      "Epoch 30 | Loss = -8.994184690363268\n",
      "Epoch 31 | Loss = -8.709804899552289\n",
      "Epoch 32 | Loss = -8.954846620559692\n",
      "Epoch 33 | Loss = -9.07731136153726\n",
      "Epoch 34 | Loss = -10.305199076147641\n",
      "Epoch 35 | Loss = -10.051556208554436\n",
      "Epoch 36 | Loss = -10.49191714735592\n",
      "Epoch 37 | Loss = -11.414491344900693\n",
      "Epoch 38 | Loss = -12.16597453285666\n",
      "Epoch 39 | Loss = -12.474469184875488\n",
      "Epoch 40 | Loss = -12.172352201798383\n",
      "Epoch 41 | Loss = -12.469658122343176\n",
      "Epoch 42 | Loss = -11.878899798673743\n",
      "Epoch 43 | Loss = -12.299033557667451\n",
      "Epoch 44 | Loss = -12.808456729440127\n",
      "Epoch 45 | Loss = -13.34115712782916\n",
      "Epoch 46 | Loss = -14.169388153973747\n",
      "Epoch 47 | Loss = -15.168681677650003\n",
      "Epoch 48 | Loss = -14.675316165475284\n",
      "Epoch 49 | Loss = -14.6030712408178\n",
      "Epoch 50 | Loss = -13.768954781924977\n",
      "Epoch 51 | Loss = -15.138217140646542\n",
      "Epoch 52 | Loss = -14.962269698872285\n",
      "Epoch 53 | Loss = -15.592902323778938\n",
      "Epoch 54 | Loss = -15.71782647862154\n",
      "Epoch 55 | Loss = -16.150507169611313\n",
      "Epoch 56 | Loss = -15.614637739518109\n",
      "Epoch 57 | Loss = -15.34859172035666\n",
      "Epoch 58 | Loss = -16.753357803120334\n",
      "Epoch 59 | Loss = -15.65355084924137\n",
      "Epoch 60 | Loss = -15.889898272121654\n",
      "Epoch 61 | Loss = -16.768516035640943\n",
      "Epoch 62 | Loss = -17.273092157700482\n",
      "Epoch 63 | Loss = -17.012384863460767\n",
      "Epoch 64 | Loss = -17.37181531681734\n",
      "Epoch 65 | Loss = -17.67002840603099\n",
      "Epoch 66 | Loss = -17.224459002999698\n",
      "Epoch 67 | Loss = -17.151599126703598\n",
      "Epoch 68 | Loss = -18.92945146560669\n",
      "Epoch 69 | Loss = -19.106407894807703\n",
      "Epoch 70 | Loss = -17.947036350474637\n",
      "Epoch 71 | Loss = -18.405548600589526\n",
      "Epoch 72 | Loss = -18.816125449012308\n",
      "Epoch 73 | Loss = -18.545970103319952\n",
      "Epoch 74 | Loss = -19.460606266470517\n",
      "Epoch 75 | Loss = -17.762659493614645\n",
      "Epoch 76 | Loss = -19.6841776230756\n",
      "Epoch 77 | Loss = -19.774327502531165\n",
      "Epoch 78 | Loss = -19.306006459628833\n",
      "Epoch 79 | Loss = -19.689631377949436\n",
      "Epoch 80 | Loss = -19.639786131241742\n",
      "Epoch 81 | Loss = -20.130193626179416\n",
      "Epoch 82 | Loss = -19.619726181030273\n",
      "Epoch 83 | Loss = -20.33004903793335\n",
      "Epoch 84 | Loss = -20.452675118165857\n",
      "Epoch 85 | Loss = -19.91411012761733\n",
      "Epoch 86 | Loss = -21.331427433911493\n",
      "Epoch 87 | Loss = -20.84766093422385\n",
      "Epoch 88 | Loss = -20.41463865953333\n",
      "Epoch 89 | Loss = -19.82737689859727\n",
      "Epoch 90 | Loss = -20.4260677169351\n",
      "Epoch 91 | Loss = -21.724259544821347\n",
      "Epoch 92 | Loss = -21.44875223496381\n",
      "Epoch 93 | Loss = -21.926398585824405\n",
      "Epoch 94 | Loss = -21.374748622669895\n",
      "Epoch 95 | Loss = -21.476754833670224\n",
      "Epoch 96 | Loss = -22.195377041311826\n",
      "Epoch 97 | Loss = -22.310805376838236\n",
      "Epoch 98 | Loss = -21.056187517502728\n",
      "Epoch 99 | Loss = -21.967904679915485\n",
      "Epoch 100 | Loss = -22.004558563232422\n",
      "Epoch 101 | Loss = -21.95586151235244\n",
      "Epoch 102 | Loss = -22.79524864869959\n",
      "Epoch 103 | Loss = -22.524614698746625\n",
      "Epoch 104 | Loss = -22.583953997668097\n",
      "Epoch 105 | Loss = -23.122451810275805\n",
      "Epoch 106 | Loss = -23.69234794728896\n",
      "Epoch 107 | Loss = -23.356513864853802\n",
      "Epoch 108 | Loss = -22.39165547314812\n",
      "Epoch 109 | Loss = -22.597008929533118\n",
      "Epoch 110 | Loss = -23.46245319703046\n",
      "Epoch 111 | Loss = -22.681985574610092\n",
      "Epoch 112 | Loss = -22.43273852853214\n",
      "Epoch 113 | Loss = -23.579872271593878\n",
      "Epoch 114 | Loss = -22.635724768919104\n",
      "Epoch 115 | Loss = -23.232540046467502\n",
      "Epoch 116 | Loss = -24.399408480700323\n",
      "Epoch 117 | Loss = -23.592824739568375\n",
      "Epoch 118 | Loss = -23.547459377962\n",
      "Epoch 119 | Loss = -24.02776331060073\n",
      "Epoch 120 | Loss = -24.091478488024542\n",
      "Epoch 121 | Loss = -23.558671025668872\n",
      "Epoch 122 | Loss = -23.652935112223904\n",
      "Epoch 123 | Loss = -23.818261988022748\n",
      "Epoch 124 | Loss = -24.67970073924345\n",
      "Epoch 125 | Loss = -24.129942613489487\n",
      "Epoch 126 | Loss = -24.174327766194065\n",
      "Epoch 127 | Loss = -24.50113821029663\n",
      "Epoch 128 | Loss = -25.635880077586453\n",
      "Epoch 129 | Loss = -25.23710158291985\n",
      "Epoch 130 | Loss = -25.24081266627592\n",
      "Epoch 131 | Loss = -25.208619370180017\n",
      "Epoch 132 | Loss = -25.022053466123694\n",
      "Epoch 133 | Loss = -24.97837781906128\n",
      "Epoch 134 | Loss = -25.022655991946948\n",
      "Epoch 135 | Loss = -25.204949322868796\n",
      "Epoch 136 | Loss = -25.311879438512467\n",
      "Epoch 137 | Loss = -24.874350491692038\n",
      "Epoch 138 | Loss = -24.542131592245664\n",
      "Epoch 139 | Loss = -24.977805558372946\n",
      "Epoch 140 | Loss = -24.81990000780891\n",
      "Epoch 141 | Loss = -25.356025247012866\n",
      "Epoch 142 | Loss = -25.685468757853787\n",
      "Epoch 143 | Loss = -26.624788817237405\n",
      "Epoch 144 | Loss = -25.760812703300925\n",
      "Epoch 145 | Loss = -25.940624882193173\n",
      "Epoch 146 | Loss = -25.93414892869837\n",
      "Epoch 147 | Loss = -26.219993843751794\n",
      "Epoch 148 | Loss = -25.90739168840296\n",
      "Epoch 149 | Loss = -25.813968153560864\n",
      "Epoch 150 | Loss = -26.598164979149313\n",
      "Epoch 151 | Loss = -25.75445343466366\n",
      "Epoch 152 | Loss = -25.758541471817914\n",
      "Epoch 153 | Loss = -26.365341663360596\n",
      "Epoch 154 | Loss = -26.031632395351636\n",
      "Epoch 155 | Loss = -26.439815689535703\n",
      "Epoch 156 | Loss = -26.25657263924094\n",
      "Epoch 157 | Loss = -27.83080633948831\n",
      "Epoch 158 | Loss = -27.375866525313434\n",
      "Epoch 159 | Loss = -26.84528185339535\n",
      "Epoch 160 | Loss = -27.047517467947568\n",
      "Epoch 161 | Loss = -26.973106328178854\n",
      "Epoch 162 | Loss = -27.547543497646558\n",
      "Epoch 163 | Loss = -27.235843153560864\n",
      "Epoch 164 | Loss = -26.478203773498535\n",
      "Epoch 165 | Loss = -27.271603107452393\n",
      "Epoch 166 | Loss = -26.424620796652402\n",
      "Epoch 167 | Loss = -26.510972752290613\n",
      "Epoch 168 | Loss = -27.449317932128906\n",
      "Epoch 169 | Loss = -26.975992427152747\n",
      "Epoch 170 | Loss = -26.68975005430334\n",
      "Epoch 171 | Loss = -27.365662995506735\n",
      "Epoch 172 | Loss = -26.49686872257906\n",
      "Epoch 173 | Loss = -26.801132959478043\n",
      "Epoch 174 | Loss = -27.489160341375015\n",
      "Epoch 175 | Loss = -27.535335400525263\n",
      "Epoch 176 | Loss = -27.78725478228401\n",
      "Epoch 177 | Loss = -28.233500901390524\n",
      "Epoch 178 | Loss = -27.121588426477768\n",
      "Epoch 179 | Loss = -27.899173371932086\n",
      "Epoch 180 | Loss = -27.944641758413876\n",
      "Epoch 181 | Loss = -28.438066622790167\n",
      "Epoch 182 | Loss = -28.089551336625043\n",
      "Epoch 183 | Loss = -28.515213629778692\n",
      "Epoch 184 | Loss = -28.044215959661148\n",
      "Epoch 185 | Loss = -27.660377446342917\n",
      "Epoch 186 | Loss = -28.77075372022741\n",
      "Epoch 187 | Loss = -27.59780729518217\n",
      "Epoch 188 | Loss = -28.245528922361487\n",
      "Epoch 189 | Loss = -28.966370274038876\n",
      "Epoch 190 | Loss = -28.922619567197913\n",
      "Epoch 191 | Loss = -28.134846322676715\n",
      "Epoch 192 | Loss = -28.882485109217026\n",
      "Epoch 193 | Loss = -28.691747441011316\n",
      "Epoch 194 | Loss = -28.76803156908821\n",
      "Epoch 195 | Loss = -28.519785404205322\n",
      "Epoch 196 | Loss = -27.673289439257452\n",
      "Epoch 197 | Loss = -27.889331424937527\n",
      "Epoch 198 | Loss = -28.729770492104922\n",
      "Epoch 199 | Loss = -29.15163822735057\n",
      "Epoch 200 | Loss = -29.480102819554947\n",
      "+-----------------------------------------------------------------------------+\n",
      "Training Time: 24100.477415323257 seconds.\n",
      "Get Embedding...\n",
      "(118, 128) (118,)\n",
      "SVC Accuracy: [0.7545454545454546, 0.7348484848484849]\n",
      "+-----------------------------------------------------------------------------+\n",
      "Embedding Time: 13.493553161621094 seconds.\n",
      "Experient 2\n",
      "20211218-113510 :Log the record!\n",
      "+-----------------------------------------------------------------------------+\n",
      "Experient 3\n",
      "!!!\n",
      "89\n",
      "!!!\n",
      "[553, 217, 346, 850, 1139, 72, 280, 861, 916, 589, 50, 182, 307, 1067, 172, 583, 831, 791, 1014, 246, 18, 1013, 59, 1023, 984, 69, 755, 65, 71, 742, 1153, 256, 856, 266, 427, 956, 679, 826, 541, 147, 910, 0, 922, 683, 131, 103, 91, 1001, 166, 558, 796, 1078, 811, 852, 351, 1002, 383, 145, 197, 1072, 789, 17, 606, 9, 608, 1111, 664, 414, 154, 290, 954, 667, 959, 1097, 652, 339, 184, 368, 327, 506, 933, 370, 36, 768, 67, 1158, 93, 1123, 47, 624, 311, 883, 484, 702, 379, 316, 794, 822, 818, 55, 375, 1008, 605, 584, 920, 575, 986, 675, 82, 468, 1104, 202, 428, 566, 85, 550, 914, 1142, 545, 1145, 870, 1149, 332, 732, 185, 401, 359, 540, 714, 277, 785, 56, 45, 330, 1137, 564, 941, 703, 157, 1056, 1025, 325, 394, 360, 1077, 611, 1058, 1131, 830, 1081, 857, 1105, 364, 57, 233, 291, 1065, 926, 181, 231, 230, 808, 731, 678, 588, 929, 226, 537, 680, 38, 746, 595, 733, 244, 138, 1039, 441, 216, 762, 220, 284, 570, 1166, 1103, 117, 476, 261, 312, 232, 424, 149, 1110, 183, 651, 776, 522, 898, 381, 1100, 1088, 221, 1043, 596, 1084, 719, 669, 253, 715, 498, 35, 112, 495, 524, 548, 369, 735, 574, 580, 391, 167, 293, 942, 793, 273, 204, 1049, 474, 292, 973, 129, 1006, 402, 444, 975, 137, 1071, 647, 656, 1080, 1045, 333, 648, 555, 1127, 576, 486, 602, 989, 399, 3, 1030, 1016, 782, 153, 962, 421, 620, 1036, 264, 944, 848, 736, 717, 26, 1121, 978, 338, 398, 449, 46, 778, 877, 773, 633, 287, 465, 957, 828, 462, 800, 419, 902, 709, 953, 549, 997, 724, 78, 779, 1019, 2, 1050, 546, 1048, 195, 1061, 995, 1032, 10, 25, 450, 357, 1135, 254, 559, 612, 912, 965, 179, 94, 739, 470, 616, 298, 900, 224, 598, 301, 436, 1099, 234, 1060, 1053, 839, 815, 158, 804, 945, 89, 31, 210, 365, 993, 413, 225, 115, 618, 42, 567, 320, 1125, 644, 532, 337, 513, 168, 609, 243, 494, 186, 734, 1159, 193, 439, 305, 641, 628, 300, 521, 636, 948, 198, 994, 556, 96, 1130, 568, 22, 557, 1021, 132, 938, 707, 463, 723, 1152, 915, 711, 156, 631, 931, 457, 1066, 433, 666, 464, 459, 597, 816, 471, 725, 614, 1063, 397, 716, 684, 176, 227, 577, 1004, 801, 637, 34, 761, 302, 404, 1090, 980, 1028, 262, 161, 426, 751, 617, 1000, 39, 345, 504, 880, 907, 409, 173, 515, 249, 1069, 285, 1156, 730, 350, 260, 150, 281, 923, 361, 1062, 918, 1052, 904, 282, 1095, 219, 503, 738, 88, 668, 563, 710, 946, 207, 146, 60, 340, 760, 691, 106, 827, 265, 571, 15, 1132, 1015, 985, 825, 970, 758, 223, 406, 921, 1044, 772, 120, 247, 4, 542, 101, 859, 467, 1150, 12, 871, 268, 1012, 630, 1009, 336, 322, 621, 491, 1026, 187, 629, 432, 385, 992, 809, 1024, 756, 366, 62, 188, 53, 899, 950, 939, 1175, 988, 13, 105, 692, 659, 160, 960, 134, 875, 492, 107, 505, 757, 932, 653, 533, 175, 342, 314, 1165, 936, 1114, 252, 639, 8, 695, 838, 539, 824, 458, 408, 508, 248, 148, 352, 1162, 663, 911, 578, 318, 1116, 108, 1106, 235, 844, 54, 889, 645, 586, 1068, 239, 326, 763, 689, 104, 81, 323, 48, 1031, 908, 1041, 851, 1124, 452, 961, 122, 127, 1108, 388, 215, 155, 73, 887, 475, 77, 348, 485, 84, 1148, 650, 509, 75, 299, 331, 144, 241, 288, 727, 619, 775, 625, 206, 977, 455, 934, 697, 777, 865, 245, 835, 585, 133, 70, 655, 1161, 1092, 892, 190, 7, 32, 807, 110, 693, 1134, 1064, 295, 19, 872, 729, 1154, 289, 1176, 657, 142, 890, 445, 952, 275, 866, 587, 927, 603, 943, 211, 1118, 728, 593, 701, 477, 222, 196, 440, 205, 531, 981, 354, 478, 165, 242, 958, 1115, 1075, 1074, 1007, 24, 967, 913, 49, 744, 11, 554, 201, 321, 437, 517, 999, 814, 529, 681, 236, 180, 750, 780, 135, 868, 516, 879, 820, 315, 1128, 560, 218, 845, 297, 949, 841, 1059, 309, 1167, 1155, 665, 425, 1172, 461, 812, 79, 143, 622, 983, 937, 438, 353, 832, 6, 901, 795, 990, 759, 869, 1120, 276, 686, 810, 199, 371, 174, 677, 627, 116, 1133, 435, 519, 113, 496, 1029, 706, 255, 764, 130, 1005, 525, 500, 453, 367, 720, 171, 278, 483, 1122, 754, 698, 1051, 599, 1126, 74, 867, 726, 885, 250, 191, 1027, 229, 996, 310, 209, 884, 111, 456, 1082, 76, 358, 343, 66, 356, 213, 874, 1164, 362, 813, 1138, 5, 1055, 324, 888, 480, 849, 1033, 228, 267, 410, 786, 1035, 139, 873, 972, 740, 303, 951, 304, 1140, 543, 682, 382, 393, 896, 488, 718, 447, 1054, 417, 643, 43, 964, 14, 237, 189, 662, 29, 306, 1144, 119, 654, 86, 1107, 783, 511, 876, 1136, 696, 1146, 1086, 507, 341, 649, 771, 169, 274, 37, 376, 317, 279, 395, 840, 966, 251, 687, 878, 787, 159, 600, 1087, 905, 319, 1160, 378, 1094, 263, 52, 987, 344, 308, 363, 416, 396, 924, 769, 688, 33, 615, 460, 979, 906, 121, 895, 1171, 102, 579, 526, 882, 1168, 177, 523, 380, 1076, 864, 1085, 125, 674, 83, 781, 64, 124, 1047, 51, 20, 661, 767, 258, 151, 833, 638, 1109, 590, 635, 1169, 930, 372, 448, 982, 991, 846, 538, 374, 792, 446, 708, 528, 283, 819, 1089, 858, 1038, 806, 466, 493, 1174, 347, 114, 136, 128, 863, 935, 1093, 415, 269, 510, 1129, 834, 1, 389, 68, 749, 963, 552, 487, 430, 212, 971, 847, 745, 194, 162, 713, 296, 1011, 592, 582, 917, 594, 429, 853, 1073, 784, 520, 499, 469, 451, 1079, 572, 862, 753, 737, 27, 1117, 547, 947, 123, 894, 1112, 855, 384, 1034, 422, 490, 712, 843, 192, 1173, 203, 634, 514, 928, 694, 423, 141, 420, 329, 1017, 1119, 377, 1046, 527, 998, 349, 272, 454, 1057, 501, 178, 392, 405, 774, 472, 238, 1042, 534, 699, 482, 1018, 569, 842, 164, 660, 1113, 536, 817, 95, 240, 387, 854, 126, 561, 743, 431, 672, 802, 208, 766, 690, 390, 1101, 591, 604, 109, 909, 803, 28, 286, 765, 163, 601, 1163, 722, 90, 1177, 1037, 152, 1098, 805, 334, 294, 829, 700, 974, 21]\n",
      "Number of Graphs (dataset_train): 1060\n",
      "Number of Graphs (dataset_test): 118\n",
      "+-----------------------------------------------------------------------------+\n",
      "Learning Rate: 0.001\n",
      "Epochs: 200\n",
      "Batch Size: 64\n",
      "Hidden Dimension 512\n",
      "Number of Layer in Encoder: 2\n",
      "Opimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device: cuda\n",
      "Model:\n",
      "SimCLR(\n",
      "  (encoder): Encoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=89, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      ))\n",
      "      (1): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      ))\n",
      "    )\n",
      "    (bns): ModuleList(\n",
      "      (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Projection_MLP(\n",
      "    (layer1): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (unilayer): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Start Training...\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss = 27.396857766544116\n",
      "Epoch 2 | Loss = 11.132874138215008\n",
      "Epoch 3 | Loss = 4.645824895185583\n",
      "Epoch 4 | Loss = 0.13254042933968938\n",
      "Epoch 5 | Loss = -3.379126117509954\n",
      "Epoch 6 | Loss = -5.341889265705557\n",
      "Epoch 7 | Loss = -7.397039487081416\n",
      "Epoch 8 | Loss = -7.60520588650423\n",
      "Epoch 9 | Loss = -9.900574515847598\n",
      "Epoch 10 | Loss = -11.691782390370088\n",
      "Epoch 11 | Loss = -12.76183142381556\n",
      "Epoch 12 | Loss = -13.651669838849235\n",
      "Epoch 13 | Loss = -14.045415261212517\n",
      "Epoch 14 | Loss = -15.572548754074994\n",
      "Epoch 15 | Loss = -17.059913495007685\n",
      "Epoch 16 | Loss = -17.890267961165485\n",
      "Epoch 17 | Loss = -18.115222566268024\n",
      "Epoch 18 | Loss = -19.642552572138168\n",
      "Epoch 19 | Loss = -20.337293091942282\n",
      "Epoch 20 | Loss = -20.06951430264641\n",
      "Epoch 21 | Loss = -22.22222886366003\n",
      "Epoch 22 | Loss = -21.64889369291418\n",
      "Epoch 23 | Loss = -23.13565186893239\n",
      "Epoch 24 | Loss = -24.22534471399644\n",
      "Epoch 25 | Loss = -23.389287191278793\n",
      "Epoch 26 | Loss = -24.668320038739374\n",
      "Epoch 27 | Loss = -24.731928881476907\n",
      "Epoch 28 | Loss = -25.30348185931935\n",
      "Epoch 29 | Loss = -26.442555399502027\n",
      "Epoch 30 | Loss = -26.852601191576788\n",
      "Epoch 31 | Loss = -27.63712843726663\n",
      "Epoch 32 | Loss = -27.622862843906177\n",
      "Epoch 33 | Loss = -27.322766051572913\n",
      "Epoch 34 | Loss = -28.731236149283017\n",
      "Epoch 35 | Loss = -28.919666318332446\n",
      "Epoch 36 | Loss = -29.79252607682172\n",
      "Epoch 37 | Loss = -30.230697323294248\n",
      "Epoch 38 | Loss = -30.508634202620563\n",
      "Epoch 39 | Loss = -30.801709848291733\n",
      "Epoch 40 | Loss = -31.237137093263513\n",
      "Epoch 41 | Loss = -32.181613220888025\n",
      "Epoch 42 | Loss = -32.072925595676196\n",
      "Epoch 43 | Loss = -32.7975318291608\n",
      "Epoch 44 | Loss = -32.37324950274299\n",
      "Epoch 45 | Loss = -32.72837195676916\n",
      "Epoch 46 | Loss = -33.90763644611134\n",
      "Epoch 47 | Loss = -34.37183416590971\n",
      "Epoch 48 | Loss = -34.55688888886396\n",
      "Epoch 49 | Loss = -34.684486248913935\n",
      "Epoch 50 | Loss = -35.43910475338207\n",
      "Epoch 51 | Loss = -35.667815545025995\n",
      "Epoch 52 | Loss = -35.386877705069146\n",
      "Epoch 53 | Loss = -34.90776275185978\n",
      "Epoch 54 | Loss = -35.9661174100988\n",
      "Epoch 55 | Loss = -36.62048381917617\n",
      "Epoch 56 | Loss = -37.0981541521409\n",
      "Epoch 57 | Loss = -36.859263335957245\n",
      "Epoch 58 | Loss = -38.1267188577091\n",
      "Epoch 59 | Loss = -37.64165320115931\n",
      "Epoch 60 | Loss = -38.342780365663415\n",
      "Epoch 61 | Loss = -38.33818343106438\n",
      "Epoch 62 | Loss = -38.72615718841553\n",
      "Epoch 63 | Loss = -38.632482668932745\n",
      "Epoch 64 | Loss = -38.93200787375955\n",
      "Epoch 65 | Loss = -39.5973536827985\n",
      "Epoch 66 | Loss = -40.06552589640898\n",
      "Epoch 67 | Loss = -40.67676244062536\n",
      "Epoch 68 | Loss = -39.97450940749224\n",
      "Epoch 69 | Loss = -40.329102095435644\n",
      "Epoch 70 | Loss = -40.932424292844885\n",
      "Epoch 71 | Loss = -40.97338696087108\n",
      "Epoch 72 | Loss = -41.26580701154821\n",
      "Epoch 73 | Loss = -41.95606144736795\n",
      "Epoch 74 | Loss = -42.436980471891516\n",
      "Epoch 75 | Loss = -42.45610113704906\n",
      "Epoch 76 | Loss = -41.586993778453156\n",
      "Epoch 77 | Loss = -42.18610915015726\n",
      "Epoch 78 | Loss = -43.0560245513916\n",
      "Epoch 79 | Loss = -43.12129486308378\n",
      "Epoch 80 | Loss = -43.222061830408435\n",
      "Epoch 81 | Loss = -43.026071071624756\n",
      "Epoch 82 | Loss = -43.33544568454518\n",
      "Epoch 83 | Loss = -43.70605207892025\n",
      "Epoch 84 | Loss = -43.943051646737494\n",
      "Epoch 85 | Loss = -43.824978940627155\n",
      "Epoch 86 | Loss = -43.63598831962137\n",
      "Epoch 87 | Loss = -43.806116216322955\n",
      "Epoch 88 | Loss = -44.33947251824772\n",
      "Epoch 89 | Loss = -44.65089750289917\n",
      "Epoch 90 | Loss = -45.08591736064238\n",
      "Epoch 91 | Loss = -45.217720424427704\n",
      "Epoch 92 | Loss = -45.60801865072811\n",
      "Epoch 93 | Loss = -45.77494141634773\n",
      "Epoch 94 | Loss = -46.00162346222822\n",
      "Epoch 95 | Loss = -46.11938765469719\n",
      "Epoch 96 | Loss = -45.33751465292538\n",
      "Epoch 97 | Loss = -46.02923999113195\n",
      "Epoch 98 | Loss = -45.98116594202378\n",
      "Epoch 99 | Loss = -46.388562847586236\n",
      "Epoch 100 | Loss = -46.56153395596672\n",
      "Epoch 101 | Loss = -46.74834156036377\n",
      "Epoch 102 | Loss = -46.88418699713314\n",
      "Epoch 103 | Loss = -46.44243133769316\n",
      "Epoch 104 | Loss = -47.238416475408215\n",
      "Epoch 105 | Loss = -47.35597214979284\n",
      "Epoch 106 | Loss = -47.18372754489674\n",
      "Epoch 107 | Loss = -47.42963684306425\n",
      "Epoch 108 | Loss = -47.65725632274852\n",
      "Epoch 109 | Loss = -47.70405160679537\n",
      "Epoch 110 | Loss = -47.93785221436445\n",
      "Epoch 111 | Loss = -48.23962335025563\n",
      "Epoch 112 | Loss = -48.06345044865328\n",
      "Epoch 113 | Loss = -48.51495512794046\n",
      "Epoch 114 | Loss = -48.51390650693108\n",
      "Epoch 115 | Loss = -49.08196783065796\n",
      "Epoch 116 | Loss = -49.11283551945406\n",
      "Epoch 117 | Loss = -49.05864488377291\n",
      "Epoch 118 | Loss = -49.04486712287454\n",
      "Epoch 119 | Loss = -48.72064453012803\n",
      "Epoch 120 | Loss = -49.281082461862006\n",
      "Epoch 121 | Loss = -48.815246778375965\n",
      "Epoch 122 | Loss = -49.545495341805854\n",
      "Epoch 123 | Loss = -49.54302700828104\n",
      "Epoch 124 | Loss = -49.71641192716711\n",
      "Epoch 125 | Loss = -49.73480538760914\n",
      "Epoch 126 | Loss = -49.77745496525484\n",
      "Epoch 127 | Loss = -49.354447813595044\n",
      "Epoch 128 | Loss = -49.9888035549837\n",
      "Epoch 129 | Loss = -49.70204858218922\n",
      "Epoch 130 | Loss = -50.39915802899529\n",
      "Epoch 131 | Loss = -50.1757701144499\n",
      "Epoch 133 | Loss = -50.70544879576739\n",
      "Epoch 134 | Loss = -50.76937033148373\n",
      "Epoch 135 | Loss = -51.043202316059784\n",
      "Epoch 136 | Loss = -51.19527732624727\n",
      "Epoch 137 | Loss = -50.77573153551887\n",
      "Epoch 138 | Loss = -50.87194827023674\n",
      "Epoch 139 | Loss = -51.0748405456543\n",
      "Epoch 140 | Loss = -51.03337262658512\n",
      "Epoch 141 | Loss = -51.39632460650276\n",
      "Epoch 142 | Loss = -51.567213395062616\n",
      "Epoch 143 | Loss = -51.841236899880805\n",
      "Epoch 144 | Loss = -51.41978928622078\n",
      "Epoch 145 | Loss = -51.82004611632403\n",
      "Epoch 146 | Loss = -51.84297126882217\n",
      "Epoch 147 | Loss = -51.079875861897186\n",
      "Epoch 148 | Loss = -51.80068518133724\n",
      "Epoch 149 | Loss = -51.977106038261866\n",
      "Epoch 150 | Loss = -52.2038143382353\n",
      "Epoch 151 | Loss = -52.116878453423\n",
      "Epoch 152 | Loss = -52.282751840703625\n",
      "Epoch 153 | Loss = -52.312515931970935\n",
      "Epoch 154 | Loss = -52.31381104974186\n",
      "Epoch 155 | Loss = -52.25770863364725\n",
      "Epoch 156 | Loss = -52.54832315444946\n",
      "Epoch 157 | Loss = -52.86259673623478\n",
      "Epoch 158 | Loss = -52.757530969731945\n",
      "Epoch 159 | Loss = -52.66952236960916\n",
      "Epoch 161 | Loss = -53.030480693368354\n",
      "Epoch 162 | Loss = -52.99618045021506\n",
      "Epoch 163 | Loss = -52.9152052823235\n",
      "Epoch 164 | Loss = -53.15288804559147\n",
      "Epoch 165 | Loss = -53.1088817540337\n",
      "Epoch 166 | Loss = -53.14842911327587\n",
      "Epoch 167 | Loss = -53.41007075590246\n",
      "Epoch 168 | Loss = -53.443038547740265\n",
      "Epoch 169 | Loss = -53.399277069989374\n",
      "Epoch 170 | Loss = -53.0458337839912\n",
      "Epoch 171 | Loss = -53.34148157344145\n",
      "Epoch 172 | Loss = -53.59757069980397\n",
      "Epoch 173 | Loss = -53.537901401519775\n",
      "Epoch 174 | Loss = -53.722449134377875\n",
      "Epoch 175 | Loss = -53.78596016939949\n",
      "Epoch 176 | Loss = -53.78082645640654\n",
      "Epoch 177 | Loss = -53.975185366237866\n",
      "Epoch 178 | Loss = -53.86671315922457\n",
      "Epoch 179 | Loss = -53.83377829719992\n",
      "Epoch 180 | Loss = -54.01371804405661\n",
      "Epoch 181 | Loss = -54.039491961984076\n",
      "Epoch 182 | Loss = -54.02050626979155\n",
      "Epoch 183 | Loss = -54.37163111742805\n",
      "Epoch 184 | Loss = -54.25734977161183\n",
      "Epoch 185 | Loss = -54.601615597220025\n",
      "Epoch 186 | Loss = -54.406609170577106\n",
      "Epoch 187 | Loss = -54.11169879576739\n",
      "Epoch 188 | Loss = -54.399833651149976\n",
      "Epoch 189 | Loss = -54.28032502006082\n",
      "Epoch 190 | Loss = -54.3051866643569\n",
      "Epoch 191 | Loss = -54.437054970685175\n",
      "Epoch 192 | Loss = -54.43307467067943\n",
      "Epoch 193 | Loss = -54.144197688383215\n",
      "Epoch 194 | Loss = -54.468408949234906\n",
      "Epoch 195 | Loss = -54.58881139755249\n",
      "Epoch 196 | Loss = -54.74579006082871\n",
      "Epoch 197 | Loss = -54.72262202992159\n",
      "Epoch 198 | Loss = -54.91844393225277\n",
      "Epoch 199 | Loss = -54.90468406677246\n",
      "Epoch 200 | Loss = -54.884597806369555\n",
      "+-----------------------------------------------------------------------------+\n",
      "Training Time: 24095.25116610527 seconds.\n",
      "Get Embedding...\n",
      "(118, 1024) (118,)\n",
      "SVC Accuracy: [0.7901515151515153, 0.7462121212121212]\n",
      "+-----------------------------------------------------------------------------+\n",
      "Embedding Time: 14.365458250045776 seconds.\n",
      "Experient 3\n",
      "20211218-181701 :Log the record!\n",
      "+-----------------------------------------------------------------------------+\n",
      "Experient 4\n",
      "!!!\n",
      "89\n",
      "!!!\n",
      "[553, 217, 346, 850, 1139, 72, 280, 861, 916, 589, 50, 182, 307, 1067, 172, 583, 831, 791, 1014, 246, 18, 1013, 59, 1023, 984, 69, 755, 65, 71, 742, 1153, 256, 856, 266, 427, 956, 679, 826, 541, 147, 910, 0, 922, 683, 131, 103, 91, 1001, 166, 558, 796, 1078, 811, 852, 351, 1002, 383, 145, 197, 1072, 789, 17, 606, 9, 608, 1111, 664, 414, 154, 290, 954, 667, 959, 1097, 652, 339, 184, 368, 327, 506, 933, 370, 36, 768, 67, 1158, 93, 1123, 47, 624, 311, 883, 484, 702, 379, 316, 794, 822, 818, 55, 375, 1008, 605, 584, 920, 575, 986, 675, 82, 468, 1104, 202, 428, 566, 85, 550, 914, 1142, 545, 1145, 870, 1149, 332, 732, 185, 401, 359, 540, 714, 277, 785, 56, 45, 330, 1137, 564, 941, 703, 157, 1056, 1025, 325, 394, 360, 1077, 611, 1058, 1131, 830, 1081, 857, 1105, 364, 57, 233, 291, 1065, 926, 181, 231, 230, 808, 731, 678, 588, 929, 226, 537, 680, 38, 746, 595, 733, 244, 138, 1039, 441, 216, 762, 220, 284, 570, 1166, 1103, 117, 476, 261, 312, 232, 424, 149, 1110, 183, 651, 776, 522, 898, 381, 1100, 1088, 221, 1043, 596, 1084, 719, 669, 253, 715, 498, 35, 112, 495, 524, 548, 369, 735, 574, 580, 391, 167, 293, 942, 793, 273, 204, 1049, 474, 292, 973, 129, 1006, 402, 444, 975, 137, 1071, 647, 656, 1080, 1045, 333, 648, 555, 1127, 576, 486, 602, 989, 399, 3, 1030, 1016, 782, 153, 962, 421, 620, 1036, 264, 944, 848, 736, 717, 26, 1121, 978, 338, 398, 449, 46, 778, 877, 773, 633, 287, 465, 957, 828, 462, 800, 419, 902, 709, 953, 549, 997, 724, 78, 779, 1019, 2, 1050, 546, 1048, 195, 1061, 995, 1032, 10, 25, 450, 357, 1135, 254, 559, 612, 912, 965, 179, 94, 739, 470, 616, 298, 900, 224, 598, 301, 436, 1099, 234, 1060, 1053, 839, 815, 158, 804, 945, 89, 31, 210, 365, 993, 413, 225, 115, 618, 42, 567, 320, 1125, 644, 532, 337, 513, 168, 609, 243, 494, 186, 734, 1159, 193, 439, 305, 641, 628, 300, 521, 636, 948, 198, 994, 556, 96, 1130, 568, 22, 557, 1021, 132, 938, 707, 463, 723, 1152, 915, 711, 156, 631, 931, 457, 1066, 433, 666, 464, 459, 597, 816, 471, 725, 614, 1063, 397, 716, 684, 176, 227, 577, 1004, 801, 637, 34, 761, 302, 404, 1090, 980, 1028, 262, 161, 426, 751, 617, 1000, 39, 345, 504, 880, 907, 409, 173, 515, 249, 1069, 285, 1156, 730, 350, 260, 150, 281, 923, 361, 1062, 918, 1052, 904, 282, 1095, 219, 503, 738, 88, 668, 563, 710, 946, 207, 146, 60, 340, 760, 691, 106, 827, 265, 571, 15, 1132, 1015, 985, 825, 970, 758, 223, 406, 921, 1044, 772, 120, 247, 4, 542, 101, 859, 467, 1150, 12, 871, 268, 1012, 630, 1009, 336, 322, 621, 491, 1026, 187, 629, 432, 385, 992, 809, 1024, 756, 366, 62, 188, 53, 899, 950, 939, 1175, 988, 13, 105, 692, 659, 160, 960, 134, 875, 492, 107, 505, 757, 932, 653, 533, 175, 342, 314, 1165, 936, 1114, 252, 639, 8, 695, 838, 539, 824, 458, 408, 508, 248, 148, 352, 1162, 663, 911, 578, 318, 1116, 108, 1106, 235, 844, 54, 889, 645, 586, 1068, 239, 326, 763, 689, 104, 81, 323, 48, 1031, 908, 1041, 851, 1124, 452, 961, 122, 127, 1108, 388, 215, 155, 73, 887, 475, 77, 348, 485, 84, 1148, 650, 509, 75, 299, 331, 144, 241, 288, 727, 619, 775, 625, 206, 977, 455, 934, 697, 777, 865, 245, 835, 585, 133, 70, 655, 1161, 1092, 892, 190, 7, 32, 807, 110, 693, 1134, 1064, 295, 19, 872, 729, 1154, 289, 1176, 657, 142, 890, 445, 952, 275, 866, 587, 927, 603, 943, 211, 1118, 728, 593, 701, 477, 222, 196, 440, 205, 531, 981, 354, 478, 165, 242, 958, 1115, 1075, 1074, 1007, 24, 967, 913, 49, 744, 11, 554, 201, 321, 437, 517, 999, 814, 529, 681, 236, 180, 750, 780, 135, 868, 516, 879, 820, 315, 1128, 560, 218, 845, 297, 949, 841, 1059, 309, 1167, 1155, 665, 425, 1172, 461, 812, 79, 143, 622, 983, 937, 438, 353, 832, 6, 901, 795, 990, 759, 869, 1120, 276, 686, 810, 199, 371, 174, 677, 627, 116, 1133, 435, 519, 113, 496, 1029, 706, 255, 764, 130, 1005, 525, 500, 453, 367, 720, 171, 278, 483, 1122, 754, 698, 1051, 599, 1126, 74, 867, 726, 885, 250, 191, 1027, 229, 996, 310, 209, 884, 111, 456, 1082, 76, 358, 343, 66, 356, 213, 874, 1164, 362, 813, 1138, 5, 1055, 324, 888, 480, 849, 1033, 228, 267, 410, 786, 1035, 139, 873, 972, 740, 303, 951, 304, 1140, 543, 682, 382, 393, 896, 488, 718, 447, 1054, 417, 643, 43, 964, 14, 237, 189, 662, 29, 306, 1144, 119, 654, 86, 1107, 783, 511, 876, 1136, 696, 1146, 1086, 507, 341, 649, 771, 169, 274, 37, 376, 317, 279, 395, 840, 966, 251, 687, 878, 787, 159, 600, 1087, 905, 319, 1160, 378, 1094, 263, 52, 987, 344, 308, 363, 416, 396, 924, 769, 688, 33, 615, 460, 979, 906, 121, 895, 1171, 102, 579, 526, 882, 1168, 177, 523, 380, 1076, 864, 1085, 125, 674, 83, 781, 64, 124, 1047, 51, 20, 661, 767, 258, 151, 833, 638, 1109, 590, 635, 1169, 930, 372, 448, 982, 991, 846, 538, 374, 792, 446, 708, 528, 283, 819, 1089, 858, 1038, 806, 466, 493, 1174, 347, 114, 136, 128, 863, 935, 1093, 415, 269, 510, 1129, 834, 1, 389, 68, 749, 963, 552, 487, 430, 212, 971, 847, 745, 194, 162, 713, 296, 1011, 592, 582, 917, 594, 429, 853, 1073, 784, 520, 499, 469, 451, 1079, 572, 862, 753, 737, 27, 1117, 547, 947, 123, 894, 1112, 855, 384, 1034, 422, 490, 712, 843, 192, 1173, 203, 634, 514, 928, 694, 423, 141, 420, 329, 1017, 1119, 377, 1046, 527, 998, 349, 272, 454, 1057, 501, 178, 392, 405, 774, 472, 238, 1042, 534, 699, 482, 1018, 569, 842, 164, 660, 1113, 536, 817, 95, 240, 387, 854, 126, 561, 743, 431, 672, 802, 208, 766, 690, 390, 1101, 591, 604, 109, 909, 803, 28, 286, 765, 163, 601, 1163, 722, 90, 1177, 1037, 152, 1098, 805, 334, 294, 829, 700, 974, 21]\n",
      "Number of Graphs (dataset_train): 1060\n",
      "Number of Graphs (dataset_test): 118\n",
      "+-----------------------------------------------------------------------------+\n",
      "Learning Rate: 0.001\n",
      "Epochs: 200\n",
      "Batch Size: 64\n",
      "Hidden Dimension 64\n",
      "Number of Layer in Encoder: 3\n",
      "Opimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device: cuda\n",
      "Model:\n",
      "SimCLR(\n",
      "  (encoder): Encoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=89, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      ))\n",
      "      (1): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      ))\n",
      "      (2): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      ))\n",
      "    )\n",
      "    (bns): ModuleList(\n",
      "      (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Projection_MLP(\n",
      "    (layer1): Sequential(\n",
      "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (1): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (1): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (1): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (unilayer): Sequential(\n",
      "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=192, out_features=192, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Start Training...\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss = 71.44692621511571\n",
      "Epoch 2 | Loss = 44.93205659529742\n",
      "Epoch 3 | Loss = 34.70623938826954\n",
      "Epoch 4 | Loss = 25.4407638241263\n",
      "Epoch 5 | Loss = 21.23957115061143\n",
      "Epoch 6 | Loss = 17.786540494245642\n",
      "Epoch 7 | Loss = 14.547998344196992\n",
      "Epoch 8 | Loss = 11.38283543025746\n",
      "Epoch 9 | Loss = 9.021919152315926\n",
      "Epoch 10 | Loss = 7.907720439574298\n",
      "Epoch 11 | Loss = 7.017472505569458\n",
      "Epoch 12 | Loss = 5.440371204825008\n",
      "Epoch 13 | Loss = 3.1005782099331127\n",
      "Epoch 14 | Loss = 1.7944025923224056\n",
      "Epoch 15 | Loss = 0.9329517623957466\n",
      "Epoch 16 | Loss = -1.1031675338745117\n",
      "Epoch 17 | Loss = -0.8060603772892672\n",
      "Epoch 18 | Loss = -3.13887055831797\n",
      "Epoch 19 | Loss = -3.371211241273319\n",
      "Epoch 20 | Loss = -3.8754614451352287\n",
      "Epoch 21 | Loss = -4.769638787297642\n",
      "Epoch 22 | Loss = -4.8735531182850105\n",
      "Epoch 23 | Loss = -5.544977153048796\n",
      "Epoch 24 | Loss = -6.290142157498528\n",
      "Epoch 25 | Loss = -7.4355448274051446\n",
      "Epoch 26 | Loss = -8.122339795617496\n",
      "Epoch 27 | Loss = -8.58924041074865\n",
      "Epoch 28 | Loss = -9.416383518892175\n",
      "Epoch 29 | Loss = -9.104602918905371\n",
      "Epoch 30 | Loss = -9.71337231467752\n",
      "Epoch 31 | Loss = -10.289747476577759\n",
      "Epoch 32 | Loss = -10.996257852105533\n",
      "Epoch 33 | Loss = -10.491760758792653\n",
      "Epoch 34 | Loss = -11.31498561185949\n",
      "Epoch 35 | Loss = -11.763487591462976\n",
      "Epoch 36 | Loss = -12.466630753348856\n",
      "Epoch 37 | Loss = -11.747069919810576\n",
      "Epoch 38 | Loss = -12.666141930748434\n",
      "Epoch 39 | Loss = -13.219220189487233\n",
      "Epoch 40 | Loss = -14.451853135052849\n",
      "Epoch 41 | Loss = -13.554778099060059\n",
      "Epoch 42 | Loss = -13.366519731633803\n",
      "Epoch 43 | Loss = -15.181806115543141\n",
      "Epoch 45 | Loss = -14.332266919753131\n",
      "Epoch 46 | Loss = -15.614029351402731\n",
      "Epoch 47 | Loss = -15.682103465585147\n",
      "Epoch 48 | Loss = -16.338148593902588\n",
      "Epoch 49 | Loss = -16.734497603248148\n",
      "Epoch 50 | Loss = -16.487371781293085\n",
      "Epoch 51 | Loss = -17.140987957225125\n",
      "Epoch 52 | Loss = -17.627098167643826\n",
      "Epoch 53 | Loss = -17.8247548271628\n",
      "Epoch 54 | Loss = -18.210773636313046\n",
      "Epoch 55 | Loss = -17.52801965264713\n",
      "Epoch 56 | Loss = -18.650659953846652\n",
      "Epoch 57 | Loss = -19.039160139420453\n",
      "Epoch 58 | Loss = -18.93794009264778\n",
      "Epoch 59 | Loss = -19.57113406237434\n",
      "Epoch 60 | Loss = -20.100834145265466\n",
      "Epoch 61 | Loss = -19.03862731597003\n",
      "Epoch 62 | Loss = -19.30579869887408\n",
      "Epoch 63 | Loss = -19.953194225535672\n",
      "Epoch 64 | Loss = -19.600933271295883\n",
      "Epoch 65 | Loss = -20.933666145100315\n",
      "Epoch 66 | Loss = -20.661000896902646\n",
      "Epoch 67 | Loss = -20.48956573710722\n",
      "Epoch 68 | Loss = -20.56736348657047\n",
      "Epoch 69 | Loss = -21.886795352487002\n",
      "Epoch 70 | Loss = -21.259631521561566\n",
      "Epoch 71 | Loss = -22.478411926942712\n",
      "Epoch 72 | Loss = -20.82113754048067\n",
      "Epoch 73 | Loss = -22.081176056581384\n",
      "Epoch 74 | Loss = -22.116923079771155\n",
      "Epoch 75 | Loss = -21.63830639334286\n",
      "Epoch 76 | Loss = -22.28513296912698\n",
      "Epoch 77 | Loss = -22.667404932134293\n",
      "Epoch 78 | Loss = -23.34242686103372\n",
      "Epoch 79 | Loss = -22.95622219758875\n",
      "Epoch 80 | Loss = -23.686480914845188\n",
      "Epoch 81 | Loss = -23.25271494248334\n",
      "Epoch 82 | Loss = -23.145813829758588\n",
      "Epoch 83 | Loss = -23.635119830860813\n",
      "Epoch 84 | Loss = -23.54479688756606\n",
      "Epoch 85 | Loss = -23.885150965522318\n",
      "Epoch 86 | Loss = -22.793922340168674\n",
      "Epoch 87 | Loss = -24.241838455200195\n",
      "Epoch 88 | Loss = -24.2790924801546\n",
      "Epoch 89 | Loss = -23.94249153137207\n",
      "Epoch 90 | Loss = -23.750452546512378\n",
      "Epoch 91 | Loss = -25.06744586720186\n",
      "Epoch 92 | Loss = -25.554974219378302\n",
      "Epoch 93 | Loss = -24.98478906294879\n",
      "Epoch 94 | Loss = -24.247103522805606\n",
      "Epoch 95 | Loss = -24.396421123953427\n",
      "Epoch 96 | Loss = -24.35655060936423\n",
      "Epoch 97 | Loss = -24.85048016379861\n",
      "Epoch 98 | Loss = -25.210298033321607\n",
      "Epoch 99 | Loss = -25.345410459181842\n",
      "Epoch 100 | Loss = -24.671992301940918\n",
      "Epoch 101 | Loss = -25.760031644035788\n",
      "Epoch 102 | Loss = -26.194940342622644\n",
      "Epoch 103 | Loss = -25.770956376019647\n",
      "Epoch 104 | Loss = -25.833592891693115\n",
      "Epoch 105 | Loss = -25.933408849379596\n",
      "Epoch 106 | Loss = -26.682282391716452\n",
      "Epoch 107 | Loss = -25.96029800527236\n",
      "Epoch 108 | Loss = -26.043397090014288\n",
      "Epoch 109 | Loss = -26.446228672476376\n",
      "Epoch 110 | Loss = -27.345383279463825\n",
      "Epoch 111 | Loss = -26.46105662514182\n",
      "Epoch 112 | Loss = -26.578010811525232\n",
      "Epoch 113 | Loss = -27.314244663014133\n",
      "Epoch 114 | Loss = -27.261076478397143\n",
      "Epoch 115 | Loss = -27.38017233680276\n",
      "Epoch 116 | Loss = -27.216530743767233\n",
      "Epoch 117 | Loss = -27.461572562946994\n",
      "Epoch 118 | Loss = -27.186875203076532\n",
      "Epoch 119 | Loss = -27.316357191871194\n",
      "Epoch 120 | Loss = -27.14084566340727\n",
      "Epoch 121 | Loss = -28.21061050190645\n",
      "Epoch 122 | Loss = -28.38496348437141\n",
      "Epoch 123 | Loss = -28.36191194197711\n",
      "Epoch 124 | Loss = -28.42063028672162\n",
      "Epoch 125 | Loss = -28.537498165579404\n",
      "Epoch 126 | Loss = -28.426009991589716\n",
      "Epoch 127 | Loss = -28.676473028519574\n",
      "Epoch 128 | Loss = -28.88846503987032\n",
      "Epoch 129 | Loss = -29.35893908668967\n",
      "Epoch 130 | Loss = -28.6394102713641\n",
      "Epoch 131 | Loss = -29.568702978246353\n",
      "Epoch 132 | Loss = -29.341404157526352\n",
      "Epoch 133 | Loss = -28.752583082984476\n",
      "Epoch 134 | Loss = -29.221201083239386\n",
      "Epoch 135 | Loss = -29.32016316582175\n",
      "Epoch 136 | Loss = -29.435272076550653\n",
      "Epoch 137 | Loss = -28.883155738606174\n",
      "Epoch 138 | Loss = -28.985642797806683\n",
      "Epoch 139 | Loss = -29.249300479888916\n",
      "Epoch 140 | Loss = -29.346232610590317\n",
      "Epoch 141 | Loss = -29.960651313557346\n",
      "Epoch 142 | Loss = -29.373485424939325\n",
      "Epoch 143 | Loss = -29.326546164119947\n",
      "Epoch 144 | Loss = -29.59555460424984\n",
      "Epoch 145 | Loss = -30.38326939414529\n",
      "Epoch 146 | Loss = -29.883500856511734\n",
      "Epoch 147 | Loss = -29.39621064242195\n",
      "Epoch 148 | Loss = -30.855216531192553\n",
      "Epoch 149 | Loss = -30.406613995047177\n",
      "Epoch 150 | Loss = -30.40271557078642\n",
      "Epoch 151 | Loss = -29.562798135420856\n",
      "Epoch 152 | Loss = -30.83099438162411\n",
      "Epoch 153 | Loss = -31.35691754958209\n",
      "Epoch 154 | Loss = -31.168590433457318\n",
      "Epoch 155 | Loss = -31.08953686321483\n",
      "Epoch 156 | Loss = -30.2731593356413\n",
      "Epoch 157 | Loss = -31.070162913378546\n",
      "Epoch 158 | Loss = -30.641952767091638\n",
      "Epoch 159 | Loss = -30.811030976912555\n",
      "Epoch 160 | Loss = -30.88175448249368\n",
      "Epoch 161 | Loss = -30.91980530233944\n",
      "Epoch 162 | Loss = -30.700198930852554\n",
      "Epoch 163 | Loss = -31.781100946314194\n",
      "Epoch 164 | Loss = -31.359222327961643\n",
      "Epoch 165 | Loss = -31.21228568694171\n",
      "Epoch 166 | Loss = -31.05256955763873\n",
      "Epoch 167 | Loss = -30.928767484777115\n",
      "Epoch 168 | Loss = -31.523479012882007\n",
      "Epoch 169 | Loss = -31.59099592882044\n",
      "Epoch 170 | Loss = -32.837439256555896\n",
      "Epoch 171 | Loss = -31.730980171876794\n",
      "Epoch 172 | Loss = -32.38575769873226\n",
      "Epoch 173 | Loss = -32.14848765204935\n",
      "Epoch 174 | Loss = -32.55376047246597\n",
      "Epoch 175 | Loss = -32.21689681445851\n",
      "Epoch 176 | Loss = -32.3070717418895\n",
      "Epoch 177 | Loss = -31.513286618625415\n",
      "Epoch 178 | Loss = -32.16831925336052\n",
      "Epoch 180 | Loss = -32.35558383605059\n",
      "Epoch 181 | Loss = -32.679279299343335\n",
      "Epoch 182 | Loss = -32.230924269732306\n",
      "Epoch 183 | Loss = -32.35583585851333\n",
      "Epoch 184 | Loss = -33.4266396410325\n",
      "Epoch 185 | Loss = -33.25531794043148\n",
      "Epoch 186 | Loss = -33.05460921455832\n",
      "Epoch 187 | Loss = -33.8028572026421\n",
      "Epoch 188 | Loss = -33.10633732290829\n",
      "Epoch 189 | Loss = -32.9172301011927\n",
      "Epoch 190 | Loss = -33.194428724401135\n",
      "Epoch 191 | Loss = -33.43284873401417\n",
      "Epoch 192 | Loss = -33.2709257181953\n",
      "Epoch 193 | Loss = -33.61581117966596\n",
      "Epoch 194 | Loss = -33.470064640045166\n",
      "Epoch 195 | Loss = -33.451557748457965\n",
      "Epoch 196 | Loss = -33.54876305075253\n",
      "Epoch 197 | Loss = -32.96556478388169\n",
      "Epoch 198 | Loss = -34.216813283808094\n",
      "Epoch 199 | Loss = -33.86552412369672\n",
      "Epoch 200 | Loss = -33.64283399020924\n",
      "+-----------------------------------------------------------------------------+\n",
      "Training Time: 24026.267570257187 seconds.\n",
      "Get Embedding...\n",
      "(118, 192) (118,)\n",
      "SVC Accuracy: [0.781060606060606, 0.737878787878788]\n",
      "+-----------------------------------------------------------------------------+\n",
      "Embedding Time: 13.438380002975464 seconds.\n",
      "Experient 4\n",
      "20211219-005741 :Log the record!\n",
      "+-----------------------------------------------------------------------------+\n",
      "Experient 5\n",
      "!!!\n",
      "89\n",
      "!!!\n",
      "[553, 217, 346, 850, 1139, 72, 280, 861, 916, 589, 50, 182, 307, 1067, 172, 583, 831, 791, 1014, 246, 18, 1013, 59, 1023, 984, 69, 755, 65, 71, 742, 1153, 256, 856, 266, 427, 956, 679, 826, 541, 147, 910, 0, 922, 683, 131, 103, 91, 1001, 166, 558, 796, 1078, 811, 852, 351, 1002, 383, 145, 197, 1072, 789, 17, 606, 9, 608, 1111, 664, 414, 154, 290, 954, 667, 959, 1097, 652, 339, 184, 368, 327, 506, 933, 370, 36, 768, 67, 1158, 93, 1123, 47, 624, 311, 883, 484, 702, 379, 316, 794, 822, 818, 55, 375, 1008, 605, 584, 920, 575, 986, 675, 82, 468, 1104, 202, 428, 566, 85, 550, 914, 1142, 545, 1145, 870, 1149, 332, 732, 185, 401, 359, 540, 714, 277, 785, 56, 45, 330, 1137, 564, 941, 703, 157, 1056, 1025, 325, 394, 360, 1077, 611, 1058, 1131, 830, 1081, 857, 1105, 364, 57, 233, 291, 1065, 926, 181, 231, 230, 808, 731, 678, 588, 929, 226, 537, 680, 38, 746, 595, 733, 244, 138, 1039, 441, 216, 762, 220, 284, 570, 1166, 1103, 117, 476, 261, 312, 232, 424, 149, 1110, 183, 651, 776, 522, 898, 381, 1100, 1088, 221, 1043, 596, 1084, 719, 669, 253, 715, 498, 35, 112, 495, 524, 548, 369, 735, 574, 580, 391, 167, 293, 942, 793, 273, 204, 1049, 474, 292, 973, 129, 1006, 402, 444, 975, 137, 1071, 647, 656, 1080, 1045, 333, 648, 555, 1127, 576, 486, 602, 989, 399, 3, 1030, 1016, 782, 153, 962, 421, 620, 1036, 264, 944, 848, 736, 717, 26, 1121, 978, 338, 398, 449, 46, 778, 877, 773, 633, 287, 465, 957, 828, 462, 800, 419, 902, 709, 953, 549, 997, 724, 78, 779, 1019, 2, 1050, 546, 1048, 195, 1061, 995, 1032, 10, 25, 450, 357, 1135, 254, 559, 612, 912, 965, 179, 94, 739, 470, 616, 298, 900, 224, 598, 301, 436, 1099, 234, 1060, 1053, 839, 815, 158, 804, 945, 89, 31, 210, 365, 993, 413, 225, 115, 618, 42, 567, 320, 1125, 644, 532, 337, 513, 168, 609, 243, 494, 186, 734, 1159, 193, 439, 305, 641, 628, 300, 521, 636, 948, 198, 994, 556, 96, 1130, 568, 22, 557, 1021, 132, 938, 707, 463, 723, 1152, 915, 711, 156, 631, 931, 457, 1066, 433, 666, 464, 459, 597, 816, 471, 725, 614, 1063, 397, 716, 684, 176, 227, 577, 1004, 801, 637, 34, 761, 302, 404, 1090, 980, 1028, 262, 161, 426, 751, 617, 1000, 39, 345, 504, 880, 907, 409, 173, 515, 249, 1069, 285, 1156, 730, 350, 260, 150, 281, 923, 361, 1062, 918, 1052, 904, 282, 1095, 219, 503, 738, 88, 668, 563, 710, 946, 207, 146, 60, 340, 760, 691, 106, 827, 265, 571, 15, 1132, 1015, 985, 825, 970, 758, 223, 406, 921, 1044, 772, 120, 247, 4, 542, 101, 859, 467, 1150, 12, 871, 268, 1012, 630, 1009, 336, 322, 621, 491, 1026, 187, 629, 432, 385, 992, 809, 1024, 756, 366, 62, 188, 53, 899, 950, 939, 1175, 988, 13, 105, 692, 659, 160, 960, 134, 875, 492, 107, 505, 757, 932, 653, 533, 175, 342, 314, 1165, 936, 1114, 252, 639, 8, 695, 838, 539, 824, 458, 408, 508, 248, 148, 352, 1162, 663, 911, 578, 318, 1116, 108, 1106, 235, 844, 54, 889, 645, 586, 1068, 239, 326, 763, 689, 104, 81, 323, 48, 1031, 908, 1041, 851, 1124, 452, 961, 122, 127, 1108, 388, 215, 155, 73, 887, 475, 77, 348, 485, 84, 1148, 650, 509, 75, 299, 331, 144, 241, 288, 727, 619, 775, 625, 206, 977, 455, 934, 697, 777, 865, 245, 835, 585, 133, 70, 655, 1161, 1092, 892, 190, 7, 32, 807, 110, 693, 1134, 1064, 295, 19, 872, 729, 1154, 289, 1176, 657, 142, 890, 445, 952, 275, 866, 587, 927, 603, 943, 211, 1118, 728, 593, 701, 477, 222, 196, 440, 205, 531, 981, 354, 478, 165, 242, 958, 1115, 1075, 1074, 1007, 24, 967, 913, 49, 744, 11, 554, 201, 321, 437, 517, 999, 814, 529, 681, 236, 180, 750, 780, 135, 868, 516, 879, 820, 315, 1128, 560, 218, 845, 297, 949, 841, 1059, 309, 1167, 1155, 665, 425, 1172, 461, 812, 79, 143, 622, 983, 937, 438, 353, 832, 6, 901, 795, 990, 759, 869, 1120, 276, 686, 810, 199, 371, 174, 677, 627, 116, 1133, 435, 519, 113, 496, 1029, 706, 255, 764, 130, 1005, 525, 500, 453, 367, 720, 171, 278, 483, 1122, 754, 698, 1051, 599, 1126, 74, 867, 726, 885, 250, 191, 1027, 229, 996, 310, 209, 884, 111, 456, 1082, 76, 358, 343, 66, 356, 213, 874, 1164, 362, 813, 1138, 5, 1055, 324, 888, 480, 849, 1033, 228, 267, 410, 786, 1035, 139, 873, 972, 740, 303, 951, 304, 1140, 543, 682, 382, 393, 896, 488, 718, 447, 1054, 417, 643, 43, 964, 14, 237, 189, 662, 29, 306, 1144, 119, 654, 86, 1107, 783, 511, 876, 1136, 696, 1146, 1086, 507, 341, 649, 771, 169, 274, 37, 376, 317, 279, 395, 840, 966, 251, 687, 878, 787, 159, 600, 1087, 905, 319, 1160, 378, 1094, 263, 52, 987, 344, 308, 363, 416, 396, 924, 769, 688, 33, 615, 460, 979, 906, 121, 895, 1171, 102, 579, 526, 882, 1168, 177, 523, 380, 1076, 864, 1085, 125, 674, 83, 781, 64, 124, 1047, 51, 20, 661, 767, 258, 151, 833, 638, 1109, 590, 635, 1169, 930, 372, 448, 982, 991, 846, 538, 374, 792, 446, 708, 528, 283, 819, 1089, 858, 1038, 806, 466, 493, 1174, 347, 114, 136, 128, 863, 935, 1093, 415, 269, 510, 1129, 834, 1, 389, 68, 749, 963, 552, 487, 430, 212, 971, 847, 745, 194, 162, 713, 296, 1011, 592, 582, 917, 594, 429, 853, 1073, 784, 520, 499, 469, 451, 1079, 572, 862, 753, 737, 27, 1117, 547, 947, 123, 894, 1112, 855, 384, 1034, 422, 490, 712, 843, 192, 1173, 203, 634, 514, 928, 694, 423, 141, 420, 329, 1017, 1119, 377, 1046, 527, 998, 349, 272, 454, 1057, 501, 178, 392, 405, 774, 472, 238, 1042, 534, 699, 482, 1018, 569, 842, 164, 660, 1113, 536, 817, 95, 240, 387, 854, 126, 561, 743, 431, 672, 802, 208, 766, 690, 390, 1101, 591, 604, 109, 909, 803, 28, 286, 765, 163, 601, 1163, 722, 90, 1177, 1037, 152, 1098, 805, 334, 294, 829, 700, 974, 21]\n",
      "Number of Graphs (dataset_train): 1060\n",
      "Number of Graphs (dataset_test): 118\n",
      "+-----------------------------------------------------------------------------+\n",
      "Learning Rate: 0.001\n",
      "Epochs: 200\n",
      "Batch Size: 64\n",
      "Hidden Dimension 512\n",
      "Number of Layer in Encoder: 3\n",
      "Opimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device: cuda\n",
      "Model:\n",
      "SimCLR(\n",
      "  (encoder): Encoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=89, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      ))\n",
      "      (1): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      ))\n",
      "      (2): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      ))\n",
      "    )\n",
      "    (bns): ModuleList(\n",
      "      (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Projection_MLP(\n",
      "    (layer1): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (unilayer): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Start Training...\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss = 33.06378836491529\n",
      "Epoch 2 | Loss = 15.343166126924402\n",
      "Epoch 3 | Loss = 7.097939996158376\n",
      "Epoch 4 | Loss = 2.9924911470974194\n",
      "Epoch 5 | Loss = 0.11979328884797938\n",
      "Epoch 6 | Loss = -1.3919234240756315\n",
      "Epoch 8 | Loss = -6.582771935883691\n",
      "Epoch 9 | Loss = -8.495903604170856\n",
      "Epoch 10 | Loss = -10.065223329207477\n",
      "Epoch 11 | Loss = -11.513290405273438\n",
      "Epoch 12 | Loss = -12.680636980954338\n",
      "Epoch 13 | Loss = -13.383758011986227\n",
      "Epoch 14 | Loss = -14.591155052185059\n",
      "Epoch 15 | Loss = -15.78812307469985\n",
      "Epoch 16 | Loss = -17.263451856725357\n",
      "Epoch 17 | Loss = -17.88667563831105\n",
      "Epoch 18 | Loss = -18.93968859840842\n",
      "Epoch 19 | Loss = -18.0471329128041\n",
      "Epoch 20 | Loss = -20.652859267066507\n",
      "Epoch 21 | Loss = -21.030930603251736\n",
      "Epoch 22 | Loss = -22.24388302073759\n",
      "Epoch 23 | Loss = -21.771870416753433\n",
      "Epoch 24 | Loss = -22.91563252841725\n",
      "Epoch 25 | Loss = -24.785290157093723\n",
      "Epoch 26 | Loss = -24.409918168011835\n",
      "Epoch 27 | Loss = -24.999038471895105\n",
      "Epoch 28 | Loss = -26.415337141822366\n",
      "Epoch 29 | Loss = -26.02238960827098\n",
      "Epoch 30 | Loss = -27.359281119178323\n",
      "Epoch 31 | Loss = -27.694711572983685\n",
      "Epoch 32 | Loss = -27.75085998983944\n",
      "Epoch 33 | Loss = -27.94141721725464\n",
      "Epoch 34 | Loss = -29.104917806737564\n",
      "Epoch 35 | Loss = -28.9361387982088\n",
      "Epoch 36 | Loss = -29.19326986986048\n",
      "Epoch 37 | Loss = -29.696505855111514\n",
      "Epoch 38 | Loss = -31.039541553048526\n",
      "Epoch 39 | Loss = -31.26800026613123\n",
      "Epoch 40 | Loss = -31.834800131180707\n",
      "Epoch 41 | Loss = -32.253297469195196\n",
      "Epoch 42 | Loss = -32.89351438073551\n",
      "Epoch 43 | Loss = -33.249827945933625\n",
      "Epoch 44 | Loss = -33.93542054120232\n",
      "Epoch 45 | Loss = -34.11642601910759\n",
      "Epoch 46 | Loss = -34.17137507831349\n",
      "Epoch 47 | Loss = -35.03939698724186\n",
      "Epoch 48 | Loss = -35.95988021177404\n",
      "Epoch 49 | Loss = -35.0573619113249\n",
      "Epoch 50 | Loss = -35.67667021470911\n",
      "Epoch 51 | Loss = -35.92914589713602\n",
      "Epoch 52 | Loss = -35.429231699775244\n",
      "Epoch 53 | Loss = -37.27310172249289\n",
      "Epoch 54 | Loss = -37.538794601664826\n",
      "Epoch 55 | Loss = -37.987285754259894\n",
      "Epoch 56 | Loss = -37.587515971239874\n",
      "Epoch 57 | Loss = -38.23947502585018\n",
      "Epoch 58 | Loss = -38.9140502985786\n",
      "Epoch 59 | Loss = -39.107305498684156\n",
      "Epoch 60 | Loss = -39.20797087164486\n",
      "Epoch 61 | Loss = -39.68486687716316\n",
      "Epoch 62 | Loss = -39.90410089492798\n",
      "Epoch 63 | Loss = -40.38340607811423\n",
      "Epoch 64 | Loss = -40.43406480901382\n",
      "Epoch 65 | Loss = -40.80008035547593\n",
      "Epoch 66 | Loss = -40.42440664066988\n",
      "Epoch 67 | Loss = -41.210942408617804\n",
      "Epoch 68 | Loss = -41.17159464780022\n",
      "Epoch 69 | Loss = -41.32214882794548\n",
      "Epoch 70 | Loss = -41.74611958335428\n",
      "Epoch 71 | Loss = -41.93551069147446\n",
      "Epoch 72 | Loss = -42.17347736919628\n",
      "Epoch 73 | Loss = -42.85511636734009\n",
      "Epoch 74 | Loss = -43.12143283731797\n",
      "Epoch 75 | Loss = -43.18921330395867\n",
      "Epoch 76 | Loss = -43.68858157887178\n",
      "Epoch 77 | Loss = -43.68840256859274\n",
      "Epoch 78 | Loss = -44.44620393304264\n",
      "Epoch 79 | Loss = -44.29289276459638\n",
      "Epoch 80 | Loss = -44.70166472827687\n",
      "Epoch 81 | Loss = -44.48247079288258\n",
      "Epoch 82 | Loss = -45.141675023471606\n",
      "Epoch 83 | Loss = -45.587235647089344\n",
      "Epoch 84 | Loss = -45.3314905166626\n",
      "Epoch 85 | Loss = -45.549390624551215\n",
      "Epoch 86 | Loss = -45.247545045964856\n",
      "Epoch 87 | Loss = -45.99301489661722\n",
      "Epoch 88 | Loss = -46.30461474025951\n",
      "Epoch 89 | Loss = -46.56021538902731\n",
      "Epoch 90 | Loss = -46.36970576118021\n",
      "Epoch 91 | Loss = -46.81142125410192\n",
      "Epoch 92 | Loss = -47.094990253448486\n",
      "Epoch 93 | Loss = -46.63743439842673\n",
      "Epoch 94 | Loss = -47.19324055839987\n",
      "Epoch 95 | Loss = -47.18721440259148\n",
      "Epoch 96 | Loss = -47.490456244524786\n",
      "Epoch 97 | Loss = -48.06486452327055\n",
      "Epoch 98 | Loss = -47.611916934742645\n",
      "Epoch 99 | Loss = -48.50565394233255\n",
      "Epoch 100 | Loss = -48.46919525370878\n",
      "Epoch 101 | Loss = -48.87314664616304\n",
      "Epoch 102 | Loss = -48.99213333690868\n",
      "Epoch 103 | Loss = -48.733266409705664\n",
      "Epoch 104 | Loss = -49.03923486260807\n",
      "Epoch 105 | Loss = -49.06384838328642\n",
      "Epoch 106 | Loss = -49.316048229441925\n",
      "Epoch 107 | Loss = -49.311466329237994\n",
      "Epoch 108 | Loss = -49.690637167762304\n",
      "Epoch 109 | Loss = -49.94230427461512\n",
      "Epoch 110 | Loss = -49.909498663509595\n",
      "Epoch 111 | Loss = -49.569814794203815\n",
      "Epoch 112 | Loss = -50.09469259486479\n",
      "Epoch 113 | Loss = -50.045706159928265\n",
      "Epoch 114 | Loss = -50.33902712429271\n",
      "Epoch 115 | Loss = -50.60593706018784\n",
      "Epoch 116 | Loss = -50.468690900241626\n",
      "Epoch 117 | Loss = -51.03364980922026\n",
      "Epoch 118 | Loss = -51.01195453195011\n",
      "Epoch 119 | Loss = -50.91882652394912\n",
      "Epoch 120 | Loss = -50.80520596223719\n",
      "Epoch 121 | Loss = -51.17981602163876\n",
      "Epoch 122 | Loss = -51.21256048539106\n",
      "Epoch 123 | Loss = -51.356913314146155\n",
      "Epoch 124 | Loss = -51.65001874811509\n",
      "Epoch 125 | Loss = -51.42588842616362\n",
      "Epoch 126 | Loss = -51.56709957122803\n",
      "Epoch 127 | Loss = -51.70317613377291\n",
      "Epoch 128 | Loss = -52.19470040938433\n",
      "Epoch 129 | Loss = -52.116223110872156\n",
      "Epoch 130 | Loss = -52.342936880448285\n",
      "Epoch 131 | Loss = -52.31204299365773\n",
      "Epoch 132 | Loss = -52.43400453118717\n",
      "Epoch 133 | Loss = -52.70859628565171\n",
      "Epoch 134 | Loss = -52.61399667403277\n",
      "Epoch 135 | Loss = -52.451806994045484\n",
      "Epoch 136 | Loss = -52.562444153954004\n",
      "Epoch 137 | Loss = -52.879681390874524\n",
      "Epoch 138 | Loss = -52.92699463227216\n",
      "Epoch 139 | Loss = -52.93009326037239\n",
      "Epoch 140 | Loss = -53.152444194344916\n",
      "Epoch 141 | Loss = -52.68379808874691\n",
      "Epoch 142 | Loss = -53.041225489448095\n",
      "Epoch 143 | Loss = -52.890274468590235\n",
      "Epoch 144 | Loss = -53.06889699487125\n",
      "Epoch 145 | Loss = -53.368277577792895\n",
      "Epoch 146 | Loss = -53.517798592062555\n",
      "Epoch 147 | Loss = -53.79526082207175\n",
      "Epoch 148 | Loss = -53.99021252463846\n",
      "Epoch 149 | Loss = -53.7023598727058\n",
      "Epoch 150 | Loss = -53.84539870654835\n",
      "Epoch 151 | Loss = -53.59110818189733\n",
      "Epoch 152 | Loss = -54.00136613845825\n",
      "Epoch 153 | Loss = -53.79303029004265\n",
      "Epoch 154 | Loss = -54.14277222577263\n",
      "Epoch 155 | Loss = -53.914277609656835\n",
      "Epoch 156 | Loss = -54.12679576873779\n",
      "Epoch 157 | Loss = -54.44403996187098\n",
      "Epoch 158 | Loss = -54.32126522064209\n",
      "Epoch 159 | Loss = -54.443864822387695\n",
      "Epoch 160 | Loss = -54.77519823523129\n",
      "Epoch 161 | Loss = -54.680768097148224\n",
      "Epoch 162 | Loss = -54.800882507773004\n",
      "Epoch 163 | Loss = -54.82464523876415\n",
      "Epoch 164 | Loss = -54.74199062235215\n",
      "Epoch 165 | Loss = -54.852315453922046\n",
      "Epoch 166 | Loss = -54.93881149852977\n",
      "Epoch 167 | Loss = -54.832080252030316\n",
      "Epoch 168 | Loss = -54.95422472673304\n",
      "Epoch 169 | Loss = -55.04453631008373\n",
      "Epoch 170 | Loss = -54.88645792007446\n",
      "Epoch 171 | Loss = -54.99442641875323\n",
      "Epoch 172 | Loss = -55.0222534011392\n",
      "Epoch 173 | Loss = -55.25223631017349\n",
      "Epoch 174 | Loss = -55.1983998523039\n",
      "Epoch 175 | Loss = -55.214074695811554\n",
      "Epoch 176 | Loss = -54.92939651713652\n",
      "Epoch 177 | Loss = -54.98179295483757\n",
      "Epoch 178 | Loss = -54.921763616449695\n",
      "Epoch 179 | Loss = -55.313872786129224\n",
      "Epoch 180 | Loss = -54.833726546343634\n",
      "Epoch 181 | Loss = -55.092726819655475\n",
      "Epoch 182 | Loss = -55.03650028565351\n",
      "Epoch 183 | Loss = -55.26054312201107\n",
      "Epoch 184 | Loss = -55.48345433964449\n",
      "Epoch 185 | Loss = -55.420406481798956\n",
      "Epoch 186 | Loss = -55.42655364204855\n",
      "Epoch 187 | Loss = -55.62608441184549\n",
      "Epoch 188 | Loss = -55.366409722496485\n",
      "Epoch 189 | Loss = -55.643527171191046\n",
      "Epoch 190 | Loss = -55.739148280199835\n",
      "Epoch 191 | Loss = -55.84138157788445\n",
      "Epoch 192 | Loss = -55.885625418494726\n",
      "Epoch 193 | Loss = -55.85859265046961\n",
      "Epoch 194 | Loss = -56.11265277862549\n",
      "Epoch 195 | Loss = -56.233886550454535\n",
      "Epoch 196 | Loss = -56.39426638098324\n",
      "Epoch 197 | Loss = -56.25289445764878\n",
      "Epoch 198 | Loss = -56.41785357980167\n",
      "Epoch 199 | Loss = -56.38416666143081\n",
      "Epoch 200 | Loss = -56.487499461454505\n",
      "+-----------------------------------------------------------------------------+\n",
      "Training Time: 24048.22526192665 seconds.\n",
      "Get Embedding...\n",
      "(118, 1536) (118,)\n",
      "SVC Accuracy: [0.7613636363636365, 0.734090909090909]\n",
      "+-----------------------------------------------------------------------------+\n",
      "Embedding Time: 15.444741487503052 seconds.\n",
      "Experient 5\n",
      "20211219-073845 :Log the record!\n",
      "+-----------------------------------------------------------------------------+\n",
      "Experient 6\n",
      "!!!\n",
      "89\n",
      "!!!\n",
      "[553, 217, 346, 850, 1139, 72, 280, 861, 916, 589, 50, 182, 307, 1067, 172, 583, 831, 791, 1014, 246, 18, 1013, 59, 1023, 984, 69, 755, 65, 71, 742, 1153, 256, 856, 266, 427, 956, 679, 826, 541, 147, 910, 0, 922, 683, 131, 103, 91, 1001, 166, 558, 796, 1078, 811, 852, 351, 1002, 383, 145, 197, 1072, 789, 17, 606, 9, 608, 1111, 664, 414, 154, 290, 954, 667, 959, 1097, 652, 339, 184, 368, 327, 506, 933, 370, 36, 768, 67, 1158, 93, 1123, 47, 624, 311, 883, 484, 702, 379, 316, 794, 822, 818, 55, 375, 1008, 605, 584, 920, 575, 986, 675, 82, 468, 1104, 202, 428, 566, 85, 550, 914, 1142, 545, 1145, 870, 1149, 332, 732, 185, 401, 359, 540, 714, 277, 785, 56, 45, 330, 1137, 564, 941, 703, 157, 1056, 1025, 325, 394, 360, 1077, 611, 1058, 1131, 830, 1081, 857, 1105, 364, 57, 233, 291, 1065, 926, 181, 231, 230, 808, 731, 678, 588, 929, 226, 537, 680, 38, 746, 595, 733, 244, 138, 1039, 441, 216, 762, 220, 284, 570, 1166, 1103, 117, 476, 261, 312, 232, 424, 149, 1110, 183, 651, 776, 522, 898, 381, 1100, 1088, 221, 1043, 596, 1084, 719, 669, 253, 715, 498, 35, 112, 495, 524, 548, 369, 735, 574, 580, 391, 167, 293, 942, 793, 273, 204, 1049, 474, 292, 973, 129, 1006, 402, 444, 975, 137, 1071, 647, 656, 1080, 1045, 333, 648, 555, 1127, 576, 486, 602, 989, 399, 3, 1030, 1016, 782, 153, 962, 421, 620, 1036, 264, 944, 848, 736, 717, 26, 1121, 978, 338, 398, 449, 46, 778, 877, 773, 633, 287, 465, 957, 828, 462, 800, 419, 902, 709, 953, 549, 997, 724, 78, 779, 1019, 2, 1050, 546, 1048, 195, 1061, 995, 1032, 10, 25, 450, 357, 1135, 254, 559, 612, 912, 965, 179, 94, 739, 470, 616, 298, 900, 224, 598, 301, 436, 1099, 234, 1060, 1053, 839, 815, 158, 804, 945, 89, 31, 210, 365, 993, 413, 225, 115, 618, 42, 567, 320, 1125, 644, 532, 337, 513, 168, 609, 243, 494, 186, 734, 1159, 193, 439, 305, 641, 628, 300, 521, 636, 948, 198, 994, 556, 96, 1130, 568, 22, 557, 1021, 132, 938, 707, 463, 723, 1152, 915, 711, 156, 631, 931, 457, 1066, 433, 666, 464, 459, 597, 816, 471, 725, 614, 1063, 397, 716, 684, 176, 227, 577, 1004, 801, 637, 34, 761, 302, 404, 1090, 980, 1028, 262, 161, 426, 751, 617, 1000, 39, 345, 504, 880, 907, 409, 173, 515, 249, 1069, 285, 1156, 730, 350, 260, 150, 281, 923, 361, 1062, 918, 1052, 904, 282, 1095, 219, 503, 738, 88, 668, 563, 710, 946, 207, 146, 60, 340, 760, 691, 106, 827, 265, 571, 15, 1132, 1015, 985, 825, 970, 758, 223, 406, 921, 1044, 772, 120, 247, 4, 542, 101, 859, 467, 1150, 12, 871, 268, 1012, 630, 1009, 336, 322, 621, 491, 1026, 187, 629, 432, 385, 992, 809, 1024, 756, 366, 62, 188, 53, 899, 950, 939, 1175, 988, 13, 105, 692, 659, 160, 960, 134, 875, 492, 107, 505, 757, 932, 653, 533, 175, 342, 314, 1165, 936, 1114, 252, 639, 8, 695, 838, 539, 824, 458, 408, 508, 248, 148, 352, 1162, 663, 911, 578, 318, 1116, 108, 1106, 235, 844, 54, 889, 645, 586, 1068, 239, 326, 763, 689, 104, 81, 323, 48, 1031, 908, 1041, 851, 1124, 452, 961, 122, 127, 1108, 388, 215, 155, 73, 887, 475, 77, 348, 485, 84, 1148, 650, 509, 75, 299, 331, 144, 241, 288, 727, 619, 775, 625, 206, 977, 455, 934, 697, 777, 865, 245, 835, 585, 133, 70, 655, 1161, 1092, 892, 190, 7, 32, 807, 110, 693, 1134, 1064, 295, 19, 872, 729, 1154, 289, 1176, 657, 142, 890, 445, 952, 275, 866, 587, 927, 603, 943, 211, 1118, 728, 593, 701, 477, 222, 196, 440, 205, 531, 981, 354, 478, 165, 242, 958, 1115, 1075, 1074, 1007, 24, 967, 913, 49, 744, 11, 554, 201, 321, 437, 517, 999, 814, 529, 681, 236, 180, 750, 780, 135, 868, 516, 879, 820, 315, 1128, 560, 218, 845, 297, 949, 841, 1059, 309, 1167, 1155, 665, 425, 1172, 461, 812, 79, 143, 622, 983, 937, 438, 353, 832, 6, 901, 795, 990, 759, 869, 1120, 276, 686, 810, 199, 371, 174, 677, 627, 116, 1133, 435, 519, 113, 496, 1029, 706, 255, 764, 130, 1005, 525, 500, 453, 367, 720, 171, 278, 483, 1122, 754, 698, 1051, 599, 1126, 74, 867, 726, 885, 250, 191, 1027, 229, 996, 310, 209, 884, 111, 456, 1082, 76, 358, 343, 66, 356, 213, 874, 1164, 362, 813, 1138, 5, 1055, 324, 888, 480, 849, 1033, 228, 267, 410, 786, 1035, 139, 873, 972, 740, 303, 951, 304, 1140, 543, 682, 382, 393, 896, 488, 718, 447, 1054, 417, 643, 43, 964, 14, 237, 189, 662, 29, 306, 1144, 119, 654, 86, 1107, 783, 511, 876, 1136, 696, 1146, 1086, 507, 341, 649, 771, 169, 274, 37, 376, 317, 279, 395, 840, 966, 251, 687, 878, 787, 159, 600, 1087, 905, 319, 1160, 378, 1094, 263, 52, 987, 344, 308, 363, 416, 396, 924, 769, 688, 33, 615, 460, 979, 906, 121, 895, 1171, 102, 579, 526, 882, 1168, 177, 523, 380, 1076, 864, 1085, 125, 674, 83, 781, 64, 124, 1047, 51, 20, 661, 767, 258, 151, 833, 638, 1109, 590, 635, 1169, 930, 372, 448, 982, 991, 846, 538, 374, 792, 446, 708, 528, 283, 819, 1089, 858, 1038, 806, 466, 493, 1174, 347, 114, 136, 128, 863, 935, 1093, 415, 269, 510, 1129, 834, 1, 389, 68, 749, 963, 552, 487, 430, 212, 971, 847, 745, 194, 162, 713, 296, 1011, 592, 582, 917, 594, 429, 853, 1073, 784, 520, 499, 469, 451, 1079, 572, 862, 753, 737, 27, 1117, 547, 947, 123, 894, 1112, 855, 384, 1034, 422, 490, 712, 843, 192, 1173, 203, 634, 514, 928, 694, 423, 141, 420, 329, 1017, 1119, 377, 1046, 527, 998, 349, 272, 454, 1057, 501, 178, 392, 405, 774, 472, 238, 1042, 534, 699, 482, 1018, 569, 842, 164, 660, 1113, 536, 817, 95, 240, 387, 854, 126, 561, 743, 431, 672, 802, 208, 766, 690, 390, 1101, 591, 604, 109, 909, 803, 28, 286, 765, 163, 601, 1163, 722, 90, 1177, 1037, 152, 1098, 805, 334, 294, 829, 700, 974, 21]\n",
      "Number of Graphs (dataset_train): 1060\n",
      "Number of Graphs (dataset_test): 118\n",
      "+-----------------------------------------------------------------------------+\n",
      "Learning Rate: 0.001\n",
      "Epochs: 200\n",
      "Batch Size: 128\n",
      "Hidden Dimension 64\n",
      "Number of Layer in Encoder: 1\n",
      "Opimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device: cuda\n",
      "Model:\n",
      "SimCLR(\n",
      "  (encoder): Encoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=89, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      ))\n",
      "    )\n",
      "    (bns): ModuleList(\n",
      "      (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Projection_MLP(\n",
      "    (layer1): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (unilayer): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Start Training...\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss = 218.04321604304843\n",
      "Epoch 2 | Loss = 187.73811015155582\n",
      "Epoch 3 | Loss = 165.60232883029514\n",
      "Epoch 4 | Loss = 158.46901634997792\n",
      "Epoch 5 | Loss = 147.40726663006677\n",
      "Epoch 6 | Loss = 141.56539216306476\n",
      "Epoch 7 | Loss = 136.2381116549174\n",
      "Epoch 8 | Loss = 129.357203218672\n",
      "Epoch 9 | Loss = 124.9133169915941\n",
      "Epoch 10 | Loss = 124.20018882221646\n",
      "Epoch 11 | Loss = 119.41877839300368\n",
      "Epoch 12 | Loss = 116.60494433508978\n",
      "Epoch 13 | Loss = 111.99120791753133\n",
      "Epoch 14 | Loss = 111.94571468565199\n",
      "Epoch 15 | Loss = 109.79459600978427\n",
      "Epoch 16 | Loss = 108.51464176177979\n",
      "Epoch 17 | Loss = 106.69400421778361\n",
      "Epoch 18 | Loss = 104.84774030579462\n",
      "Epoch 19 | Loss = 101.88744399282668\n",
      "Epoch 20 | Loss = 103.09764019648235\n",
      "Epoch 21 | Loss = 101.53460219171312\n",
      "Epoch 22 | Loss = 97.86140608787537\n",
      "Epoch 23 | Loss = 99.58856524361505\n",
      "Epoch 24 | Loss = 98.80790789922078\n",
      "Epoch 25 | Loss = 96.83947417471144\n",
      "Epoch 27 | Loss = 94.67829881774054\n",
      "Epoch 28 | Loss = 92.5369261900584\n",
      "Epoch 29 | Loss = 93.83907792303297\n",
      "Epoch 30 | Loss = 90.3043003877004\n",
      "Epoch 31 | Loss = 89.01998906665378\n",
      "Epoch 32 | Loss = 87.51047174135844\n",
      "Epoch 33 | Loss = 89.05868699815538\n",
      "Epoch 34 | Loss = 87.24309153027005\n",
      "Epoch 35 | Loss = 87.67570016119215\n",
      "Epoch 36 | Loss = 87.81234198146396\n",
      "Epoch 37 | Loss = 87.55806483162775\n",
      "Epoch 38 | Loss = 84.35012186898126\n",
      "Epoch 39 | Loss = 86.98882630136278\n",
      "Epoch 40 | Loss = 85.79781158765157\n",
      "Epoch 41 | Loss = 82.80043376816644\n",
      "Epoch 42 | Loss = 85.91746558083429\n",
      "Epoch 43 | Loss = 83.02583310339186\n",
      "Epoch 44 | Loss = 82.3781577481164\n",
      "Epoch 45 | Loss = 83.08151250415378\n",
      "Epoch 46 | Loss = 82.87467228041754\n",
      "Epoch 47 | Loss = 82.00977857907613\n",
      "Epoch 48 | Loss = 84.31988657845392\n",
      "Epoch 49 | Loss = 81.7773945066664\n",
      "Epoch 50 | Loss = 82.36888382169936\n",
      "Epoch 51 | Loss = 81.60170229276021\n",
      "Epoch 52 | Loss = 81.51536056730482\n",
      "Epoch 53 | Loss = 78.82963551415338\n",
      "Epoch 54 | Loss = 80.92954579989116\n",
      "Epoch 55 | Loss = 78.16783727539911\n",
      "Epoch 56 | Loss = 76.8154788017273\n",
      "Epoch 57 | Loss = 78.26417038175795\n",
      "Epoch 58 | Loss = 78.40950136714511\n",
      "Epoch 59 | Loss = 78.63023312886556\n",
      "Epoch 60 | Loss = 76.85151492224799\n",
      "Epoch 61 | Loss = 77.07350550757513\n",
      "Epoch 62 | Loss = 77.75605805714925\n",
      "Epoch 63 | Loss = 76.97953118218317\n",
      "Epoch 64 | Loss = 76.19500992033217\n",
      "Epoch 65 | Loss = 76.67607869042291\n",
      "Epoch 66 | Loss = 74.63466723759969\n",
      "Epoch 67 | Loss = 74.93466880586412\n",
      "Epoch 68 | Loss = 75.63835051324632\n",
      "Epoch 69 | Loss = 75.02595154444377\n",
      "Epoch 70 | Loss = 75.27902332941692\n",
      "Epoch 71 | Loss = 74.12921990288629\n",
      "Epoch 72 | Loss = 73.4517863061693\n",
      "Epoch 73 | Loss = 74.10242552227444\n",
      "Epoch 74 | Loss = 73.94965765211317\n",
      "Epoch 75 | Loss = 72.33857366773817\n",
      "Epoch 76 | Loss = 72.07190201017592\n",
      "Epoch 77 | Loss = 72.0368611547682\n",
      "Epoch 78 | Loss = 70.40485186047025\n",
      "Epoch 79 | Loss = 71.95832671059503\n",
      "Epoch 80 | Loss = 72.68433237075806\n",
      "Epoch 81 | Loss = 72.92535909016927\n",
      "Epoch 82 | Loss = 71.55374638239543\n",
      "Epoch 83 | Loss = 70.5573295487298\n",
      "Epoch 84 | Loss = 70.81126769383748\n",
      "Epoch 85 | Loss = 70.82302580939398\n",
      "Epoch 86 | Loss = 71.38781118392944\n",
      "Epoch 87 | Loss = 70.5089839829339\n",
      "Epoch 88 | Loss = 69.32552433013916\n",
      "Epoch 89 | Loss = 70.71058962080214\n",
      "Epoch 90 | Loss = 70.21194807688396\n",
      "Epoch 91 | Loss = 69.61930327945285\n",
      "Epoch 92 | Loss = 69.65951702329848\n",
      "Epoch 93 | Loss = 69.59268177880182\n",
      "Epoch 94 | Loss = 68.07005219989352\n",
      "Epoch 95 | Loss = 68.95102622773912\n",
      "Epoch 96 | Loss = 68.85165929794312\n",
      "Epoch 97 | Loss = 70.36055564880371\n",
      "Epoch 98 | Loss = 66.27667464150323\n",
      "Epoch 99 | Loss = 67.87132607565985\n",
      "Epoch 100 | Loss = 66.60072162416246\n",
      "Epoch 101 | Loss = 65.87800571653578\n",
      "Epoch 102 | Loss = 69.08754385842218\n",
      "Epoch 103 | Loss = 65.71465322706435\n",
      "Epoch 104 | Loss = 67.39973545074463\n",
      "Epoch 105 | Loss = 65.80791674719916\n",
      "Epoch 106 | Loss = 67.03663190205891\n",
      "Epoch 107 | Loss = 66.05714755588107\n",
      "Epoch 108 | Loss = 66.40572272406683\n",
      "Epoch 109 | Loss = 64.99199342727661\n",
      "Epoch 110 | Loss = 66.03118838204279\n",
      "Epoch 111 | Loss = 64.34920946756999\n",
      "Epoch 112 | Loss = 64.77357281578912\n",
      "Epoch 113 | Loss = 65.56720802519057\n",
      "Epoch 114 | Loss = 65.58279418945312\n",
      "Epoch 115 | Loss = 65.33659712473552\n",
      "Epoch 116 | Loss = 65.18231513765123\n",
      "Epoch 117 | Loss = 65.79082335366144\n",
      "Epoch 118 | Loss = 64.6682243347168\n",
      "Epoch 119 | Loss = 65.75491078694661\n",
      "Epoch 120 | Loss = 64.34626807106866\n",
      "Epoch 121 | Loss = 63.435481548309326\n",
      "Epoch 122 | Loss = 64.8035651312934\n",
      "Epoch 123 | Loss = 64.44561566246881\n",
      "Epoch 124 | Loss = 64.05025747087267\n",
      "Epoch 125 | Loss = 63.35487434599135\n",
      "Epoch 126 | Loss = 63.30454524358114\n",
      "Epoch 127 | Loss = 63.66033914354112\n",
      "Epoch 128 | Loss = 62.53135257297092\n",
      "Epoch 129 | Loss = 62.34314510557387\n",
      "Epoch 130 | Loss = 64.29102738698323\n",
      "Epoch 131 | Loss = 62.932324197557236\n",
      "Epoch 132 | Loss = 61.87940735287137\n",
      "Epoch 133 | Loss = 62.72575023439195\n",
      "Epoch 134 | Loss = 63.06442520353529\n",
      "Epoch 135 | Loss = 62.98704269197252\n",
      "Epoch 136 | Loss = 64.9403599633111\n",
      "Epoch 137 | Loss = 61.13684156205919\n",
      "Epoch 138 | Loss = 63.136724683973526\n",
      "Epoch 139 | Loss = 63.799662325117325\n",
      "Epoch 140 | Loss = 63.01738331052992\n",
      "Epoch 141 | Loss = 60.937866316901314\n",
      "Epoch 142 | Loss = 62.98411220974393\n",
      "Epoch 143 | Loss = 62.48643594317966\n",
      "Epoch 144 | Loss = 59.96211650636461\n",
      "Epoch 145 | Loss = 62.200181272294785\n",
      "Epoch 146 | Loss = 60.605353196462\n",
      "Epoch 147 | Loss = 62.948222637176514\n",
      "Epoch 148 | Loss = 60.41760932074653\n",
      "Epoch 149 | Loss = 60.32061682807075\n",
      "Epoch 150 | Loss = 61.24784114625719\n",
      "Epoch 151 | Loss = 62.4951900906033\n",
      "Epoch 152 | Loss = 61.17462078730265\n",
      "Epoch 153 | Loss = 59.94756767484877\n",
      "Epoch 154 | Loss = 59.80776039759318\n",
      "Epoch 155 | Loss = 59.78304518593682\n",
      "Epoch 156 | Loss = 58.609308030870224\n",
      "Epoch 157 | Loss = 60.30643293592665\n",
      "Epoch 158 | Loss = 59.27486742867364\n",
      "Epoch 159 | Loss = 59.31028827031454\n",
      "Epoch 160 | Loss = 60.824938244289825\n",
      "Epoch 161 | Loss = 58.72512239880032\n",
      "Epoch 162 | Loss = 59.70074309243096\n",
      "Epoch 163 | Loss = 60.00531848271688\n",
      "Epoch 164 | Loss = 58.86616828706529\n",
      "Epoch 165 | Loss = 58.2738806936476\n",
      "Epoch 166 | Loss = 58.10224199295044\n",
      "Epoch 167 | Loss = 58.11220693588257\n",
      "Epoch 168 | Loss = 58.20484744177924\n",
      "Epoch 169 | Loss = 58.592944886949326\n",
      "Epoch 170 | Loss = 58.48636711968316\n",
      "Epoch 171 | Loss = 59.338856008317734\n",
      "Epoch 172 | Loss = 59.54618814256456\n",
      "Epoch 173 | Loss = 59.1459002494812\n",
      "Epoch 174 | Loss = 60.20686149597168\n",
      "Epoch 175 | Loss = 58.76229174931844\n",
      "Epoch 176 | Loss = 57.734385914272735\n",
      "Epoch 177 | Loss = 59.91080702675713\n",
      "Epoch 178 | Loss = 56.55583005481296\n",
      "Epoch 179 | Loss = 56.79375256432427\n",
      "Epoch 180 | Loss = 58.50696871015761\n",
      "Epoch 181 | Loss = 55.70054091347588\n",
      "Epoch 182 | Loss = 56.77415784200033\n",
      "Epoch 183 | Loss = 57.084278636508515\n",
      "Epoch 184 | Loss = 57.38710970348782\n",
      "Epoch 185 | Loss = 57.78945938746134\n",
      "Epoch 186 | Loss = 57.31425454881456\n",
      "Epoch 187 | Loss = 58.607843187120224\n",
      "Epoch 188 | Loss = 58.61857589085897\n",
      "Epoch 189 | Loss = 57.70343277189467\n",
      "Epoch 190 | Loss = 55.65122885174222\n",
      "Epoch 191 | Loss = 56.408249855041504\n",
      "Epoch 192 | Loss = 56.43360291586982\n",
      "Epoch 193 | Loss = 58.82973496119181\n",
      "Epoch 194 | Loss = 56.45127375920614\n",
      "Epoch 195 | Loss = 56.80361790127225\n",
      "Epoch 196 | Loss = 56.357425371805824\n",
      "Epoch 197 | Loss = 56.54215245776706\n",
      "Epoch 198 | Loss = 56.34810972213745\n",
      "Epoch 199 | Loss = 55.97763464185927\n",
      "Epoch 200 | Loss = 56.06278573142158\n",
      "+-----------------------------------------------------------------------------+\n",
      "Training Time: 25686.416550159454 seconds.\n",
      "Get Embedding...\n",
      "(118, 64) (118,)\n",
      "SVC Accuracy: [0.7537878787878789, 0.7553030303030304]\n",
      "+-----------------------------------------------------------------------------+\n",
      "Embedding Time: 12.989705324172974 seconds.\n",
      "Experient 6\n",
      "20211219-144705 :Log the record!\n",
      "+-----------------------------------------------------------------------------+\n",
      "Experient 7\n",
      "!!!\n",
      "89\n",
      "!!!\n",
      "[553, 217, 346, 850, 1139, 72, 280, 861, 916, 589, 50, 182, 307, 1067, 172, 583, 831, 791, 1014, 246, 18, 1013, 59, 1023, 984, 69, 755, 65, 71, 742, 1153, 256, 856, 266, 427, 956, 679, 826, 541, 147, 910, 0, 922, 683, 131, 103, 91, 1001, 166, 558, 796, 1078, 811, 852, 351, 1002, 383, 145, 197, 1072, 789, 17, 606, 9, 608, 1111, 664, 414, 154, 290, 954, 667, 959, 1097, 652, 339, 184, 368, 327, 506, 933, 370, 36, 768, 67, 1158, 93, 1123, 47, 624, 311, 883, 484, 702, 379, 316, 794, 822, 818, 55, 375, 1008, 605, 584, 920, 575, 986, 675, 82, 468, 1104, 202, 428, 566, 85, 550, 914, 1142, 545, 1145, 870, 1149, 332, 732, 185, 401, 359, 540, 714, 277, 785, 56, 45, 330, 1137, 564, 941, 703, 157, 1056, 1025, 325, 394, 360, 1077, 611, 1058, 1131, 830, 1081, 857, 1105, 364, 57, 233, 291, 1065, 926, 181, 231, 230, 808, 731, 678, 588, 929, 226, 537, 680, 38, 746, 595, 733, 244, 138, 1039, 441, 216, 762, 220, 284, 570, 1166, 1103, 117, 476, 261, 312, 232, 424, 149, 1110, 183, 651, 776, 522, 898, 381, 1100, 1088, 221, 1043, 596, 1084, 719, 669, 253, 715, 498, 35, 112, 495, 524, 548, 369, 735, 574, 580, 391, 167, 293, 942, 793, 273, 204, 1049, 474, 292, 973, 129, 1006, 402, 444, 975, 137, 1071, 647, 656, 1080, 1045, 333, 648, 555, 1127, 576, 486, 602, 989, 399, 3, 1030, 1016, 782, 153, 962, 421, 620, 1036, 264, 944, 848, 736, 717, 26, 1121, 978, 338, 398, 449, 46, 778, 877, 773, 633, 287, 465, 957, 828, 462, 800, 419, 902, 709, 953, 549, 997, 724, 78, 779, 1019, 2, 1050, 546, 1048, 195, 1061, 995, 1032, 10, 25, 450, 357, 1135, 254, 559, 612, 912, 965, 179, 94, 739, 470, 616, 298, 900, 224, 598, 301, 436, 1099, 234, 1060, 1053, 839, 815, 158, 804, 945, 89, 31, 210, 365, 993, 413, 225, 115, 618, 42, 567, 320, 1125, 644, 532, 337, 513, 168, 609, 243, 494, 186, 734, 1159, 193, 439, 305, 641, 628, 300, 521, 636, 948, 198, 994, 556, 96, 1130, 568, 22, 557, 1021, 132, 938, 707, 463, 723, 1152, 915, 711, 156, 631, 931, 457, 1066, 433, 666, 464, 459, 597, 816, 471, 725, 614, 1063, 397, 716, 684, 176, 227, 577, 1004, 801, 637, 34, 761, 302, 404, 1090, 980, 1028, 262, 161, 426, 751, 617, 1000, 39, 345, 504, 880, 907, 409, 173, 515, 249, 1069, 285, 1156, 730, 350, 260, 150, 281, 923, 361, 1062, 918, 1052, 904, 282, 1095, 219, 503, 738, 88, 668, 563, 710, 946, 207, 146, 60, 340, 760, 691, 106, 827, 265, 571, 15, 1132, 1015, 985, 825, 970, 758, 223, 406, 921, 1044, 772, 120, 247, 4, 542, 101, 859, 467, 1150, 12, 871, 268, 1012, 630, 1009, 336, 322, 621, 491, 1026, 187, 629, 432, 385, 992, 809, 1024, 756, 366, 62, 188, 53, 899, 950, 939, 1175, 988, 13, 105, 692, 659, 160, 960, 134, 875, 492, 107, 505, 757, 932, 653, 533, 175, 342, 314, 1165, 936, 1114, 252, 639, 8, 695, 838, 539, 824, 458, 408, 508, 248, 148, 352, 1162, 663, 911, 578, 318, 1116, 108, 1106, 235, 844, 54, 889, 645, 586, 1068, 239, 326, 763, 689, 104, 81, 323, 48, 1031, 908, 1041, 851, 1124, 452, 961, 122, 127, 1108, 388, 215, 155, 73, 887, 475, 77, 348, 485, 84, 1148, 650, 509, 75, 299, 331, 144, 241, 288, 727, 619, 775, 625, 206, 977, 455, 934, 697, 777, 865, 245, 835, 585, 133, 70, 655, 1161, 1092, 892, 190, 7, 32, 807, 110, 693, 1134, 1064, 295, 19, 872, 729, 1154, 289, 1176, 657, 142, 890, 445, 952, 275, 866, 587, 927, 603, 943, 211, 1118, 728, 593, 701, 477, 222, 196, 440, 205, 531, 981, 354, 478, 165, 242, 958, 1115, 1075, 1074, 1007, 24, 967, 913, 49, 744, 11, 554, 201, 321, 437, 517, 999, 814, 529, 681, 236, 180, 750, 780, 135, 868, 516, 879, 820, 315, 1128, 560, 218, 845, 297, 949, 841, 1059, 309, 1167, 1155, 665, 425, 1172, 461, 812, 79, 143, 622, 983, 937, 438, 353, 832, 6, 901, 795, 990, 759, 869, 1120, 276, 686, 810, 199, 371, 174, 677, 627, 116, 1133, 435, 519, 113, 496, 1029, 706, 255, 764, 130, 1005, 525, 500, 453, 367, 720, 171, 278, 483, 1122, 754, 698, 1051, 599, 1126, 74, 867, 726, 885, 250, 191, 1027, 229, 996, 310, 209, 884, 111, 456, 1082, 76, 358, 343, 66, 356, 213, 874, 1164, 362, 813, 1138, 5, 1055, 324, 888, 480, 849, 1033, 228, 267, 410, 786, 1035, 139, 873, 972, 740, 303, 951, 304, 1140, 543, 682, 382, 393, 896, 488, 718, 447, 1054, 417, 643, 43, 964, 14, 237, 189, 662, 29, 306, 1144, 119, 654, 86, 1107, 783, 511, 876, 1136, 696, 1146, 1086, 507, 341, 649, 771, 169, 274, 37, 376, 317, 279, 395, 840, 966, 251, 687, 878, 787, 159, 600, 1087, 905, 319, 1160, 378, 1094, 263, 52, 987, 344, 308, 363, 416, 396, 924, 769, 688, 33, 615, 460, 979, 906, 121, 895, 1171, 102, 579, 526, 882, 1168, 177, 523, 380, 1076, 864, 1085, 125, 674, 83, 781, 64, 124, 1047, 51, 20, 661, 767, 258, 151, 833, 638, 1109, 590, 635, 1169, 930, 372, 448, 982, 991, 846, 538, 374, 792, 446, 708, 528, 283, 819, 1089, 858, 1038, 806, 466, 493, 1174, 347, 114, 136, 128, 863, 935, 1093, 415, 269, 510, 1129, 834, 1, 389, 68, 749, 963, 552, 487, 430, 212, 971, 847, 745, 194, 162, 713, 296, 1011, 592, 582, 917, 594, 429, 853, 1073, 784, 520, 499, 469, 451, 1079, 572, 862, 753, 737, 27, 1117, 547, 947, 123, 894, 1112, 855, 384, 1034, 422, 490, 712, 843, 192, 1173, 203, 634, 514, 928, 694, 423, 141, 420, 329, 1017, 1119, 377, 1046, 527, 998, 349, 272, 454, 1057, 501, 178, 392, 405, 774, 472, 238, 1042, 534, 699, 482, 1018, 569, 842, 164, 660, 1113, 536, 817, 95, 240, 387, 854, 126, 561, 743, 431, 672, 802, 208, 766, 690, 390, 1101, 591, 604, 109, 909, 803, 28, 286, 765, 163, 601, 1163, 722, 90, 1177, 1037, 152, 1098, 805, 334, 294, 829, 700, 974, 21]\n",
      "Number of Graphs (dataset_train): 1060\n",
      "Number of Graphs (dataset_test): 118\n",
      "+-----------------------------------------------------------------------------+\n",
      "Learning Rate: 0.001\n",
      "Epochs: 200\n",
      "Batch Size: 128\n",
      "Hidden Dimension 512\n",
      "Number of Layer in Encoder: 1\n",
      "Opimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device: cuda\n",
      "Model:\n",
      "SimCLR(\n",
      "  (encoder): Encoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=89, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      ))\n",
      "    )\n",
      "    (bns): ModuleList(\n",
      "      (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Projection_MLP(\n",
      "    (layer1): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (unilayer): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Start Training...\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss = 136.58511896928152\n",
      "Epoch 2 | Loss = 107.57777733272977\n",
      "Epoch 3 | Loss = 96.9950646824307\n",
      "Epoch 4 | Loss = 89.25723465283711\n",
      "Epoch 5 | Loss = 87.75640612178378\n",
      "Epoch 6 | Loss = 83.26446872287326\n",
      "Epoch 7 | Loss = 80.69484292136298\n",
      "Epoch 8 | Loss = 76.59148290422227\n",
      "Epoch 9 | Loss = 75.22789695527818\n",
      "Epoch 10 | Loss = 71.77425633536444\n",
      "Epoch 11 | Loss = 70.18563975228204\n",
      "Epoch 12 | Loss = 69.72471009360419\n",
      "Epoch 13 | Loss = 68.41165669759114\n",
      "Epoch 14 | Loss = 66.1051665412055\n",
      "Epoch 15 | Loss = 64.43500402238634\n",
      "Epoch 16 | Loss = 64.59048557281494\n",
      "Epoch 17 | Loss = 60.82656553056505\n",
      "Epoch 18 | Loss = 60.431552992926704\n",
      "Epoch 19 | Loss = 59.85291454527113\n",
      "Epoch 20 | Loss = 57.35891204410129\n",
      "Epoch 21 | Loss = 55.70166042115953\n",
      "Epoch 22 | Loss = 55.57093207041422\n",
      "Epoch 23 | Loss = 54.01707130008273\n",
      "Epoch 24 | Loss = 53.272630267673065\n",
      "Epoch 25 | Loss = 53.71371518241035\n",
      "Epoch 26 | Loss = 53.00819412867228\n",
      "Epoch 27 | Loss = 50.206335597568085\n",
      "Epoch 28 | Loss = 50.7588799794515\n",
      "Epoch 29 | Loss = 50.687251779768204\n",
      "Epoch 30 | Loss = 49.943159421284996\n",
      "Epoch 31 | Loss = 48.943733798133\n",
      "Epoch 32 | Loss = 47.39869409137302\n",
      "Epoch 33 | Loss = 47.938378440009224\n",
      "Epoch 34 | Loss = 46.20665380689833\n",
      "Epoch 35 | Loss = 44.97037728627523\n",
      "Epoch 36 | Loss = 45.17635838190714\n",
      "Epoch 37 | Loss = 43.35323238372803\n",
      "Epoch 38 | Loss = 43.82086870405409\n",
      "Epoch 39 | Loss = 44.33834769990709\n",
      "Epoch 40 | Loss = 42.49162377251519\n",
      "Epoch 41 | Loss = 41.000958601633705\n",
      "Epoch 42 | Loss = 42.828555054134796\n",
      "Epoch 43 | Loss = 42.696499029795326\n",
      "Epoch 44 | Loss = 39.03141080008613\n",
      "Epoch 45 | Loss = 39.91572713851929\n",
      "Epoch 46 | Loss = 39.063001208835175\n",
      "Epoch 47 | Loss = 39.90974871317545\n",
      "Epoch 48 | Loss = 39.44375493791368\n",
      "Epoch 49 | Loss = 37.09688292609321\n",
      "Epoch 50 | Loss = 36.94223393334283\n",
      "Epoch 51 | Loss = 37.441663053300644\n",
      "Epoch 52 | Loss = 34.286158879597984\n",
      "Epoch 53 | Loss = 35.82369041442871\n",
      "Epoch 54 | Loss = 35.695518281724716\n",
      "Epoch 55 | Loss = 35.82444212171767\n",
      "Epoch 56 | Loss = 33.756112045711944\n",
      "Epoch 57 | Loss = 33.8108696937561\n",
      "Epoch 58 | Loss = 34.312927881876625\n",
      "Epoch 59 | Loss = 32.36291032367282\n",
      "Epoch 60 | Loss = 31.809673415289986\n",
      "Epoch 61 | Loss = 31.021352503034805\n",
      "Epoch 62 | Loss = 33.000094731648765\n",
      "Epoch 63 | Loss = 32.60533406999376\n",
      "Epoch 64 | Loss = 30.76275947358873\n",
      "Epoch 65 | Loss = 30.20816405614217\n",
      "Epoch 66 | Loss = 29.025432480706108\n",
      "Epoch 67 | Loss = 29.59882460700141\n",
      "Epoch 68 | Loss = 29.905757586161297\n",
      "Epoch 69 | Loss = 30.024059772491455\n",
      "Epoch 70 | Loss = 29.014092763264973\n",
      "Epoch 71 | Loss = 28.487433645460342\n",
      "Epoch 72 | Loss = 27.86813237931993\n",
      "Epoch 73 | Loss = 26.893614080217148\n",
      "Epoch 74 | Loss = 27.8875167104933\n",
      "Epoch 75 | Loss = 27.23731623755561\n",
      "Epoch 76 | Loss = 27.425241046481663\n",
      "Epoch 77 | Loss = 25.549469576941597\n",
      "Epoch 78 | Loss = 25.581204732259113\n",
      "Epoch 79 | Loss = 25.923327710893417\n",
      "Epoch 80 | Loss = 25.469892448849148\n",
      "Epoch 81 | Loss = 26.321529176500107\n",
      "Epoch 82 | Loss = 24.785534964667427\n",
      "Epoch 83 | Loss = 25.363394260406494\n",
      "Epoch 84 | Loss = 24.788754887051052\n",
      "Epoch 85 | Loss = 23.013462755415176\n",
      "Epoch 86 | Loss = 23.82707272635566\n",
      "Epoch 87 | Loss = 23.784431881374783\n",
      "Epoch 88 | Loss = 22.11929835213555\n",
      "Epoch 89 | Loss = 22.42042615678575\n",
      "Epoch 90 | Loss = 23.4469739596049\n",
      "Epoch 91 | Loss = 22.321116818322075\n",
      "Epoch 92 | Loss = 22.688662740919327\n",
      "Epoch 93 | Loss = 22.60016499625312\n",
      "Epoch 94 | Loss = 22.134663422902424\n",
      "Epoch 95 | Loss = 21.40312597486708\n",
      "Epoch 96 | Loss = 20.74175039927165\n",
      "Epoch 97 | Loss = 21.05262041091919\n",
      "Epoch 98 | Loss = 20.746242576175266\n",
      "Epoch 99 | Loss = 20.85229704115126\n",
      "Epoch 100 | Loss = 19.86992534001668\n",
      "Epoch 101 | Loss = 18.475258085462784\n",
      "Epoch 102 | Loss = 19.672185791863335\n",
      "Epoch 103 | Loss = 19.543479230668808\n",
      "Epoch 104 | Loss = 18.67626847161187\n",
      "Epoch 105 | Loss = 19.687219778696697\n",
      "Epoch 106 | Loss = 18.626546541849773\n",
      "Epoch 107 | Loss = 18.498376422458225\n",
      "Epoch 108 | Loss = 18.006619347466362\n",
      "Epoch 109 | Loss = 18.538651519351536\n",
      "Epoch 110 | Loss = 17.971109443240696\n",
      "Epoch 111 | Loss = 16.795620441436768\n",
      "Epoch 112 | Loss = 16.54052914513482\n",
      "Epoch 113 | Loss = 16.869904200236004\n",
      "Epoch 114 | Loss = 15.952864699893528\n",
      "Epoch 115 | Loss = 16.260742982228596\n",
      "Epoch 116 | Loss = 16.273527993096245\n",
      "Epoch 117 | Loss = 16.09952900144789\n",
      "Epoch 118 | Loss = 16.265355798933243\n",
      "Epoch 119 | Loss = 15.179501268598768\n",
      "Epoch 120 | Loss = 15.341233942243788\n",
      "Epoch 121 | Loss = 15.585086981455484\n",
      "Epoch 122 | Loss = 15.603224383460152\n",
      "Epoch 123 | Loss = 14.606591277652317\n",
      "Epoch 124 | Loss = 15.146159331003824\n",
      "Epoch 125 | Loss = 14.166945934295654\n",
      "Epoch 126 | Loss = 13.749222384558784\n",
      "Epoch 127 | Loss = 13.713390350341797\n",
      "Epoch 128 | Loss = 14.574988259209526\n",
      "Epoch 129 | Loss = 13.726742797427708\n",
      "Epoch 130 | Loss = 14.326973385281033\n",
      "Epoch 131 | Loss = 13.502191967434353\n",
      "Epoch 132 | Loss = 12.420324272579617\n",
      "Epoch 133 | Loss = 12.922945976257324\n",
      "Epoch 134 | Loss = 11.796345763736301\n",
      "Epoch 135 | Loss = 12.712568442026773\n",
      "Epoch 136 | Loss = 12.74379332860311\n",
      "Epoch 137 | Loss = 12.057842042711046\n",
      "Epoch 138 | Loss = 11.576931476593018\n",
      "Epoch 139 | Loss = 10.975648138258192\n",
      "Epoch 140 | Loss = 11.698836379581028\n",
      "Epoch 141 | Loss = 11.263177341885036\n",
      "Epoch 142 | Loss = 10.803519778781467\n",
      "Epoch 143 | Loss = 10.691911909315321\n",
      "Epoch 144 | Loss = 10.886375480228\n",
      "Epoch 145 | Loss = 11.152795632680258\n",
      "Epoch 146 | Loss = 10.709313869476318\n",
      "Epoch 147 | Loss = 9.960865603552925\n",
      "Epoch 148 | Loss = 10.206478065914578\n",
      "Epoch 149 | Loss = 9.439207130008274\n",
      "Epoch 150 | Loss = 10.308914290534126\n",
      "Epoch 151 | Loss = 9.210684988233778\n",
      "Epoch 152 | Loss = 9.407804912990994\n",
      "Epoch 153 | Loss = 8.599712689717611\n",
      "Epoch 154 | Loss = 8.772333092159695\n",
      "Epoch 155 | Loss = 9.541963683234322\n",
      "Epoch 156 | Loss = 8.599807050493029\n",
      "Epoch 157 | Loss = 8.66284810172187\n",
      "Epoch 158 | Loss = 7.796540154351129\n",
      "Epoch 159 | Loss = 7.830299748314752\n",
      "Epoch 160 | Loss = 8.937191486358643\n",
      "Epoch 161 | Loss = 8.311828030480278\n",
      "Epoch 162 | Loss = 8.013420104980469\n",
      "Epoch 163 | Loss = 8.303579330444336\n",
      "Epoch 164 | Loss = 7.920093218485515\n",
      "Epoch 165 | Loss = 7.252171092563206\n",
      "Epoch 166 | Loss = 6.588178899553087\n",
      "Epoch 167 | Loss = 7.021959145863851\n",
      "Epoch 168 | Loss = 6.984866354200575\n",
      "Epoch 169 | Loss = 6.8325826856825085\n",
      "Epoch 170 | Loss = 7.679966184828016\n",
      "Epoch 171 | Loss = 6.85783937242296\n",
      "Epoch 172 | Loss = 6.35749610265096\n",
      "Epoch 173 | Loss = 6.678168720669216\n",
      "Epoch 174 | Loss = 6.237281428443061\n",
      "Epoch 175 | Loss = 5.710172229342991\n",
      "Epoch 176 | Loss = 5.138758129543728\n",
      "Epoch 177 | Loss = 5.011251396603054\n",
      "Epoch 178 | Loss = 5.599543306562635\n",
      "Epoch 179 | Loss = 4.157867219712999\n",
      "Epoch 180 | Loss = 4.339381800757514\n",
      "Epoch 181 | Loss = 4.771515740288629\n",
      "Epoch 182 | Loss = 5.009848170810276\n",
      "Epoch 183 | Loss = 5.22006008360121\n",
      "Epoch 184 | Loss = 4.27256965637207\n",
      "Epoch 185 | Loss = 4.167444070180257\n",
      "Epoch 186 | Loss = 4.2789871957567005\n",
      "Epoch 187 | Loss = 4.93661175833808\n",
      "Epoch 188 | Loss = 3.6346565352545843\n",
      "Epoch 189 | Loss = 4.032664140065511\n",
      "Epoch 190 | Loss = 3.974284013112386\n",
      "Epoch 191 | Loss = 3.3093255360921225\n",
      "Epoch 192 | Loss = 3.8558038075764975\n",
      "Epoch 193 | Loss = 3.8721244600084095\n",
      "Epoch 194 | Loss = 3.227768156263563\n",
      "Epoch 195 | Loss = 3.136532677544488\n",
      "Epoch 196 | Loss = 3.441521750556098\n",
      "Epoch 197 | Loss = 3.758157041337755\n",
      "Epoch 198 | Loss = 2.1554571787516275\n",
      "Epoch 199 | Loss = 3.3748923937479653\n",
      "Epoch 200 | Loss = 2.2591017617119684\n",
      "+-----------------------------------------------------------------------------+\n",
      "Training Time: 25681.47132086754 seconds.\n",
      "Get Embedding...\n",
      "(118, 512) (118,)\n",
      "SVC Accuracy: [0.7628787878787879, 0.787878787878788]\n",
      "+-----------------------------------------------------------------------------+\n",
      "Embedding Time: 13.725670099258423 seconds.\n",
      "Experient 7\n",
      "20211219-215521 :Log the record!\n",
      "+-----------------------------------------------------------------------------+\n",
      "Experient 8\n",
      "!!!\n",
      "89\n",
      "!!!\n",
      "[553, 217, 346, 850, 1139, 72, 280, 861, 916, 589, 50, 182, 307, 1067, 172, 583, 831, 791, 1014, 246, 18, 1013, 59, 1023, 984, 69, 755, 65, 71, 742, 1153, 256, 856, 266, 427, 956, 679, 826, 541, 147, 910, 0, 922, 683, 131, 103, 91, 1001, 166, 558, 796, 1078, 811, 852, 351, 1002, 383, 145, 197, 1072, 789, 17, 606, 9, 608, 1111, 664, 414, 154, 290, 954, 667, 959, 1097, 652, 339, 184, 368, 327, 506, 933, 370, 36, 768, 67, 1158, 93, 1123, 47, 624, 311, 883, 484, 702, 379, 316, 794, 822, 818, 55, 375, 1008, 605, 584, 920, 575, 986, 675, 82, 468, 1104, 202, 428, 566, 85, 550, 914, 1142, 545, 1145, 870, 1149, 332, 732, 185, 401, 359, 540, 714, 277, 785, 56, 45, 330, 1137, 564, 941, 703, 157, 1056, 1025, 325, 394, 360, 1077, 611, 1058, 1131, 830, 1081, 857, 1105, 364, 57, 233, 291, 1065, 926, 181, 231, 230, 808, 731, 678, 588, 929, 226, 537, 680, 38, 746, 595, 733, 244, 138, 1039, 441, 216, 762, 220, 284, 570, 1166, 1103, 117, 476, 261, 312, 232, 424, 149, 1110, 183, 651, 776, 522, 898, 381, 1100, 1088, 221, 1043, 596, 1084, 719, 669, 253, 715, 498, 35, 112, 495, 524, 548, 369, 735, 574, 580, 391, 167, 293, 942, 793, 273, 204, 1049, 474, 292, 973, 129, 1006, 402, 444, 975, 137, 1071, 647, 656, 1080, 1045, 333, 648, 555, 1127, 576, 486, 602, 989, 399, 3, 1030, 1016, 782, 153, 962, 421, 620, 1036, 264, 944, 848, 736, 717, 26, 1121, 978, 338, 398, 449, 46, 778, 877, 773, 633, 287, 465, 957, 828, 462, 800, 419, 902, 709, 953, 549, 997, 724, 78, 779, 1019, 2, 1050, 546, 1048, 195, 1061, 995, 1032, 10, 25, 450, 357, 1135, 254, 559, 612, 912, 965, 179, 94, 739, 470, 616, 298, 900, 224, 598, 301, 436, 1099, 234, 1060, 1053, 839, 815, 158, 804, 945, 89, 31, 210, 365, 993, 413, 225, 115, 618, 42, 567, 320, 1125, 644, 532, 337, 513, 168, 609, 243, 494, 186, 734, 1159, 193, 439, 305, 641, 628, 300, 521, 636, 948, 198, 994, 556, 96, 1130, 568, 22, 557, 1021, 132, 938, 707, 463, 723, 1152, 915, 711, 156, 631, 931, 457, 1066, 433, 666, 464, 459, 597, 816, 471, 725, 614, 1063, 397, 716, 684, 176, 227, 577, 1004, 801, 637, 34, 761, 302, 404, 1090, 980, 1028, 262, 161, 426, 751, 617, 1000, 39, 345, 504, 880, 907, 409, 173, 515, 249, 1069, 285, 1156, 730, 350, 260, 150, 281, 923, 361, 1062, 918, 1052, 904, 282, 1095, 219, 503, 738, 88, 668, 563, 710, 946, 207, 146, 60, 340, 760, 691, 106, 827, 265, 571, 15, 1132, 1015, 985, 825, 970, 758, 223, 406, 921, 1044, 772, 120, 247, 4, 542, 101, 859, 467, 1150, 12, 871, 268, 1012, 630, 1009, 336, 322, 621, 491, 1026, 187, 629, 432, 385, 992, 809, 1024, 756, 366, 62, 188, 53, 899, 950, 939, 1175, 988, 13, 105, 692, 659, 160, 960, 134, 875, 492, 107, 505, 757, 932, 653, 533, 175, 342, 314, 1165, 936, 1114, 252, 639, 8, 695, 838, 539, 824, 458, 408, 508, 248, 148, 352, 1162, 663, 911, 578, 318, 1116, 108, 1106, 235, 844, 54, 889, 645, 586, 1068, 239, 326, 763, 689, 104, 81, 323, 48, 1031, 908, 1041, 851, 1124, 452, 961, 122, 127, 1108, 388, 215, 155, 73, 887, 475, 77, 348, 485, 84, 1148, 650, 509, 75, 299, 331, 144, 241, 288, 727, 619, 775, 625, 206, 977, 455, 934, 697, 777, 865, 245, 835, 585, 133, 70, 655, 1161, 1092, 892, 190, 7, 32, 807, 110, 693, 1134, 1064, 295, 19, 872, 729, 1154, 289, 1176, 657, 142, 890, 445, 952, 275, 866, 587, 927, 603, 943, 211, 1118, 728, 593, 701, 477, 222, 196, 440, 205, 531, 981, 354, 478, 165, 242, 958, 1115, 1075, 1074, 1007, 24, 967, 913, 49, 744, 11, 554, 201, 321, 437, 517, 999, 814, 529, 681, 236, 180, 750, 780, 135, 868, 516, 879, 820, 315, 1128, 560, 218, 845, 297, 949, 841, 1059, 309, 1167, 1155, 665, 425, 1172, 461, 812, 79, 143, 622, 983, 937, 438, 353, 832, 6, 901, 795, 990, 759, 869, 1120, 276, 686, 810, 199, 371, 174, 677, 627, 116, 1133, 435, 519, 113, 496, 1029, 706, 255, 764, 130, 1005, 525, 500, 453, 367, 720, 171, 278, 483, 1122, 754, 698, 1051, 599, 1126, 74, 867, 726, 885, 250, 191, 1027, 229, 996, 310, 209, 884, 111, 456, 1082, 76, 358, 343, 66, 356, 213, 874, 1164, 362, 813, 1138, 5, 1055, 324, 888, 480, 849, 1033, 228, 267, 410, 786, 1035, 139, 873, 972, 740, 303, 951, 304, 1140, 543, 682, 382, 393, 896, 488, 718, 447, 1054, 417, 643, 43, 964, 14, 237, 189, 662, 29, 306, 1144, 119, 654, 86, 1107, 783, 511, 876, 1136, 696, 1146, 1086, 507, 341, 649, 771, 169, 274, 37, 376, 317, 279, 395, 840, 966, 251, 687, 878, 787, 159, 600, 1087, 905, 319, 1160, 378, 1094, 263, 52, 987, 344, 308, 363, 416, 396, 924, 769, 688, 33, 615, 460, 979, 906, 121, 895, 1171, 102, 579, 526, 882, 1168, 177, 523, 380, 1076, 864, 1085, 125, 674, 83, 781, 64, 124, 1047, 51, 20, 661, 767, 258, 151, 833, 638, 1109, 590, 635, 1169, 930, 372, 448, 982, 991, 846, 538, 374, 792, 446, 708, 528, 283, 819, 1089, 858, 1038, 806, 466, 493, 1174, 347, 114, 136, 128, 863, 935, 1093, 415, 269, 510, 1129, 834, 1, 389, 68, 749, 963, 552, 487, 430, 212, 971, 847, 745, 194, 162, 713, 296, 1011, 592, 582, 917, 594, 429, 853, 1073, 784, 520, 499, 469, 451, 1079, 572, 862, 753, 737, 27, 1117, 547, 947, 123, 894, 1112, 855, 384, 1034, 422, 490, 712, 843, 192, 1173, 203, 634, 514, 928, 694, 423, 141, 420, 329, 1017, 1119, 377, 1046, 527, 998, 349, 272, 454, 1057, 501, 178, 392, 405, 774, 472, 238, 1042, 534, 699, 482, 1018, 569, 842, 164, 660, 1113, 536, 817, 95, 240, 387, 854, 126, 561, 743, 431, 672, 802, 208, 766, 690, 390, 1101, 591, 604, 109, 909, 803, 28, 286, 765, 163, 601, 1163, 722, 90, 1177, 1037, 152, 1098, 805, 334, 294, 829, 700, 974, 21]\n",
      "Number of Graphs (dataset_train): 1060\n",
      "Number of Graphs (dataset_test): 118\n",
      "+-----------------------------------------------------------------------------+\n",
      "Learning Rate: 0.001\n",
      "Epochs: 200\n",
      "Batch Size: 128\n",
      "Hidden Dimension 64\n",
      "Number of Layer in Encoder: 2\n",
      "Opimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device: cuda\n",
      "Model:\n",
      "SimCLR(\n",
      "  (encoder): Encoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=89, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      ))\n",
      "      (1): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      ))\n",
      "    )\n",
      "    (bns): ModuleList(\n",
      "      (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Projection_MLP(\n",
      "    (layer1): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (unilayer): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Start Training...\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss = 212.57085984945297\n",
      "Epoch 2 | Loss = 173.2976393169827\n",
      "Epoch 3 | Loss = 153.70556485652924\n",
      "Epoch 4 | Loss = 142.0945829153061\n",
      "Epoch 5 | Loss = 131.6842930846744\n",
      "Epoch 6 | Loss = 124.68797002898322\n",
      "Epoch 7 | Loss = 121.23375516467624\n",
      "Epoch 8 | Loss = 115.31601977348328\n",
      "Epoch 9 | Loss = 115.0362450811598\n",
      "Epoch 10 | Loss = 109.83583826488919\n",
      "Epoch 11 | Loss = 105.87771140204535\n",
      "Epoch 12 | Loss = 105.05789444181654\n",
      "Epoch 13 | Loss = 100.99767213397556\n",
      "Epoch 14 | Loss = 102.08869512875874\n",
      "Epoch 15 | Loss = 99.86002821392483\n",
      "Epoch 16 | Loss = 97.88839128282335\n",
      "Epoch 17 | Loss = 95.80034716924031\n",
      "Epoch 18 | Loss = 92.24065823025174\n",
      "Epoch 19 | Loss = 91.60958822568257\n",
      "Epoch 20 | Loss = 90.89577537112766\n",
      "Epoch 21 | Loss = 89.73893223868475\n",
      "Epoch 22 | Loss = 87.92333806885614\n",
      "Epoch 23 | Loss = 87.46533279948764\n",
      "Epoch 24 | Loss = 84.16687652799818\n",
      "Epoch 25 | Loss = 84.02735874387953\n",
      "Epoch 26 | Loss = 84.65681584676106\n",
      "Epoch 27 | Loss = 82.05605565177069\n",
      "Epoch 28 | Loss = 80.51936859554715\n",
      "Epoch 29 | Loss = 81.80568938785129\n",
      "Epoch 30 | Loss = 80.97008185916476\n",
      "Epoch 31 | Loss = 78.66690635681152\n",
      "Epoch 32 | Loss = 78.58521525065105\n",
      "Epoch 33 | Loss = 77.5341952111986\n",
      "Epoch 34 | Loss = 76.08692895041571\n",
      "Epoch 35 | Loss = 78.72558005650838\n",
      "Epoch 36 | Loss = 75.85371488995023\n",
      "Epoch 37 | Loss = 74.82173776626587\n",
      "Epoch 38 | Loss = 73.65787649154663\n",
      "Epoch 39 | Loss = 74.72190872828166\n",
      "Epoch 40 | Loss = 73.55563020706177\n",
      "Epoch 41 | Loss = 71.85406393475003\n",
      "Epoch 42 | Loss = 72.86302010218303\n",
      "Epoch 43 | Loss = 73.58026096555922\n",
      "Epoch 44 | Loss = 72.12797207302518\n",
      "Epoch 45 | Loss = 70.48519494798448\n",
      "Epoch 46 | Loss = 70.80753077401056\n",
      "Epoch 47 | Loss = 69.32071198357477\n",
      "Epoch 48 | Loss = 68.67239904403687\n",
      "Epoch 49 | Loss = 69.00129625532362\n",
      "Epoch 50 | Loss = 68.18686697218153\n",
      "Epoch 51 | Loss = 68.56003332138062\n",
      "Epoch 52 | Loss = 67.56240431467693\n",
      "Epoch 53 | Loss = 68.65416479110718\n",
      "Epoch 54 | Loss = 67.94486125310262\n",
      "Epoch 55 | Loss = 67.33013232549031\n",
      "Epoch 56 | Loss = 65.96144967608981\n",
      "Epoch 57 | Loss = 66.89438962936401\n",
      "Epoch 58 | Loss = 65.89501682917277\n",
      "Epoch 59 | Loss = 66.4044508934021\n",
      "Epoch 60 | Loss = 62.868949731191\n",
      "Epoch 61 | Loss = 65.9235578642951\n",
      "Epoch 62 | Loss = 63.861719343397354\n",
      "Epoch 63 | Loss = 64.99500703811646\n",
      "Epoch 64 | Loss = 63.91901286443075\n",
      "Epoch 65 | Loss = 62.57920683754815\n",
      "Epoch 66 | Loss = 63.685081481933594\n",
      "Epoch 67 | Loss = 61.09548415078057\n",
      "Epoch 68 | Loss = 62.84599388970269\n",
      "Epoch 69 | Loss = 61.763815297020805\n",
      "Epoch 70 | Loss = 61.6627984046936\n",
      "Epoch 71 | Loss = 61.46755870183309\n",
      "Epoch 72 | Loss = 61.22235785590278\n",
      "Epoch 73 | Loss = 60.487785975138344\n",
      "Epoch 74 | Loss = 62.40689410103692\n",
      "Epoch 75 | Loss = 60.89200936423408\n",
      "Epoch 76 | Loss = 59.57811482747396\n",
      "Epoch 77 | Loss = 59.44655074013604\n",
      "Epoch 78 | Loss = 59.94450627432929\n",
      "Epoch 79 | Loss = 58.87769465976291\n",
      "Epoch 80 | Loss = 57.62751976648966\n",
      "Epoch 81 | Loss = 58.93976757261488\n",
      "Epoch 82 | Loss = 58.94369390275743\n",
      "Epoch 83 | Loss = 56.99318763944838\n",
      "Epoch 84 | Loss = 57.09483676486545\n",
      "Epoch 85 | Loss = 57.399975299835205\n",
      "Epoch 86 | Loss = 56.042691972520615\n",
      "Epoch 87 | Loss = 56.618635442521835\n",
      "Epoch 88 | Loss = 58.15092584821913\n",
      "Epoch 89 | Loss = 56.52837954627143\n",
      "Epoch 90 | Loss = 56.24114751815796\n",
      "Epoch 91 | Loss = 57.23275513119168\n",
      "Epoch 92 | Loss = 57.1420816315545\n",
      "Epoch 93 | Loss = 55.0384472211202\n",
      "Epoch 94 | Loss = 55.53067874908447\n",
      "Epoch 95 | Loss = 56.732087559170196\n",
      "Epoch 96 | Loss = 54.16954220665826\n",
      "Epoch 97 | Loss = 53.84054157469008\n",
      "Epoch 98 | Loss = 54.36329073376126\n",
      "Epoch 99 | Loss = 55.40514866511027\n",
      "Epoch 100 | Loss = 53.4771031803555\n",
      "Epoch 101 | Loss = 54.23608928256564\n",
      "Epoch 102 | Loss = 53.49840052922567\n",
      "Epoch 103 | Loss = 53.913197411431206\n",
      "Epoch 104 | Loss = 53.96410894393921\n",
      "Epoch 105 | Loss = 55.17485883500841\n",
      "Epoch 106 | Loss = 52.5941178533766\n",
      "Epoch 107 | Loss = 52.403135246700714\n",
      "Epoch 108 | Loss = 53.052122063106964\n",
      "Epoch 109 | Loss = 52.51811488469442\n",
      "Epoch 110 | Loss = 53.16940837436252\n",
      "Epoch 111 | Loss = 51.71045700709025\n",
      "Epoch 112 | Loss = 50.235855685340034\n",
      "Epoch 113 | Loss = 52.82279263602363\n",
      "Epoch 114 | Loss = 51.88049364089966\n",
      "Epoch 115 | Loss = 51.43244483735826\n",
      "Epoch 116 | Loss = 52.23737721972995\n",
      "Epoch 117 | Loss = 51.23730081982083\n",
      "Epoch 118 | Loss = 50.319837941063774\n",
      "Epoch 119 | Loss = 51.632428858015274\n",
      "Epoch 120 | Loss = 49.83729802237617\n",
      "Epoch 121 | Loss = 50.87102121777005\n",
      "Epoch 122 | Loss = 51.549486531151665\n",
      "Epoch 123 | Loss = 49.89674064848158\n",
      "Epoch 124 | Loss = 51.13942416508993\n",
      "Epoch 125 | Loss = 49.652989069620766\n",
      "Epoch 126 | Loss = 49.66562329398261\n",
      "Epoch 127 | Loss = 49.29356304804484\n",
      "Epoch 128 | Loss = 49.79097000757853\n",
      "Epoch 129 | Loss = 50.23235162099203\n",
      "Epoch 130 | Loss = 50.1908925904168\n",
      "Epoch 131 | Loss = 48.479956256018745\n",
      "Epoch 132 | Loss = 49.48227130042182\n",
      "Epoch 133 | Loss = 50.605404482947456\n",
      "Epoch 134 | Loss = 49.873805893792046\n",
      "Epoch 135 | Loss = 46.90723435084025\n",
      "Epoch 136 | Loss = 49.19181389278836\n",
      "Epoch 137 | Loss = 47.50670131047567\n",
      "Epoch 138 | Loss = 48.491413275400795\n",
      "Epoch 139 | Loss = 48.21999088923136\n",
      "Epoch 140 | Loss = 48.90526824527316\n",
      "Epoch 141 | Loss = 47.949576483832466\n",
      "Epoch 142 | Loss = 47.11541848712497\n",
      "Epoch 143 | Loss = 50.32023678885566\n",
      "Epoch 144 | Loss = 47.52491993374295\n",
      "Epoch 145 | Loss = 47.03511397043864\n",
      "Epoch 146 | Loss = 47.84518135918511\n",
      "Epoch 147 | Loss = 46.43334627151489\n",
      "Epoch 148 | Loss = 46.26822090148926\n",
      "Epoch 149 | Loss = 47.23359351687961\n",
      "Epoch 150 | Loss = 47.48044888178507\n",
      "Epoch 151 | Loss = 47.38688299391005\n",
      "Epoch 152 | Loss = 46.50436962975396\n",
      "Epoch 153 | Loss = 46.90781429078844\n",
      "Epoch 154 | Loss = 45.168407175276016\n",
      "Epoch 155 | Loss = 46.31514628728231\n",
      "Epoch 156 | Loss = 46.446351846059166\n",
      "Epoch 157 | Loss = 44.49525027804904\n",
      "Epoch 158 | Loss = 47.00257953008016\n",
      "Epoch 159 | Loss = 44.25919241375394\n",
      "Epoch 160 | Loss = 44.4460842344496\n",
      "Epoch 161 | Loss = 45.53510840733846\n",
      "Epoch 162 | Loss = 45.64717562993368\n",
      "Epoch 163 | Loss = 44.85912354787191\n",
      "Epoch 164 | Loss = 45.04112635718452\n",
      "Epoch 165 | Loss = 45.43568759494357\n",
      "Epoch 166 | Loss = 44.2647507985433\n",
      "Epoch 167 | Loss = 45.86893272399902\n",
      "Epoch 168 | Loss = 44.33350049124824\n",
      "Epoch 169 | Loss = 44.267761124504936\n",
      "Epoch 170 | Loss = 44.63543674680922\n",
      "Epoch 171 | Loss = 43.518543508317734\n",
      "Epoch 172 | Loss = 43.74152607387967\n",
      "Epoch 173 | Loss = 44.14835860994127\n",
      "Epoch 174 | Loss = 44.34210814370049\n",
      "Epoch 175 | Loss = 43.57876629299588\n",
      "Epoch 176 | Loss = 43.19113408194648\n",
      "Epoch 177 | Loss = 43.30918894873725\n",
      "Epoch 178 | Loss = 43.37709331512451\n",
      "Epoch 179 | Loss = 41.90259520212809\n",
      "Epoch 180 | Loss = 40.93858655293783\n",
      "Epoch 181 | Loss = 45.2809025976393\n",
      "Epoch 182 | Loss = 43.1841656366984\n",
      "Epoch 183 | Loss = 42.81517590416802\n",
      "Epoch 184 | Loss = 43.1762523121304\n",
      "Epoch 185 | Loss = 42.95162449942695\n",
      "Epoch 186 | Loss = 43.12925508287218\n",
      "Epoch 187 | Loss = 42.295434421963165\n",
      "Epoch 188 | Loss = 43.24657281239828\n",
      "Epoch 189 | Loss = 41.94072522057427\n",
      "Epoch 190 | Loss = 43.21599684821235\n",
      "Epoch 191 | Loss = 41.06384807162814\n",
      "Epoch 192 | Loss = 41.8249560991923\n",
      "Epoch 193 | Loss = 41.46981064478556\n",
      "Epoch 194 | Loss = 41.825565338134766\n",
      "Epoch 195 | Loss = 41.57617473602295\n",
      "Epoch 196 | Loss = 40.9813494152493\n",
      "Epoch 197 | Loss = 42.90897046195136\n",
      "Epoch 198 | Loss = 41.157614390055336\n",
      "Epoch 199 | Loss = 40.9786319732666\n",
      "Epoch 200 | Loss = 41.89902687072754\n",
      "+-----------------------------------------------------------------------------+\n",
      "Training Time: 25733.240497112274 seconds.\n",
      "Get Embedding...\n",
      "(118, 128) (118,)\n",
      "SVC Accuracy: [0.7712121212121212, 0.7704545454545455]\n",
      "+-----------------------------------------------------------------------------+\n",
      "Embedding Time: 13.181198596954346 seconds.\n",
      "Experient 8\n",
      "20211220-050428 :Log the record!\n",
      "+-----------------------------------------------------------------------------+\n",
      "Experient 9\n",
      "!!!\n",
      "89\n",
      "!!!\n",
      "[553, 217, 346, 850, 1139, 72, 280, 861, 916, 589, 50, 182, 307, 1067, 172, 583, 831, 791, 1014, 246, 18, 1013, 59, 1023, 984, 69, 755, 65, 71, 742, 1153, 256, 856, 266, 427, 956, 679, 826, 541, 147, 910, 0, 922, 683, 131, 103, 91, 1001, 166, 558, 796, 1078, 811, 852, 351, 1002, 383, 145, 197, 1072, 789, 17, 606, 9, 608, 1111, 664, 414, 154, 290, 954, 667, 959, 1097, 652, 339, 184, 368, 327, 506, 933, 370, 36, 768, 67, 1158, 93, 1123, 47, 624, 311, 883, 484, 702, 379, 316, 794, 822, 818, 55, 375, 1008, 605, 584, 920, 575, 986, 675, 82, 468, 1104, 202, 428, 566, 85, 550, 914, 1142, 545, 1145, 870, 1149, 332, 732, 185, 401, 359, 540, 714, 277, 785, 56, 45, 330, 1137, 564, 941, 703, 157, 1056, 1025, 325, 394, 360, 1077, 611, 1058, 1131, 830, 1081, 857, 1105, 364, 57, 233, 291, 1065, 926, 181, 231, 230, 808, 731, 678, 588, 929, 226, 537, 680, 38, 746, 595, 733, 244, 138, 1039, 441, 216, 762, 220, 284, 570, 1166, 1103, 117, 476, 261, 312, 232, 424, 149, 1110, 183, 651, 776, 522, 898, 381, 1100, 1088, 221, 1043, 596, 1084, 719, 669, 253, 715, 498, 35, 112, 495, 524, 548, 369, 735, 574, 580, 391, 167, 293, 942, 793, 273, 204, 1049, 474, 292, 973, 129, 1006, 402, 444, 975, 137, 1071, 647, 656, 1080, 1045, 333, 648, 555, 1127, 576, 486, 602, 989, 399, 3, 1030, 1016, 782, 153, 962, 421, 620, 1036, 264, 944, 848, 736, 717, 26, 1121, 978, 338, 398, 449, 46, 778, 877, 773, 633, 287, 465, 957, 828, 462, 800, 419, 902, 709, 953, 549, 997, 724, 78, 779, 1019, 2, 1050, 546, 1048, 195, 1061, 995, 1032, 10, 25, 450, 357, 1135, 254, 559, 612, 912, 965, 179, 94, 739, 470, 616, 298, 900, 224, 598, 301, 436, 1099, 234, 1060, 1053, 839, 815, 158, 804, 945, 89, 31, 210, 365, 993, 413, 225, 115, 618, 42, 567, 320, 1125, 644, 532, 337, 513, 168, 609, 243, 494, 186, 734, 1159, 193, 439, 305, 641, 628, 300, 521, 636, 948, 198, 994, 556, 96, 1130, 568, 22, 557, 1021, 132, 938, 707, 463, 723, 1152, 915, 711, 156, 631, 931, 457, 1066, 433, 666, 464, 459, 597, 816, 471, 725, 614, 1063, 397, 716, 684, 176, 227, 577, 1004, 801, 637, 34, 761, 302, 404, 1090, 980, 1028, 262, 161, 426, 751, 617, 1000, 39, 345, 504, 880, 907, 409, 173, 515, 249, 1069, 285, 1156, 730, 350, 260, 150, 281, 923, 361, 1062, 918, 1052, 904, 282, 1095, 219, 503, 738, 88, 668, 563, 710, 946, 207, 146, 60, 340, 760, 691, 106, 827, 265, 571, 15, 1132, 1015, 985, 825, 970, 758, 223, 406, 921, 1044, 772, 120, 247, 4, 542, 101, 859, 467, 1150, 12, 871, 268, 1012, 630, 1009, 336, 322, 621, 491, 1026, 187, 629, 432, 385, 992, 809, 1024, 756, 366, 62, 188, 53, 899, 950, 939, 1175, 988, 13, 105, 692, 659, 160, 960, 134, 875, 492, 107, 505, 757, 932, 653, 533, 175, 342, 314, 1165, 936, 1114, 252, 639, 8, 695, 838, 539, 824, 458, 408, 508, 248, 148, 352, 1162, 663, 911, 578, 318, 1116, 108, 1106, 235, 844, 54, 889, 645, 586, 1068, 239, 326, 763, 689, 104, 81, 323, 48, 1031, 908, 1041, 851, 1124, 452, 961, 122, 127, 1108, 388, 215, 155, 73, 887, 475, 77, 348, 485, 84, 1148, 650, 509, 75, 299, 331, 144, 241, 288, 727, 619, 775, 625, 206, 977, 455, 934, 697, 777, 865, 245, 835, 585, 133, 70, 655, 1161, 1092, 892, 190, 7, 32, 807, 110, 693, 1134, 1064, 295, 19, 872, 729, 1154, 289, 1176, 657, 142, 890, 445, 952, 275, 866, 587, 927, 603, 943, 211, 1118, 728, 593, 701, 477, 222, 196, 440, 205, 531, 981, 354, 478, 165, 242, 958, 1115, 1075, 1074, 1007, 24, 967, 913, 49, 744, 11, 554, 201, 321, 437, 517, 999, 814, 529, 681, 236, 180, 750, 780, 135, 868, 516, 879, 820, 315, 1128, 560, 218, 845, 297, 949, 841, 1059, 309, 1167, 1155, 665, 425, 1172, 461, 812, 79, 143, 622, 983, 937, 438, 353, 832, 6, 901, 795, 990, 759, 869, 1120, 276, 686, 810, 199, 371, 174, 677, 627, 116, 1133, 435, 519, 113, 496, 1029, 706, 255, 764, 130, 1005, 525, 500, 453, 367, 720, 171, 278, 483, 1122, 754, 698, 1051, 599, 1126, 74, 867, 726, 885, 250, 191, 1027, 229, 996, 310, 209, 884, 111, 456, 1082, 76, 358, 343, 66, 356, 213, 874, 1164, 362, 813, 1138, 5, 1055, 324, 888, 480, 849, 1033, 228, 267, 410, 786, 1035, 139, 873, 972, 740, 303, 951, 304, 1140, 543, 682, 382, 393, 896, 488, 718, 447, 1054, 417, 643, 43, 964, 14, 237, 189, 662, 29, 306, 1144, 119, 654, 86, 1107, 783, 511, 876, 1136, 696, 1146, 1086, 507, 341, 649, 771, 169, 274, 37, 376, 317, 279, 395, 840, 966, 251, 687, 878, 787, 159, 600, 1087, 905, 319, 1160, 378, 1094, 263, 52, 987, 344, 308, 363, 416, 396, 924, 769, 688, 33, 615, 460, 979, 906, 121, 895, 1171, 102, 579, 526, 882, 1168, 177, 523, 380, 1076, 864, 1085, 125, 674, 83, 781, 64, 124, 1047, 51, 20, 661, 767, 258, 151, 833, 638, 1109, 590, 635, 1169, 930, 372, 448, 982, 991, 846, 538, 374, 792, 446, 708, 528, 283, 819, 1089, 858, 1038, 806, 466, 493, 1174, 347, 114, 136, 128, 863, 935, 1093, 415, 269, 510, 1129, 834, 1, 389, 68, 749, 963, 552, 487, 430, 212, 971, 847, 745, 194, 162, 713, 296, 1011, 592, 582, 917, 594, 429, 853, 1073, 784, 520, 499, 469, 451, 1079, 572, 862, 753, 737, 27, 1117, 547, 947, 123, 894, 1112, 855, 384, 1034, 422, 490, 712, 843, 192, 1173, 203, 634, 514, 928, 694, 423, 141, 420, 329, 1017, 1119, 377, 1046, 527, 998, 349, 272, 454, 1057, 501, 178, 392, 405, 774, 472, 238, 1042, 534, 699, 482, 1018, 569, 842, 164, 660, 1113, 536, 817, 95, 240, 387, 854, 126, 561, 743, 431, 672, 802, 208, 766, 690, 390, 1101, 591, 604, 109, 909, 803, 28, 286, 765, 163, 601, 1163, 722, 90, 1177, 1037, 152, 1098, 805, 334, 294, 829, 700, 974, 21]\n",
      "Number of Graphs (dataset_train): 1060\n",
      "Number of Graphs (dataset_test): 118\n",
      "+-----------------------------------------------------------------------------+\n",
      "Learning Rate: 0.001\n",
      "Epochs: 200\n",
      "Batch Size: 128\n",
      "Hidden Dimension 512\n",
      "Number of Layer in Encoder: 2\n",
      "Opimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device: cuda\n",
      "Model:\n",
      "SimCLR(\n",
      "  (encoder): Encoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=89, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      ))\n",
      "      (1): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      ))\n",
      "    )\n",
      "    (bns): ModuleList(\n",
      "      (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Projection_MLP(\n",
      "    (layer1): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (unilayer): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Start Training...\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss = 144.6437849468655\n",
      "Epoch 2 | Loss = 111.51110757721796\n",
      "Epoch 3 | Loss = 99.56012111239963\n",
      "Epoch 4 | Loss = 91.05320959621005\n",
      "Epoch 5 | Loss = 84.44576109780206\n",
      "Epoch 6 | Loss = 79.8236924012502\n",
      "Epoch 7 | Loss = 76.43759330113728\n",
      "Epoch 8 | Loss = 74.68297412660387\n",
      "Epoch 9 | Loss = 71.15409416622586\n",
      "Epoch 10 | Loss = 68.51138051350911\n",
      "Epoch 11 | Loss = 65.69803116056654\n",
      "Epoch 12 | Loss = 64.84995487001207\n",
      "Epoch 13 | Loss = 61.57458914650811\n",
      "Epoch 14 | Loss = 58.85506947835287\n",
      "Epoch 15 | Loss = 59.04555140601264\n",
      "Epoch 16 | Loss = 59.80735031763712\n",
      "Epoch 17 | Loss = 56.214258935716416\n",
      "Epoch 18 | Loss = 54.65940607918633\n",
      "Epoch 19 | Loss = 52.35544957054986\n",
      "Epoch 20 | Loss = 52.24931022855971\n",
      "Epoch 21 | Loss = 51.841646141476105\n",
      "Epoch 22 | Loss = 50.618409368726944\n",
      "Epoch 23 | Loss = 50.03943310843574\n",
      "Epoch 24 | Loss = 47.42245599958632\n",
      "Epoch 25 | Loss = 46.38288222418891\n",
      "Epoch 26 | Loss = 45.21719053056505\n",
      "Epoch 27 | Loss = 45.0664291911655\n",
      "Epoch 28 | Loss = 44.93743366665311\n",
      "Epoch 29 | Loss = 42.86425601111518\n",
      "Epoch 30 | Loss = 41.925069650014244\n",
      "Epoch 31 | Loss = 41.91540866427951\n",
      "Epoch 32 | Loss = 40.45344506369697\n",
      "Epoch 33 | Loss = 40.52183225419786\n",
      "Epoch 34 | Loss = 38.91047557195028\n",
      "Epoch 35 | Loss = 39.6219097243415\n",
      "Epoch 36 | Loss = 37.82293075985379\n",
      "Epoch 37 | Loss = 37.77298980289035\n",
      "Epoch 38 | Loss = 36.4734779463874\n",
      "Epoch 39 | Loss = 34.887076483832466\n",
      "Epoch 40 | Loss = 34.393989933861626\n",
      "Epoch 41 | Loss = 32.82290389802721\n",
      "Epoch 42 | Loss = 32.50069448682997\n",
      "Epoch 43 | Loss = 31.192419317033554\n",
      "Epoch 44 | Loss = 33.34711302651299\n",
      "Epoch 45 | Loss = 30.325679885016548\n",
      "Epoch 46 | Loss = 29.359110567304825\n",
      "Epoch 47 | Loss = 29.633238527509903\n",
      "Epoch 48 | Loss = 28.067182699839275\n",
      "Epoch 49 | Loss = 28.404088815053303\n",
      "Epoch 50 | Loss = 27.793323781755234\n",
      "Epoch 51 | Loss = 26.348785400390625\n",
      "Epoch 52 | Loss = 26.96996651755439\n",
      "Epoch 53 | Loss = 27.30253505706787\n",
      "Epoch 54 | Loss = 25.071021397908527\n",
      "Epoch 55 | Loss = 25.60351774427626\n",
      "Epoch 56 | Loss = 25.017928388383652\n",
      "Epoch 57 | Loss = 23.623344050513374\n",
      "Epoch 58 | Loss = 22.722320238749187\n",
      "Epoch 59 | Loss = 23.279532061682808\n",
      "Epoch 60 | Loss = 22.56132173538208\n",
      "Epoch 61 | Loss = 21.794938776228165\n",
      "Epoch 62 | Loss = 21.402351750267876\n",
      "Epoch 63 | Loss = 21.305327892303467\n",
      "Epoch 64 | Loss = 19.82518418629964\n",
      "Epoch 65 | Loss = 20.184357537163628\n",
      "Epoch 66 | Loss = 18.250434080759685\n",
      "Epoch 67 | Loss = 18.121836768256294\n",
      "Epoch 68 | Loss = 18.57318311267429\n",
      "Epoch 69 | Loss = 16.678937488132053\n",
      "Epoch 70 | Loss = 18.37733581331041\n",
      "Epoch 71 | Loss = 17.1328387260437\n",
      "Epoch 72 | Loss = 17.199229611290825\n",
      "Epoch 73 | Loss = 16.561700503031414\n",
      "Epoch 74 | Loss = 16.61828221215142\n",
      "Epoch 75 | Loss = 15.981510215335422\n",
      "Epoch 76 | Loss = 16.021854718526203\n",
      "Epoch 77 | Loss = 15.13581821653578\n",
      "Epoch 78 | Loss = 14.92434941397773\n",
      "Epoch 79 | Loss = 13.993997573852539\n",
      "Epoch 80 | Loss = 13.17015012105306\n",
      "Epoch 81 | Loss = 13.409218258327908\n",
      "Epoch 82 | Loss = 12.479302088419596\n",
      "Epoch 83 | Loss = 12.85581964916653\n",
      "Epoch 84 | Loss = 11.141045570373535\n",
      "Epoch 85 | Loss = 13.209666834937202\n",
      "Epoch 86 | Loss = 11.244736936357286\n",
      "Epoch 87 | Loss = 10.658849345313179\n",
      "Epoch 88 | Loss = 11.175342930687798\n",
      "Epoch 89 | Loss = 10.338450060950386\n",
      "Epoch 90 | Loss = 9.184542708926731\n",
      "Epoch 91 | Loss = 9.500836902194553\n",
      "Epoch 92 | Loss = 9.481569766998291\n",
      "Epoch 93 | Loss = 9.00465186436971\n",
      "Epoch 94 | Loss = 8.828975412580702\n",
      "Epoch 95 | Loss = 8.103350957234701\n",
      "Epoch 96 | Loss = 8.785841782887777\n",
      "Epoch 97 | Loss = 8.389900419447157\n",
      "Epoch 98 | Loss = 8.114050706227621\n",
      "Epoch 99 | Loss = 7.380487177107069\n",
      "Epoch 100 | Loss = 6.6972826851738825\n",
      "Epoch 101 | Loss = 6.799939526451959\n",
      "Epoch 102 | Loss = 5.893911308712429\n",
      "Epoch 103 | Loss = 6.235756556193034\n",
      "Epoch 104 | Loss = 5.246589448716906\n",
      "Epoch 105 | Loss = 5.517431418100993\n",
      "Epoch 106 | Loss = 5.401012791527642\n",
      "Epoch 107 | Loss = 5.774249129825169\n",
      "Epoch 108 | Loss = 5.0234743224249945\n",
      "Epoch 109 | Loss = 4.6083981725904675\n",
      "Epoch 110 | Loss = 4.1245222091674805\n",
      "Epoch 111 | Loss = 3.5315237045288086\n",
      "Epoch 112 | Loss = 3.104327254825168\n",
      "Epoch 113 | Loss = 3.415779802534315\n",
      "Epoch 114 | Loss = 2.77568695280287\n",
      "Epoch 115 | Loss = 2.562503973642985\n",
      "Epoch 116 | Loss = 2.369277000427246\n",
      "Epoch 117 | Loss = 1.6158010164896648\n",
      "Epoch 118 | Loss = 1.856867578294542\n",
      "Epoch 119 | Loss = 1.3373173077901204\n",
      "Epoch 120 | Loss = 1.6001296043395996\n",
      "Epoch 121 | Loss = 1.5261352327134874\n",
      "Epoch 122 | Loss = 0.8601556354098849\n",
      "Epoch 123 | Loss = 0.7003439797295464\n",
      "Epoch 124 | Loss = 0.21453777949015299\n",
      "Epoch 125 | Loss = 0.2790210511949327\n",
      "Epoch 127 | Loss = 0.416872501373291\n",
      "Epoch 128 | Loss = 0.1883053249782986\n",
      "Epoch 129 | Loss = -0.1058383517795139\n",
      "Epoch 130 | Loss = -1.2764973640441895\n",
      "Epoch 131 | Loss = -0.39908954832288956\n",
      "Epoch 132 | Loss = -2.120550526512994\n",
      "Epoch 133 | Loss = -1.3951647016737196\n",
      "Epoch 134 | Loss = -1.6249722904629178\n",
      "Epoch 135 | Loss = -2.113595406214396\n",
      "Epoch 136 | Loss = -1.9209587309095595\n",
      "Epoch 137 | Loss = -2.0509181022644043\n",
      "Epoch 138 | Loss = -2.0422592163085938\n",
      "Epoch 139 | Loss = -2.229299677742852\n",
      "Epoch 140 | Loss = -2.890190429157681\n",
      "Epoch 141 | Loss = -2.34206043349372\n",
      "Epoch 142 | Loss = -3.434185200267368\n",
      "Epoch 143 | Loss = -2.765001270506117\n",
      "Epoch 144 | Loss = -3.7757476700676813\n",
      "Epoch 145 | Loss = -3.8858269850413003\n",
      "Epoch 146 | Loss = -3.923426866531372\n",
      "Epoch 147 | Loss = -4.1889136168691845\n",
      "Epoch 148 | Loss = -3.908343083328671\n",
      "Epoch 149 | Loss = -4.525783091783524\n",
      "Epoch 150 | Loss = -3.920705156193839\n",
      "Epoch 151 | Loss = -4.23388585779402\n",
      "Epoch 152 | Loss = -5.000888142320845\n",
      "Epoch 153 | Loss = -4.744434164630042\n",
      "Epoch 154 | Loss = -4.983478642172283\n",
      "Epoch 155 | Loss = -5.409994687471125\n",
      "Epoch 156 | Loss = -5.455897668997447\n",
      "Epoch 157 | Loss = -5.038704567485386\n",
      "Epoch 158 | Loss = -5.6129066877894935\n",
      "Epoch 159 | Loss = -5.689337187343174\n",
      "Epoch 160 | Loss = -6.113295164373186\n",
      "Epoch 161 | Loss = -5.623285998900731\n",
      "Epoch 162 | Loss = -5.950670348273383\n",
      "Epoch 163 | Loss = -6.3688000407483845\n",
      "Epoch 164 | Loss = -6.476078285111321\n",
      "Epoch 165 | Loss = -6.33033185866144\n",
      "Epoch 166 | Loss = -6.616581588983536\n",
      "Epoch 167 | Loss = -6.651283303896586\n",
      "Epoch 168 | Loss = -6.408435602982839\n",
      "Epoch 169 | Loss = -7.461847239070469\n",
      "Epoch 170 | Loss = -7.238277746571435\n",
      "Epoch 171 | Loss = -8.10650003949801\n",
      "Epoch 172 | Loss = -7.664958281649484\n",
      "Epoch 173 | Loss = -8.28601356347402\n",
      "Epoch 174 | Loss = -8.030314074622261\n",
      "Epoch 175 | Loss = -8.469645619392395\n",
      "Epoch 176 | Loss = -7.828957776228587\n",
      "Epoch 177 | Loss = -8.205465419424904\n",
      "Epoch 178 | Loss = -8.669231202867296\n",
      "Epoch 179 | Loss = -8.781721962822807\n",
      "Epoch 180 | Loss = -8.548154903782738\n",
      "Epoch 181 | Loss = -8.513707704014248\n",
      "Epoch 182 | Loss = -8.973965538872612\n",
      "Epoch 183 | Loss = -9.264676716592577\n",
      "Epoch 184 | Loss = -9.297129472096762\n",
      "Epoch 185 | Loss = -9.115281919638315\n",
      "Epoch 186 | Loss = -9.686119000116983\n",
      "Epoch 187 | Loss = -10.04882001876831\n",
      "Epoch 188 | Loss = -9.711269431644016\n",
      "Epoch 189 | Loss = -9.69776843653785\n",
      "Epoch 190 | Loss = -10.534011284510294\n",
      "Epoch 191 | Loss = -9.79374188847012\n",
      "Epoch 192 | Loss = -10.217623896068996\n",
      "Epoch 193 | Loss = -9.962955156962076\n",
      "Epoch 194 | Loss = -10.724360386530558\n",
      "Epoch 195 | Loss = -10.731197754542032\n",
      "Epoch 196 | Loss = -10.991012361314562\n",
      "Epoch 197 | Loss = -11.058056990305582\n",
      "Epoch 198 | Loss = -11.133472654554579\n",
      "Epoch 199 | Loss = -11.38032595316569\n",
      "Epoch 200 | Loss = -11.180870956844753\n",
      "+-----------------------------------------------------------------------------+\n",
      "Training Time: 25727.760266065598 seconds.\n",
      "Get Embedding...\n",
      "(118, 1024) (118,)\n",
      "SVC Accuracy: [0.7295454545454546, 0.7113636363636364]\n",
      "+-----------------------------------------------------------------------------+\n",
      "Embedding Time: 14.438558101654053 seconds.\n",
      "Experient 9\n",
      "20211220-121331 :Log the record!\n",
      "+-----------------------------------------------------------------------------+\n",
      "Experient 10\n",
      "!!!\n",
      "89\n",
      "!!!\n",
      "[553, 217, 346, 850, 1139, 72, 280, 861, 916, 589, 50, 182, 307, 1067, 172, 583, 831, 791, 1014, 246, 18, 1013, 59, 1023, 984, 69, 755, 65, 71, 742, 1153, 256, 856, 266, 427, 956, 679, 826, 541, 147, 910, 0, 922, 683, 131, 103, 91, 1001, 166, 558, 796, 1078, 811, 852, 351, 1002, 383, 145, 197, 1072, 789, 17, 606, 9, 608, 1111, 664, 414, 154, 290, 954, 667, 959, 1097, 652, 339, 184, 368, 327, 506, 933, 370, 36, 768, 67, 1158, 93, 1123, 47, 624, 311, 883, 484, 702, 379, 316, 794, 822, 818, 55, 375, 1008, 605, 584, 920, 575, 986, 675, 82, 468, 1104, 202, 428, 566, 85, 550, 914, 1142, 545, 1145, 870, 1149, 332, 732, 185, 401, 359, 540, 714, 277, 785, 56, 45, 330, 1137, 564, 941, 703, 157, 1056, 1025, 325, 394, 360, 1077, 611, 1058, 1131, 830, 1081, 857, 1105, 364, 57, 233, 291, 1065, 926, 181, 231, 230, 808, 731, 678, 588, 929, 226, 537, 680, 38, 746, 595, 733, 244, 138, 1039, 441, 216, 762, 220, 284, 570, 1166, 1103, 117, 476, 261, 312, 232, 424, 149, 1110, 183, 651, 776, 522, 898, 381, 1100, 1088, 221, 1043, 596, 1084, 719, 669, 253, 715, 498, 35, 112, 495, 524, 548, 369, 735, 574, 580, 391, 167, 293, 942, 793, 273, 204, 1049, 474, 292, 973, 129, 1006, 402, 444, 975, 137, 1071, 647, 656, 1080, 1045, 333, 648, 555, 1127, 576, 486, 602, 989, 399, 3, 1030, 1016, 782, 153, 962, 421, 620, 1036, 264, 944, 848, 736, 717, 26, 1121, 978, 338, 398, 449, 46, 778, 877, 773, 633, 287, 465, 957, 828, 462, 800, 419, 902, 709, 953, 549, 997, 724, 78, 779, 1019, 2, 1050, 546, 1048, 195, 1061, 995, 1032, 10, 25, 450, 357, 1135, 254, 559, 612, 912, 965, 179, 94, 739, 470, 616, 298, 900, 224, 598, 301, 436, 1099, 234, 1060, 1053, 839, 815, 158, 804, 945, 89, 31, 210, 365, 993, 413, 225, 115, 618, 42, 567, 320, 1125, 644, 532, 337, 513, 168, 609, 243, 494, 186, 734, 1159, 193, 439, 305, 641, 628, 300, 521, 636, 948, 198, 994, 556, 96, 1130, 568, 22, 557, 1021, 132, 938, 707, 463, 723, 1152, 915, 711, 156, 631, 931, 457, 1066, 433, 666, 464, 459, 597, 816, 471, 725, 614, 1063, 397, 716, 684, 176, 227, 577, 1004, 801, 637, 34, 761, 302, 404, 1090, 980, 1028, 262, 161, 426, 751, 617, 1000, 39, 345, 504, 880, 907, 409, 173, 515, 249, 1069, 285, 1156, 730, 350, 260, 150, 281, 923, 361, 1062, 918, 1052, 904, 282, 1095, 219, 503, 738, 88, 668, 563, 710, 946, 207, 146, 60, 340, 760, 691, 106, 827, 265, 571, 15, 1132, 1015, 985, 825, 970, 758, 223, 406, 921, 1044, 772, 120, 247, 4, 542, 101, 859, 467, 1150, 12, 871, 268, 1012, 630, 1009, 336, 322, 621, 491, 1026, 187, 629, 432, 385, 992, 809, 1024, 756, 366, 62, 188, 53, 899, 950, 939, 1175, 988, 13, 105, 692, 659, 160, 960, 134, 875, 492, 107, 505, 757, 932, 653, 533, 175, 342, 314, 1165, 936, 1114, 252, 639, 8, 695, 838, 539, 824, 458, 408, 508, 248, 148, 352, 1162, 663, 911, 578, 318, 1116, 108, 1106, 235, 844, 54, 889, 645, 586, 1068, 239, 326, 763, 689, 104, 81, 323, 48, 1031, 908, 1041, 851, 1124, 452, 961, 122, 127, 1108, 388, 215, 155, 73, 887, 475, 77, 348, 485, 84, 1148, 650, 509, 75, 299, 331, 144, 241, 288, 727, 619, 775, 625, 206, 977, 455, 934, 697, 777, 865, 245, 835, 585, 133, 70, 655, 1161, 1092, 892, 190, 7, 32, 807, 110, 693, 1134, 1064, 295, 19, 872, 729, 1154, 289, 1176, 657, 142, 890, 445, 952, 275, 866, 587, 927, 603, 943, 211, 1118, 728, 593, 701, 477, 222, 196, 440, 205, 531, 981, 354, 478, 165, 242, 958, 1115, 1075, 1074, 1007, 24, 967, 913, 49, 744, 11, 554, 201, 321, 437, 517, 999, 814, 529, 681, 236, 180, 750, 780, 135, 868, 516, 879, 820, 315, 1128, 560, 218, 845, 297, 949, 841, 1059, 309, 1167, 1155, 665, 425, 1172, 461, 812, 79, 143, 622, 983, 937, 438, 353, 832, 6, 901, 795, 990, 759, 869, 1120, 276, 686, 810, 199, 371, 174, 677, 627, 116, 1133, 435, 519, 113, 496, 1029, 706, 255, 764, 130, 1005, 525, 500, 453, 367, 720, 171, 278, 483, 1122, 754, 698, 1051, 599, 1126, 74, 867, 726, 885, 250, 191, 1027, 229, 996, 310, 209, 884, 111, 456, 1082, 76, 358, 343, 66, 356, 213, 874, 1164, 362, 813, 1138, 5, 1055, 324, 888, 480, 849, 1033, 228, 267, 410, 786, 1035, 139, 873, 972, 740, 303, 951, 304, 1140, 543, 682, 382, 393, 896, 488, 718, 447, 1054, 417, 643, 43, 964, 14, 237, 189, 662, 29, 306, 1144, 119, 654, 86, 1107, 783, 511, 876, 1136, 696, 1146, 1086, 507, 341, 649, 771, 169, 274, 37, 376, 317, 279, 395, 840, 966, 251, 687, 878, 787, 159, 600, 1087, 905, 319, 1160, 378, 1094, 263, 52, 987, 344, 308, 363, 416, 396, 924, 769, 688, 33, 615, 460, 979, 906, 121, 895, 1171, 102, 579, 526, 882, 1168, 177, 523, 380, 1076, 864, 1085, 125, 674, 83, 781, 64, 124, 1047, 51, 20, 661, 767, 258, 151, 833, 638, 1109, 590, 635, 1169, 930, 372, 448, 982, 991, 846, 538, 374, 792, 446, 708, 528, 283, 819, 1089, 858, 1038, 806, 466, 493, 1174, 347, 114, 136, 128, 863, 935, 1093, 415, 269, 510, 1129, 834, 1, 389, 68, 749, 963, 552, 487, 430, 212, 971, 847, 745, 194, 162, 713, 296, 1011, 592, 582, 917, 594, 429, 853, 1073, 784, 520, 499, 469, 451, 1079, 572, 862, 753, 737, 27, 1117, 547, 947, 123, 894, 1112, 855, 384, 1034, 422, 490, 712, 843, 192, 1173, 203, 634, 514, 928, 694, 423, 141, 420, 329, 1017, 1119, 377, 1046, 527, 998, 349, 272, 454, 1057, 501, 178, 392, 405, 774, 472, 238, 1042, 534, 699, 482, 1018, 569, 842, 164, 660, 1113, 536, 817, 95, 240, 387, 854, 126, 561, 743, 431, 672, 802, 208, 766, 690, 390, 1101, 591, 604, 109, 909, 803, 28, 286, 765, 163, 601, 1163, 722, 90, 1177, 1037, 152, 1098, 805, 334, 294, 829, 700, 974, 21]\n",
      "Number of Graphs (dataset_train): 1060\n",
      "Number of Graphs (dataset_test): 118\n",
      "+-----------------------------------------------------------------------------+\n",
      "Learning Rate: 0.001\n",
      "Epochs: 200\n",
      "Batch Size: 128\n",
      "Hidden Dimension 64\n",
      "Number of Layer in Encoder: 3\n",
      "Opimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device: cuda\n",
      "Model:\n",
      "SimCLR(\n",
      "  (encoder): Encoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=89, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      ))\n",
      "      (1): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      ))\n",
      "      (2): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      ))\n",
      "    )\n",
      "    (bns): ModuleList(\n",
      "      (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Projection_MLP(\n",
      "    (layer1): Sequential(\n",
      "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (1): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (1): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (1): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (unilayer): Sequential(\n",
      "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=192, out_features=192, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Start Training...\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss = 222.86898224221335\n",
      "Epoch 2 | Loss = 172.4579948650466\n",
      "Epoch 4 | Loss = 139.2869324286779\n",
      "Epoch 5 | Loss = 132.38014647695752\n",
      "Epoch 6 | Loss = 126.26611579789056\n",
      "Epoch 7 | Loss = 120.70578291681078\n",
      "Epoch 8 | Loss = 112.71009998851352\n",
      "Epoch 9 | Loss = 112.18806338310242\n",
      "Epoch 10 | Loss = 106.93236570888095\n",
      "Epoch 11 | Loss = 104.62266683578491\n",
      "Epoch 12 | Loss = 99.88727280828688\n",
      "Epoch 13 | Loss = 98.39389782481723\n",
      "Epoch 14 | Loss = 98.0605120923784\n",
      "Epoch 15 | Loss = 95.60476679272122\n",
      "Epoch 16 | Loss = 91.04338481691148\n",
      "Epoch 17 | Loss = 90.49721784061856\n",
      "Epoch 18 | Loss = 91.5751384364234\n",
      "Epoch 19 | Loss = 87.3337123129103\n",
      "Epoch 20 | Loss = 86.76188906033833\n",
      "Epoch 21 | Loss = 84.98335268762376\n",
      "Epoch 22 | Loss = 83.78261902597215\n",
      "Epoch 23 | Loss = 82.05200868182712\n",
      "Epoch 24 | Loss = 80.7977172003852\n",
      "Epoch 25 | Loss = 79.75608756807115\n",
      "Epoch 26 | Loss = 80.04535833994548\n",
      "Epoch 27 | Loss = 77.82412783304851\n",
      "Epoch 28 | Loss = 77.66990910636054\n",
      "Epoch 29 | Loss = 76.35392310884264\n",
      "Epoch 30 | Loss = 75.89971828460693\n",
      "Epoch 31 | Loss = 74.54960171381633\n",
      "Epoch 32 | Loss = 73.61910072962444\n",
      "Epoch 33 | Loss = 74.29524400499132\n",
      "Epoch 34 | Loss = 73.40700303183661\n",
      "Epoch 35 | Loss = 71.91208569208781\n",
      "Epoch 36 | Loss = 70.53635274039374\n",
      "Epoch 37 | Loss = 72.31517781151666\n",
      "Epoch 38 | Loss = 69.50411568747626\n",
      "Epoch 39 | Loss = 71.123625225491\n",
      "Epoch 40 | Loss = 70.06265385945638\n",
      "Epoch 41 | Loss = 68.91885397169325\n",
      "Epoch 42 | Loss = 67.85475842158\n",
      "Epoch 43 | Loss = 66.44259532292683\n",
      "Epoch 44 | Loss = 66.14997747209337\n",
      "Epoch 45 | Loss = 64.4253511428833\n",
      "Epoch 46 | Loss = 65.36772298812866\n",
      "Epoch 47 | Loss = 63.85747713512845\n",
      "Epoch 48 | Loss = 63.97905423906114\n",
      "Epoch 49 | Loss = 62.78173647986518\n",
      "Epoch 50 | Loss = 62.565010971493194\n",
      "Epoch 51 | Loss = 60.88724788029989\n",
      "Epoch 52 | Loss = 62.143178410000274\n",
      "Epoch 53 | Loss = 63.538737032148575\n",
      "Epoch 54 | Loss = 59.81108914481269\n",
      "Epoch 55 | Loss = 60.08467329872979\n",
      "Epoch 56 | Loss = 60.040422704484726\n",
      "Epoch 57 | Loss = 60.55945380528768\n",
      "Epoch 58 | Loss = 59.86177629894681\n",
      "Epoch 59 | Loss = 58.608299573262535\n",
      "Epoch 60 | Loss = 61.71790107091268\n",
      "Epoch 61 | Loss = 59.459677643246124\n",
      "Epoch 62 | Loss = 58.69063197241889\n",
      "Epoch 63 | Loss = 58.13137287563748\n",
      "Epoch 64 | Loss = 56.734071572621666\n",
      "Epoch 65 | Loss = 57.3497584660848\n",
      "Epoch 66 | Loss = 56.205219427744545\n",
      "Epoch 67 | Loss = 56.79716073142158\n",
      "Epoch 68 | Loss = 55.60992352167765\n",
      "Epoch 69 | Loss = 55.81282292471992\n",
      "Epoch 70 | Loss = 56.041225592295326\n",
      "Epoch 71 | Loss = 55.02752632564969\n",
      "Epoch 72 | Loss = 56.349120722876656\n",
      "Epoch 73 | Loss = 54.46982796986898\n",
      "Epoch 74 | Loss = 53.462718062930634\n",
      "Epoch 75 | Loss = 52.9627210299174\n",
      "Epoch 76 | Loss = 54.193980640835235\n",
      "Epoch 77 | Loss = 52.98864947424995\n",
      "Epoch 78 | Loss = 53.50699419445462\n",
      "Epoch 79 | Loss = 51.92957978778415\n",
      "Epoch 80 | Loss = 52.20847871568468\n",
      "Epoch 81 | Loss = 52.299002700381806\n",
      "Epoch 82 | Loss = 52.29297950532701\n",
      "Epoch 83 | Loss = 51.98797708087497\n",
      "Epoch 84 | Loss = 51.19071006774902\n",
      "Epoch 85 | Loss = 48.9538164668613\n",
      "Epoch 86 | Loss = 50.2851955625746\n",
      "Epoch 87 | Loss = 50.80625216166178\n",
      "Epoch 88 | Loss = 50.775810930464004\n",
      "Epoch 89 | Loss = 50.43229479259915\n",
      "Epoch 90 | Loss = 49.23251353369819\n",
      "Epoch 91 | Loss = 49.37166240480211\n",
      "Epoch 92 | Loss = 48.560003916422524\n",
      "Epoch 93 | Loss = 48.60977919896444\n",
      "Epoch 94 | Loss = 48.9378129641215\n",
      "Epoch 95 | Loss = 48.844632890489365\n",
      "Epoch 96 | Loss = 49.39166106118096\n",
      "Epoch 97 | Loss = 47.13185331556532\n",
      "Epoch 98 | Loss = 48.16087214152018\n",
      "Epoch 99 | Loss = 47.29163572523329\n",
      "Epoch 100 | Loss = 47.680524031321205\n",
      "Epoch 101 | Loss = 47.82268057929145\n",
      "Epoch 102 | Loss = 45.886407905154755\n",
      "Epoch 103 | Loss = 47.916795783572724\n",
      "Epoch 104 | Loss = 44.89792908562554\n",
      "Epoch 105 | Loss = 46.44828097025553\n",
      "Epoch 106 | Loss = 47.40373229980469\n",
      "Epoch 107 | Loss = 47.69960477617052\n",
      "Epoch 108 | Loss = 45.907165156470406\n",
      "Epoch 109 | Loss = 46.02485651440091\n",
      "Epoch 110 | Loss = 45.73008807500204\n",
      "Epoch 111 | Loss = 43.6594967312283\n",
      "Epoch 112 | Loss = 42.67460690604316\n",
      "Epoch 113 | Loss = 46.2760525809394\n",
      "Epoch 114 | Loss = 45.59220674302843\n",
      "Epoch 115 | Loss = 46.01567676332262\n",
      "Epoch 116 | Loss = 45.54337188932631\n",
      "Epoch 117 | Loss = 45.177449756198456\n",
      "Epoch 118 | Loss = 45.297201792399086\n",
      "Epoch 119 | Loss = 44.02581951353285\n",
      "Epoch 120 | Loss = 42.94638045628866\n",
      "Epoch 121 | Loss = 43.60304101308187\n",
      "Epoch 122 | Loss = 43.05704175101386\n",
      "Epoch 123 | Loss = 43.88102865219116\n",
      "Epoch 124 | Loss = 43.201807498931885\n",
      "Epoch 125 | Loss = 43.58179293738471\n",
      "Epoch 126 | Loss = 42.62297587924533\n",
      "Epoch 127 | Loss = 42.451284090677895\n",
      "Epoch 128 | Loss = 42.30647118886312\n",
      "Epoch 129 | Loss = 40.87344657050239\n",
      "Epoch 130 | Loss = 40.851825608147514\n",
      "Epoch 131 | Loss = 42.03353028827243\n",
      "Epoch 132 | Loss = 42.162384774949814\n",
      "Epoch 133 | Loss = 42.28507428699069\n",
      "Epoch 134 | Loss = 40.7071205774943\n",
      "Epoch 135 | Loss = 40.86706468794081\n",
      "Epoch 136 | Loss = 41.35933489269681\n",
      "Epoch 137 | Loss = 41.02207321590848\n",
      "Epoch 138 | Loss = 40.75144396887885\n",
      "Epoch 139 | Loss = 41.00100792778863\n",
      "Epoch 140 | Loss = 41.1368817753262\n",
      "Epoch 141 | Loss = 40.3264225323995\n",
      "Epoch 142 | Loss = 40.33739614486694\n",
      "Epoch 143 | Loss = 40.022618452707924\n",
      "Epoch 144 | Loss = 40.18440871768527\n",
      "Epoch 145 | Loss = 40.987644036610924\n",
      "Epoch 146 | Loss = 40.895202742682564\n",
      "Epoch 147 | Loss = 40.403533299764\n",
      "Epoch 148 | Loss = 39.651126066843666\n",
      "Epoch 149 | Loss = 39.4404550658332\n",
      "Epoch 150 | Loss = 39.286708778805206\n",
      "Epoch 151 | Loss = 39.80950265460544\n",
      "Epoch 152 | Loss = 39.24604664908515\n",
      "Epoch 153 | Loss = 39.16208865907457\n",
      "Epoch 154 | Loss = 38.857687102423775\n",
      "Epoch 155 | Loss = 38.52173884709676\n",
      "Epoch 156 | Loss = 37.99447515275743\n",
      "Epoch 157 | Loss = 38.493126498328316\n",
      "Epoch 158 | Loss = 38.008418030209015\n",
      "Epoch 159 | Loss = 37.499607139163544\n",
      "Epoch 160 | Loss = 38.70367511113485\n",
      "Epoch 161 | Loss = 37.761905405256485\n",
      "Epoch 162 | Loss = 37.67430363761054\n",
      "Epoch 163 | Loss = 37.14693392647637\n",
      "Epoch 164 | Loss = 38.143583350711395\n",
      "Epoch 165 | Loss = 38.434799353281655\n",
      "Epoch 166 | Loss = 36.717818313174774\n",
      "Epoch 167 | Loss = 38.1847513516744\n",
      "Epoch 168 | Loss = 36.51418786578708\n",
      "Epoch 169 | Loss = 37.065600554148354\n",
      "Epoch 170 | Loss = 35.972018559773765\n",
      "Epoch 171 | Loss = 36.56204875310262\n",
      "Epoch 172 | Loss = 35.40458181169298\n",
      "Epoch 173 | Loss = 35.229176097446015\n",
      "Epoch 174 | Loss = 35.2353785832723\n",
      "Epoch 175 | Loss = 35.3277898894416\n",
      "Epoch 176 | Loss = 36.25706768035889\n",
      "Epoch 177 | Loss = 36.91629457473755\n",
      "Epoch 178 | Loss = 35.34612454308404\n",
      "Epoch 179 | Loss = 36.01689916186862\n",
      "Epoch 180 | Loss = 35.19958098729452\n",
      "Epoch 181 | Loss = 37.31639575958252\n",
      "Epoch 182 | Loss = 35.15360551410251\n",
      "Epoch 183 | Loss = 34.932163927290176\n",
      "Epoch 184 | Loss = 34.55631923675537\n",
      "Epoch 185 | Loss = 35.488455719418\n",
      "Epoch 186 | Loss = 34.06899404525757\n",
      "Epoch 187 | Loss = 34.63860543568929\n",
      "Epoch 188 | Loss = 34.55857155058119\n",
      "Epoch 189 | Loss = 34.052220291561554\n",
      "Epoch 190 | Loss = 35.28817733128866\n",
      "Epoch 191 | Loss = 35.1318146387736\n",
      "Epoch 192 | Loss = 34.38587702645196\n",
      "Epoch 193 | Loss = 33.469067838456894\n",
      "Epoch 194 | Loss = 34.8177318043179\n",
      "Epoch 195 | Loss = 34.67931742138333\n",
      "Epoch 196 | Loss = 33.83880721198188\n",
      "Epoch 197 | Loss = 33.224007076687286\n",
      "Epoch 198 | Loss = 32.54870663748847\n",
      "Epoch 199 | Loss = 32.109353118472626\n",
      "Epoch 200 | Loss = 34.51774422327677\n",
      "+-----------------------------------------------------------------------------+\n",
      "Training Time: 25723.94376015663 seconds.\n",
      "Get Embedding...\n",
      "(118, 192) (118,)\n",
      "SVC Accuracy: [0.7530303030303029, 0.7022727272727274]\n",
      "+-----------------------------------------------------------------------------+\n",
      "Embedding Time: 13.463693380355835 seconds.\n",
      "Experient 10\n",
      "20211220-192234 :Log the record!\n",
      "+-----------------------------------------------------------------------------+\n",
      "Experient 11\n",
      "!!!\n",
      "89\n",
      "!!!\n",
      "[553, 217, 346, 850, 1139, 72, 280, 861, 916, 589, 50, 182, 307, 1067, 172, 583, 831, 791, 1014, 246, 18, 1013, 59, 1023, 984, 69, 755, 65, 71, 742, 1153, 256, 856, 266, 427, 956, 679, 826, 541, 147, 910, 0, 922, 683, 131, 103, 91, 1001, 166, 558, 796, 1078, 811, 852, 351, 1002, 383, 145, 197, 1072, 789, 17, 606, 9, 608, 1111, 664, 414, 154, 290, 954, 667, 959, 1097, 652, 339, 184, 368, 327, 506, 933, 370, 36, 768, 67, 1158, 93, 1123, 47, 624, 311, 883, 484, 702, 379, 316, 794, 822, 818, 55, 375, 1008, 605, 584, 920, 575, 986, 675, 82, 468, 1104, 202, 428, 566, 85, 550, 914, 1142, 545, 1145, 870, 1149, 332, 732, 185, 401, 359, 540, 714, 277, 785, 56, 45, 330, 1137, 564, 941, 703, 157, 1056, 1025, 325, 394, 360, 1077, 611, 1058, 1131, 830, 1081, 857, 1105, 364, 57, 233, 291, 1065, 926, 181, 231, 230, 808, 731, 678, 588, 929, 226, 537, 680, 38, 746, 595, 733, 244, 138, 1039, 441, 216, 762, 220, 284, 570, 1166, 1103, 117, 476, 261, 312, 232, 424, 149, 1110, 183, 651, 776, 522, 898, 381, 1100, 1088, 221, 1043, 596, 1084, 719, 669, 253, 715, 498, 35, 112, 495, 524, 548, 369, 735, 574, 580, 391, 167, 293, 942, 793, 273, 204, 1049, 474, 292, 973, 129, 1006, 402, 444, 975, 137, 1071, 647, 656, 1080, 1045, 333, 648, 555, 1127, 576, 486, 602, 989, 399, 3, 1030, 1016, 782, 153, 962, 421, 620, 1036, 264, 944, 848, 736, 717, 26, 1121, 978, 338, 398, 449, 46, 778, 877, 773, 633, 287, 465, 957, 828, 462, 800, 419, 902, 709, 953, 549, 997, 724, 78, 779, 1019, 2, 1050, 546, 1048, 195, 1061, 995, 1032, 10, 25, 450, 357, 1135, 254, 559, 612, 912, 965, 179, 94, 739, 470, 616, 298, 900, 224, 598, 301, 436, 1099, 234, 1060, 1053, 839, 815, 158, 804, 945, 89, 31, 210, 365, 993, 413, 225, 115, 618, 42, 567, 320, 1125, 644, 532, 337, 513, 168, 609, 243, 494, 186, 734, 1159, 193, 439, 305, 641, 628, 300, 521, 636, 948, 198, 994, 556, 96, 1130, 568, 22, 557, 1021, 132, 938, 707, 463, 723, 1152, 915, 711, 156, 631, 931, 457, 1066, 433, 666, 464, 459, 597, 816, 471, 725, 614, 1063, 397, 716, 684, 176, 227, 577, 1004, 801, 637, 34, 761, 302, 404, 1090, 980, 1028, 262, 161, 426, 751, 617, 1000, 39, 345, 504, 880, 907, 409, 173, 515, 249, 1069, 285, 1156, 730, 350, 260, 150, 281, 923, 361, 1062, 918, 1052, 904, 282, 1095, 219, 503, 738, 88, 668, 563, 710, 946, 207, 146, 60, 340, 760, 691, 106, 827, 265, 571, 15, 1132, 1015, 985, 825, 970, 758, 223, 406, 921, 1044, 772, 120, 247, 4, 542, 101, 859, 467, 1150, 12, 871, 268, 1012, 630, 1009, 336, 322, 621, 491, 1026, 187, 629, 432, 385, 992, 809, 1024, 756, 366, 62, 188, 53, 899, 950, 939, 1175, 988, 13, 105, 692, 659, 160, 960, 134, 875, 492, 107, 505, 757, 932, 653, 533, 175, 342, 314, 1165, 936, 1114, 252, 639, 8, 695, 838, 539, 824, 458, 408, 508, 248, 148, 352, 1162, 663, 911, 578, 318, 1116, 108, 1106, 235, 844, 54, 889, 645, 586, 1068, 239, 326, 763, 689, 104, 81, 323, 48, 1031, 908, 1041, 851, 1124, 452, 961, 122, 127, 1108, 388, 215, 155, 73, 887, 475, 77, 348, 485, 84, 1148, 650, 509, 75, 299, 331, 144, 241, 288, 727, 619, 775, 625, 206, 977, 455, 934, 697, 777, 865, 245, 835, 585, 133, 70, 655, 1161, 1092, 892, 190, 7, 32, 807, 110, 693, 1134, 1064, 295, 19, 872, 729, 1154, 289, 1176, 657, 142, 890, 445, 952, 275, 866, 587, 927, 603, 943, 211, 1118, 728, 593, 701, 477, 222, 196, 440, 205, 531, 981, 354, 478, 165, 242, 958, 1115, 1075, 1074, 1007, 24, 967, 913, 49, 744, 11, 554, 201, 321, 437, 517, 999, 814, 529, 681, 236, 180, 750, 780, 135, 868, 516, 879, 820, 315, 1128, 560, 218, 845, 297, 949, 841, 1059, 309, 1167, 1155, 665, 425, 1172, 461, 812, 79, 143, 622, 983, 937, 438, 353, 832, 6, 901, 795, 990, 759, 869, 1120, 276, 686, 810, 199, 371, 174, 677, 627, 116, 1133, 435, 519, 113, 496, 1029, 706, 255, 764, 130, 1005, 525, 500, 453, 367, 720, 171, 278, 483, 1122, 754, 698, 1051, 599, 1126, 74, 867, 726, 885, 250, 191, 1027, 229, 996, 310, 209, 884, 111, 456, 1082, 76, 358, 343, 66, 356, 213, 874, 1164, 362, 813, 1138, 5, 1055, 324, 888, 480, 849, 1033, 228, 267, 410, 786, 1035, 139, 873, 972, 740, 303, 951, 304, 1140, 543, 682, 382, 393, 896, 488, 718, 447, 1054, 417, 643, 43, 964, 14, 237, 189, 662, 29, 306, 1144, 119, 654, 86, 1107, 783, 511, 876, 1136, 696, 1146, 1086, 507, 341, 649, 771, 169, 274, 37, 376, 317, 279, 395, 840, 966, 251, 687, 878, 787, 159, 600, 1087, 905, 319, 1160, 378, 1094, 263, 52, 987, 344, 308, 363, 416, 396, 924, 769, 688, 33, 615, 460, 979, 906, 121, 895, 1171, 102, 579, 526, 882, 1168, 177, 523, 380, 1076, 864, 1085, 125, 674, 83, 781, 64, 124, 1047, 51, 20, 661, 767, 258, 151, 833, 638, 1109, 590, 635, 1169, 930, 372, 448, 982, 991, 846, 538, 374, 792, 446, 708, 528, 283, 819, 1089, 858, 1038, 806, 466, 493, 1174, 347, 114, 136, 128, 863, 935, 1093, 415, 269, 510, 1129, 834, 1, 389, 68, 749, 963, 552, 487, 430, 212, 971, 847, 745, 194, 162, 713, 296, 1011, 592, 582, 917, 594, 429, 853, 1073, 784, 520, 499, 469, 451, 1079, 572, 862, 753, 737, 27, 1117, 547, 947, 123, 894, 1112, 855, 384, 1034, 422, 490, 712, 843, 192, 1173, 203, 634, 514, 928, 694, 423, 141, 420, 329, 1017, 1119, 377, 1046, 527, 998, 349, 272, 454, 1057, 501, 178, 392, 405, 774, 472, 238, 1042, 534, 699, 482, 1018, 569, 842, 164, 660, 1113, 536, 817, 95, 240, 387, 854, 126, 561, 743, 431, 672, 802, 208, 766, 690, 390, 1101, 591, 604, 109, 909, 803, 28, 286, 765, 163, 601, 1163, 722, 90, 1177, 1037, 152, 1098, 805, 334, 294, 829, 700, 974, 21]\n",
      "Number of Graphs (dataset_train): 1060\n",
      "Number of Graphs (dataset_test): 118\n",
      "+-----------------------------------------------------------------------------+\n",
      "Learning Rate: 0.001\n",
      "Epochs: 200\n",
      "Batch Size: 128\n",
      "Hidden Dimension 512\n",
      "Number of Layer in Encoder: 3\n",
      "Opimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device: cuda\n",
      "Model:\n",
      "SimCLR(\n",
      "  (encoder): Encoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=89, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      ))\n",
      "      (1): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      ))\n",
      "      (2): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      ))\n",
      "    )\n",
      "    (bns): ModuleList(\n",
      "      (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Projection_MLP(\n",
      "    (layer1): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (unilayer): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Start Training...\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss = 153.50536453723907\n",
      "Epoch 2 | Loss = 119.72477054595947\n",
      "Epoch 3 | Loss = 104.20704529020522\n",
      "Epoch 4 | Loss = 95.05629263983832\n",
      "Epoch 5 | Loss = 89.28777607282002\n",
      "Epoch 6 | Loss = 83.5460015932719\n",
      "Epoch 7 | Loss = 79.10873741573758\n",
      "Epoch 8 | Loss = 76.81479533513387\n",
      "Epoch 9 | Loss = 72.55967039532132\n",
      "Epoch 10 | Loss = 71.11398235956828\n",
      "Epoch 11 | Loss = 66.39604791005452\n",
      "Epoch 12 | Loss = 66.46015691757202\n",
      "Epoch 13 | Loss = 62.30034515592787\n",
      "Epoch 14 | Loss = 62.75441047880385\n",
      "Epoch 15 | Loss = 61.018961376614044\n",
      "Epoch 16 | Loss = 60.31897905137804\n",
      "Epoch 17 | Loss = 58.8733532693651\n",
      "Epoch 18 | Loss = 56.795694986979164\n",
      "Epoch 19 | Loss = 54.33088148964776\n",
      "Epoch 20 | Loss = 53.71722544564141\n",
      "Epoch 21 | Loss = 52.39259354273478\n",
      "Epoch 22 | Loss = 51.81988260481093\n",
      "Epoch 23 | Loss = 50.45734855863783\n",
      "Epoch 24 | Loss = 48.514009634653725\n",
      "Epoch 25 | Loss = 47.962126678890655\n",
      "Epoch 26 | Loss = 45.631098906199135\n",
      "Epoch 27 | Loss = 45.120403660668266\n",
      "Epoch 28 | Loss = 43.80963606304593\n",
      "Epoch 29 | Loss = 43.713318824768066\n",
      "Epoch 30 | Loss = 41.93020476235284\n",
      "Epoch 31 | Loss = 41.35163587994046\n",
      "Epoch 32 | Loss = 40.451197677188446\n",
      "Epoch 33 | Loss = 38.568736447228325\n",
      "Epoch 34 | Loss = 38.78747293684218\n",
      "Epoch 35 | Loss = 38.46928596496582\n",
      "Epoch 36 | Loss = 35.67820114559598\n",
      "Epoch 37 | Loss = 35.35835027694702\n",
      "Epoch 38 | Loss = 35.76944292916192\n",
      "Epoch 39 | Loss = 34.003274016910126\n",
      "Epoch 40 | Loss = 35.05592277314928\n",
      "Epoch 41 | Loss = 32.676203886667885\n",
      "Epoch 42 | Loss = 31.861159589555527\n",
      "Epoch 43 | Loss = 31.85041030248006\n",
      "Epoch 44 | Loss = 30.590618822309708\n",
      "Epoch 45 | Loss = 29.6757370101081\n",
      "Epoch 46 | Loss = 28.605174594455296\n",
      "Epoch 47 | Loss = 28.6501178211636\n",
      "Epoch 48 | Loss = 28.192233350541855\n",
      "Epoch 49 | Loss = 26.68682638804118\n",
      "Epoch 50 | Loss = 26.516155666775173\n",
      "Epoch 51 | Loss = 25.74005142847697\n",
      "Epoch 52 | Loss = 24.21142339706421\n",
      "Epoch 53 | Loss = 24.503318521711563\n",
      "Epoch 54 | Loss = 23.31177012125651\n",
      "Epoch 55 | Loss = 23.623408317565918\n",
      "Epoch 56 | Loss = 23.146282090081108\n",
      "Epoch 57 | Loss = 23.140793800354004\n",
      "Epoch 58 | Loss = 20.87735234366523\n",
      "Epoch 59 | Loss = 21.32500966389974\n",
      "Epoch 60 | Loss = 20.28771135542128\n",
      "Epoch 61 | Loss = 19.941230720943874\n",
      "Epoch 62 | Loss = 18.670550240410698\n",
      "Epoch 63 | Loss = 18.697048981984455\n",
      "Epoch 64 | Loss = 17.764284663730198\n",
      "Epoch 65 | Loss = 18.01723708046807\n",
      "Epoch 66 | Loss = 17.828784942626953\n",
      "Epoch 67 | Loss = 16.503434816996258\n",
      "Epoch 68 | Loss = 15.77413182788425\n",
      "Epoch 69 | Loss = 15.067269431220161\n",
      "Epoch 70 | Loss = 15.086734506818983\n",
      "Epoch 71 | Loss = 14.046203825208876\n",
      "Epoch 72 | Loss = 13.915950934092203\n",
      "Epoch 73 | Loss = 14.006116973029243\n",
      "Epoch 74 | Loss = 13.11680179172092\n",
      "Epoch 75 | Loss = 13.30136908425225\n",
      "Epoch 76 | Loss = 11.894562085469564\n",
      "Epoch 77 | Loss = 12.051474412282309\n",
      "Epoch 78 | Loss = 11.268330256144205\n",
      "Epoch 79 | Loss = 11.26540862189399\n",
      "Epoch 80 | Loss = 10.699949317508274\n",
      "Epoch 81 | Loss = 9.720721085866293\n",
      "Epoch 82 | Loss = 10.117706934611002\n",
      "Epoch 83 | Loss = 8.630809254116482\n",
      "Epoch 84 | Loss = 9.033683935801188\n",
      "Epoch 85 | Loss = 8.804121123419868\n",
      "Epoch 86 | Loss = 8.847994168599447\n",
      "Epoch 87 | Loss = 7.991783354017469\n",
      "Epoch 88 | Loss = 7.3378185166252985\n",
      "Epoch 89 | Loss = 7.007995711432563\n",
      "Epoch 90 | Loss = 6.814637396070692\n",
      "Epoch 91 | Loss = 6.417553371853298\n",
      "Epoch 92 | Loss = 5.934750503963894\n",
      "Epoch 93 | Loss = 6.2194533877902565\n",
      "Epoch 94 | Loss = 4.311605771382649\n",
      "Epoch 95 | Loss = 4.845540735456678\n",
      "Epoch 96 | Loss = 4.496990309821235\n",
      "Epoch 97 | Loss = 4.9540013737148705\n",
      "Epoch 98 | Loss = 4.412414444817437\n",
      "Epoch 99 | Loss = 3.3809107144673667\n",
      "Epoch 100 | Loss = 2.825368775261773\n",
      "Epoch 101 | Loss = 2.2169407738579645\n",
      "Epoch 102 | Loss = 2.27321375740899\n",
      "Epoch 103 | Loss = 2.1686628659566245\n",
      "Epoch 104 | Loss = 1.589088545905219\n",
      "Epoch 105 | Loss = 1.4748433430989583\n",
      "Epoch 106 | Loss = 1.030521657731798\n",
      "Epoch 107 | Loss = 0.5580395062764486\n",
      "Epoch 108 | Loss = 0.40343644883897567\n",
      "Epoch 109 | Loss = 0.03306616677178277\n",
      "Epoch 110 | Loss = -0.08968671162923177\n",
      "Epoch 111 | Loss = -0.23724312252468532\n",
      "Epoch 112 | Loss = -0.37256881925794816\n",
      "Epoch 113 | Loss = -0.7622523307800293\n",
      "Epoch 114 | Loss = -0.4948544502258301\n",
      "Epoch 115 | Loss = -1.3854290379418268\n",
      "Epoch 116 | Loss = -0.46323355038960773\n",
      "Epoch 117 | Loss = -1.825415505303277\n",
      "Epoch 118 | Loss = -1.6362697283426921\n",
      "Epoch 119 | Loss = -1.968511290020413\n",
      "Epoch 120 | Loss = -1.9112085766262479\n",
      "Epoch 121 | Loss = -2.218424532148573\n",
      "Epoch 122 | Loss = -2.7096830474005804\n",
      "Epoch 123 | Loss = -3.2709916962517633\n",
      "Epoch 124 | Loss = -3.301179673936632\n",
      "Epoch 125 | Loss = -2.9601935545603433\n",
      "Epoch 126 | Loss = -3.744695676697625\n",
      "Epoch 127 | Loss = -4.086829503377278\n",
      "Epoch 128 | Loss = -4.033716380596161\n",
      "Epoch 129 | Loss = -4.770115481482612\n",
      "Epoch 130 | Loss = -4.727628487679693\n",
      "Epoch 131 | Loss = -5.286230742931366\n",
      "Epoch 132 | Loss = -5.258340805768967\n",
      "Epoch 133 | Loss = -5.3172724942366285\n",
      "Epoch 134 | Loss = -5.478327840566635\n",
      "Epoch 135 | Loss = -5.796679668956333\n",
      "Epoch 136 | Loss = -5.513307705521584\n",
      "Epoch 137 | Loss = -6.289801918798023\n",
      "Epoch 138 | Loss = -6.161337721678946\n",
      "Epoch 139 | Loss = -5.8102363182438745\n",
      "Epoch 140 | Loss = -6.4881883131133185\n",
      "Epoch 141 | Loss = -6.860723654429118\n",
      "Epoch 142 | Loss = -7.243308656745487\n",
      "Epoch 143 | Loss = -7.562567657894558\n",
      "Epoch 144 | Loss = -7.705818434556325\n",
      "Epoch 145 | Loss = -7.188293198744456\n",
      "Epoch 146 | Loss = -7.535150302780999\n",
      "Epoch 147 | Loss = -7.077517218059963\n",
      "Epoch 148 | Loss = -7.873298234409756\n",
      "Epoch 149 | Loss = -7.68520620961984\n",
      "Epoch 150 | Loss = -8.295059495502048\n",
      "Epoch 151 | Loss = -8.540230260954964\n",
      "Epoch 152 | Loss = -9.157088253233168\n",
      "Epoch 153 | Loss = -8.995926247702705\n",
      "Epoch 154 | Loss = -9.023081541061401\n",
      "Epoch 155 | Loss = -9.27548493279351\n",
      "Epoch 156 | Loss = -9.533968077765572\n",
      "Epoch 157 | Loss = -9.371426317426893\n",
      "Epoch 158 | Loss = -10.064972612592909\n",
      "Epoch 159 | Loss = -9.92503481441074\n",
      "Epoch 160 | Loss = -9.98312685224745\n",
      "Epoch 161 | Loss = -10.05812735027737\n",
      "Epoch 162 | Loss = -10.852053112453884\n",
      "Epoch 163 | Loss = -10.84349963400099\n",
      "Epoch 164 | Loss = -10.656076219346788\n",
      "Epoch 165 | Loss = -11.285771210988363\n",
      "Epoch 166 | Loss = -11.022572676340738\n",
      "Epoch 167 | Loss = -11.135067303975424\n",
      "Epoch 168 | Loss = -11.345097250408596\n",
      "Epoch 169 | Loss = -11.243482828140259\n",
      "Epoch 170 | Loss = -11.885461966196695\n",
      "Epoch 171 | Loss = -11.900059911939833\n",
      "Epoch 172 | Loss = -11.732908407847086\n",
      "Epoch 173 | Loss = -11.336964792675442\n",
      "Epoch 174 | Loss = -11.817502657572428\n",
      "Epoch 175 | Loss = -12.286664644877115\n",
      "Epoch 176 | Loss = -12.208060953352186\n",
      "Epoch 177 | Loss = -11.79704236984253\n",
      "Epoch 178 | Loss = -12.177253670162624\n",
      "Epoch 179 | Loss = -12.53800598780314\n",
      "Epoch 180 | Loss = -12.309043248494467\n",
      "Epoch 181 | Loss = -12.536143991682264\n",
      "Epoch 182 | Loss = -12.839582920074463\n",
      "Epoch 183 | Loss = -12.652878231472439\n",
      "Epoch 184 | Loss = -12.841872427198622\n",
      "Epoch 185 | Loss = -13.300324492984348\n",
      "Epoch 186 | Loss = -13.01022587882148\n",
      "Epoch 187 | Loss = -13.417478667365181\n",
      "Epoch 188 | Loss = -13.608132627275255\n",
      "Epoch 189 | Loss = -13.977896266513401\n",
      "Epoch 190 | Loss = -13.67650153901842\n",
      "Epoch 191 | Loss = -13.535448922051323\n",
      "Epoch 192 | Loss = -13.900913662380642\n",
      "Epoch 193 | Loss = -13.754010730319553\n",
      "Epoch 194 | Loss = -14.062516583336723\n",
      "Epoch 195 | Loss = -14.364797433217367\n",
      "Epoch 196 | Loss = -14.611272229088677\n",
      "Epoch 197 | Loss = -14.665783511267769\n",
      "Epoch 198 | Loss = -14.557766278584799\n",
      "Epoch 199 | Loss = -14.74685329861111\n",
      "Epoch 200 | Loss = -15.0913298924764\n",
      "+-----------------------------------------------------------------------------+\n",
      "Training Time: 25684.378502845764 seconds.\n",
      "Get Embedding...\n",
      "(118, 1536) (118,)\n",
      "SVC Accuracy: [0.7537878787878789, 0.693939393939394]\n",
      "+-----------------------------------------------------------------------------+\n",
      "Embedding Time: 15.262861251831055 seconds.\n",
      "Experient 11\n",
      "20211221-023059 :Log the record!\n",
      "+-----------------------------------------------------------------------------+\n",
      "Experient 12\n",
      "!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89\n",
      "!!!\n",
      "[553, 217, 346, 850, 1139, 72, 280, 861, 916, 589, 50, 182, 307, 1067, 172, 583, 831, 791, 1014, 246, 18, 1013, 59, 1023, 984, 69, 755, 65, 71, 742, 1153, 256, 856, 266, 427, 956, 679, 826, 541, 147, 910, 0, 922, 683, 131, 103, 91, 1001, 166, 558, 796, 1078, 811, 852, 351, 1002, 383, 145, 197, 1072, 789, 17, 606, 9, 608, 1111, 664, 414, 154, 290, 954, 667, 959, 1097, 652, 339, 184, 368, 327, 506, 933, 370, 36, 768, 67, 1158, 93, 1123, 47, 624, 311, 883, 484, 702, 379, 316, 794, 822, 818, 55, 375, 1008, 605, 584, 920, 575, 986, 675, 82, 468, 1104, 202, 428, 566, 85, 550, 914, 1142, 545, 1145, 870, 1149, 332, 732, 185, 401, 359, 540, 714, 277, 785, 56, 45, 330, 1137, 564, 941, 703, 157, 1056, 1025, 325, 394, 360, 1077, 611, 1058, 1131, 830, 1081, 857, 1105, 364, 57, 233, 291, 1065, 926, 181, 231, 230, 808, 731, 678, 588, 929, 226, 537, 680, 38, 746, 595, 733, 244, 138, 1039, 441, 216, 762, 220, 284, 570, 1166, 1103, 117, 476, 261, 312, 232, 424, 149, 1110, 183, 651, 776, 522, 898, 381, 1100, 1088, 221, 1043, 596, 1084, 719, 669, 253, 715, 498, 35, 112, 495, 524, 548, 369, 735, 574, 580, 391, 167, 293, 942, 793, 273, 204, 1049, 474, 292, 973, 129, 1006, 402, 444, 975, 137, 1071, 647, 656, 1080, 1045, 333, 648, 555, 1127, 576, 486, 602, 989, 399, 3, 1030, 1016, 782, 153, 962, 421, 620, 1036, 264, 944, 848, 736, 717, 26, 1121, 978, 338, 398, 449, 46, 778, 877, 773, 633, 287, 465, 957, 828, 462, 800, 419, 902, 709, 953, 549, 997, 724, 78, 779, 1019, 2, 1050, 546, 1048, 195, 1061, 995, 1032, 10, 25, 450, 357, 1135, 254, 559, 612, 912, 965, 179, 94, 739, 470, 616, 298, 900, 224, 598, 301, 436, 1099, 234, 1060, 1053, 839, 815, 158, 804, 945, 89, 31, 210, 365, 993, 413, 225, 115, 618, 42, 567, 320, 1125, 644, 532, 337, 513, 168, 609, 243, 494, 186, 734, 1159, 193, 439, 305, 641, 628, 300, 521, 636, 948, 198, 994, 556, 96, 1130, 568, 22, 557, 1021, 132, 938, 707, 463, 723, 1152, 915, 711, 156, 631, 931, 457, 1066, 433, 666, 464, 459, 597, 816, 471, 725, 614, 1063, 397, 716, 684, 176, 227, 577, 1004, 801, 637, 34, 761, 302, 404, 1090, 980, 1028, 262, 161, 426, 751, 617, 1000, 39, 345, 504, 880, 907, 409, 173, 515, 249, 1069, 285, 1156, 730, 350, 260, 150, 281, 923, 361, 1062, 918, 1052, 904, 282, 1095, 219, 503, 738, 88, 668, 563, 710, 946, 207, 146, 60, 340, 760, 691, 106, 827, 265, 571, 15, 1132, 1015, 985, 825, 970, 758, 223, 406, 921, 1044, 772, 120, 247, 4, 542, 101, 859, 467, 1150, 12, 871, 268, 1012, 630, 1009, 336, 322, 621, 491, 1026, 187, 629, 432, 385, 992, 809, 1024, 756, 366, 62, 188, 53, 899, 950, 939, 1175, 988, 13, 105, 692, 659, 160, 960, 134, 875, 492, 107, 505, 757, 932, 653, 533, 175, 342, 314, 1165, 936, 1114, 252, 639, 8, 695, 838, 539, 824, 458, 408, 508, 248, 148, 352, 1162, 663, 911, 578, 318, 1116, 108, 1106, 235, 844, 54, 889, 645, 586, 1068, 239, 326, 763, 689, 104, 81, 323, 48, 1031, 908, 1041, 851, 1124, 452, 961, 122, 127, 1108, 388, 215, 155, 73, 887, 475, 77, 348, 485, 84, 1148, 650, 509, 75, 299, 331, 144, 241, 288, 727, 619, 775, 625, 206, 977, 455, 934, 697, 777, 865, 245, 835, 585, 133, 70, 655, 1161, 1092, 892, 190, 7, 32, 807, 110, 693, 1134, 1064, 295, 19, 872, 729, 1154, 289, 1176, 657, 142, 890, 445, 952, 275, 866, 587, 927, 603, 943, 211, 1118, 728, 593, 701, 477, 222, 196, 440, 205, 531, 981, 354, 478, 165, 242, 958, 1115, 1075, 1074, 1007, 24, 967, 913, 49, 744, 11, 554, 201, 321, 437, 517, 999, 814, 529, 681, 236, 180, 750, 780, 135, 868, 516, 879, 820, 315, 1128, 560, 218, 845, 297, 949, 841, 1059, 309, 1167, 1155, 665, 425, 1172, 461, 812, 79, 143, 622, 983, 937, 438, 353, 832, 6, 901, 795, 990, 759, 869, 1120, 276, 686, 810, 199, 371, 174, 677, 627, 116, 1133, 435, 519, 113, 496, 1029, 706, 255, 764, 130, 1005, 525, 500, 453, 367, 720, 171, 278, 483, 1122, 754, 698, 1051, 599, 1126, 74, 867, 726, 885, 250, 191, 1027, 229, 996, 310, 209, 884, 111, 456, 1082, 76, 358, 343, 66, 356, 213, 874, 1164, 362, 813, 1138, 5, 1055, 324, 888, 480, 849, 1033, 228, 267, 410, 786, 1035, 139, 873, 972, 740, 303, 951, 304, 1140, 543, 682, 382, 393, 896, 488, 718, 447, 1054, 417, 643, 43, 964, 14, 237, 189, 662, 29, 306, 1144, 119, 654, 86, 1107, 783, 511, 876, 1136, 696, 1146, 1086, 507, 341, 649, 771, 169, 274, 37, 376, 317, 279, 395, 840, 966, 251, 687, 878, 787, 159, 600, 1087, 905, 319, 1160, 378, 1094, 263, 52, 987, 344, 308, 363, 416, 396, 924, 769, 688, 33, 615, 460, 979, 906, 121, 895, 1171, 102, 579, 526, 882, 1168, 177, 523, 380, 1076, 864, 1085, 125, 674, 83, 781, 64, 124, 1047, 51, 20, 661, 767, 258, 151, 833, 638, 1109, 590, 635, 1169, 930, 372, 448, 982, 991, 846, 538, 374, 792, 446, 708, 528, 283, 819, 1089, 858, 1038, 806, 466, 493, 1174, 347, 114, 136, 128, 863, 935, 1093, 415, 269, 510, 1129, 834, 1, 389, 68, 749, 963, 552, 487, 430, 212, 971, 847, 745, 194, 162, 713, 296, 1011, 592, 582, 917, 594, 429, 853, 1073, 784, 520, 499, 469, 451, 1079, 572, 862, 753, 737, 27, 1117, 547, 947, 123, 894, 1112, 855, 384, 1034, 422, 490, 712, 843, 192, 1173, 203, 634, 514, 928, 694, 423, 141, 420, 329, 1017, 1119, 377, 1046, 527, 998, 349, 272, 454, 1057, 501, 178, 392, 405, 774, 472, 238, 1042, 534, 699, 482, 1018, 569, 842, 164, 660, 1113, 536, 817, 95, 240, 387, 854, 126, 561, 743, 431, 672, 802, 208, 766, 690, 390, 1101, 591, 604, 109, 909, 803, 28, 286, 765, 163, 601, 1163, 722, 90, 1177, 1037, 152, 1098, 805, 334, 294, 829, 700, 974, 21]\n",
      "Number of Graphs (dataset_train): 1060\n",
      "Number of Graphs (dataset_test): 118\n",
      "+-----------------------------------------------------------------------------+\n",
      "Learning Rate: 0.001\n",
      "Epochs: 200\n",
      "Batch Size: 64\n",
      "Hidden Dimension 64\n",
      "Number of Layer in Encoder: 1\n",
      "Opimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device: cuda\n",
      "Model:\n",
      "SimCLR(\n",
      "  (encoder): Encoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=89, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      ))\n",
      "    )\n",
      "    (bns): ModuleList(\n",
      "      (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Projection_MLP(\n",
      "    (layer1): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (unilayer): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Start Training...\n",
      "+-----------------------------------------------------------------------------+\n",
      "Epoch 1 | Loss = 4.494336938156801\n",
      "Epoch 2 | Loss = -12.852183636497049\n",
      "Epoch 3 | Loss = -19.53172285416547\n",
      "Epoch 4 | Loss = -23.730299641104306\n",
      "Epoch 5 | Loss = -26.70219836515539\n",
      "Epoch 6 | Loss = -28.88053860383875\n",
      "Epoch 7 | Loss = -30.21109193914077\n",
      "Epoch 8 | Loss = -31.378295309403363\n",
      "Epoch 9 | Loss = -32.567884164697986\n",
      "Epoch 10 | Loss = -33.11909361446605\n",
      "Epoch 11 | Loss = -34.078494156108185\n",
      "Epoch 12 | Loss = -34.62542444116929\n",
      "Epoch 13 | Loss = -35.54238807453829\n",
      "Epoch 14 | Loss = -35.98964887506821\n",
      "Epoch 15 | Loss = -36.57273500105914\n",
      "Epoch 16 | Loss = -37.07623728583841\n",
      "Epoch 17 | Loss = -37.04687993666705\n",
      "Epoch 18 | Loss = -37.48984390146592\n",
      "Epoch 19 | Loss = -37.97548972859102\n",
      "Epoch 20 | Loss = -38.17268781101002\n",
      "Epoch 22 | Loss = -38.87614522260778\n",
      "Epoch 23 | Loss = -38.742030816919666\n",
      "Epoch 24 | Loss = -38.98576817793005\n",
      "Epoch 25 | Loss = -39.36356679131003\n",
      "Epoch 26 | Loss = -39.59823782303754\n",
      "Epoch 27 | Loss = -39.813670214484716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 | Loss = -39.778816896326404\n",
      "Epoch 29 | Loss = -39.94965309255264\n",
      "Epoch 30 | Loss = -39.97494837817024\n",
      "Epoch 31 | Loss = -40.10900634877822\n",
      "Epoch 32 | Loss = -40.633232341093176\n",
      "Epoch 33 | Loss = -40.51611600202673\n",
      "Epoch 34 | Loss = -40.88418623980354\n",
      "Epoch 35 | Loss = -40.94922172321993\n",
      "Epoch 36 | Loss = -41.222395448123706\n",
      "Epoch 37 | Loss = -41.23006273718441\n",
      "Epoch 38 | Loss = -41.34784210429472\n",
      "Epoch 39 | Loss = -41.27110871146707\n",
      "Epoch 40 | Loss = -41.250110261580524\n",
      "Epoch 41 | Loss = -41.32876432643217\n",
      "Epoch 42 | Loss = -41.50499989004696\n",
      "Epoch 43 | Loss = -41.84529332553639\n",
      "Epoch 44 | Loss = -41.71850050196928\n",
      "Epoch 45 | Loss = -41.61943054199219\n",
      "Epoch 46 | Loss = -41.92316248837639\n",
      "Epoch 47 | Loss = -42.08413441040937\n",
      "Epoch 48 | Loss = -42.120780804578\n",
      "Epoch 49 | Loss = -42.20037429472979\n",
      "Epoch 50 | Loss = -42.207803894491754\n",
      "Epoch 51 | Loss = -42.30928465899299\n",
      "Epoch 52 | Loss = -42.27084633883308\n",
      "Epoch 53 | Loss = -42.405857815462\n",
      "Epoch 54 | Loss = -42.29665952570298\n",
      "Epoch 55 | Loss = -42.402552240035114\n",
      "Epoch 56 | Loss = -42.63539103900685\n",
      "Epoch 57 | Loss = -42.82752802792717\n",
      "Epoch 58 | Loss = -42.78990955913768\n",
      "Epoch 59 | Loss = -42.64509164585787\n",
      "Epoch 60 | Loss = -42.900391326231116\n",
      "Epoch 61 | Loss = -42.87360253053553\n",
      "Epoch 62 | Loss = -43.164968714994544\n",
      "Epoch 63 | Loss = -43.17620684118832\n",
      "Epoch 64 | Loss = -42.96016151764814\n",
      "Epoch 65 | Loss = -43.095307770897364\n",
      "Epoch 66 | Loss = -43.296191327712116\n",
      "Epoch 67 | Loss = -43.142778396606445\n",
      "Epoch 68 | Loss = -43.37430255553301\n",
      "Epoch 69 | Loss = -43.29384231567383\n",
      "Epoch 70 | Loss = -43.21072704651777\n",
      "Epoch 71 | Loss = -43.28298807144165\n",
      "Epoch 72 | Loss = -43.645362096674305\n",
      "Epoch 73 | Loss = -43.510582222658044\n",
      "Epoch 74 | Loss = -43.278016455033246\n",
      "Epoch 75 | Loss = -43.47464171577902\n",
      "Epoch 76 | Loss = -43.553877746357635\n",
      "Epoch 77 | Loss = -43.7543989349814\n",
      "Epoch 78 | Loss = -43.779203835655665\n",
      "Epoch 79 | Loss = -43.5919877501095\n",
      "Epoch 80 | Loss = -43.63636370266185\n",
      "Epoch 81 | Loss = -43.773150107439825\n",
      "Epoch 82 | Loss = -43.78087708529304\n",
      "Epoch 83 | Loss = -43.88941215066349\n",
      "Epoch 84 | Loss = -43.960066963644586\n",
      "Epoch 85 | Loss = -44.01088131175322\n",
      "Epoch 86 | Loss = -43.799464983098645\n",
      "Epoch 87 | Loss = -43.88859022364897\n",
      "Epoch 88 | Loss = -43.87713976467357\n",
      "Epoch 89 | Loss = -44.02221676882576\n",
      "Epoch 90 | Loss = -44.092797195210174\n",
      "Epoch 91 | Loss = -44.08689022064209\n",
      "Epoch 92 | Loss = -43.92782508625704\n",
      "Epoch 93 | Loss = -44.52388987821691\n",
      "Epoch 94 | Loss = -44.29229694254258\n",
      "Epoch 95 | Loss = -44.08947515487671\n",
      "Epoch 96 | Loss = -44.303482195910284\n",
      "Epoch 97 | Loss = -44.247587961309094\n",
      "Epoch 98 | Loss = -44.21843475453994\n",
      "Epoch 99 | Loss = -44.30185758366304\n",
      "Epoch 100 | Loss = -44.26243268742281\n",
      "Epoch 101 | Loss = -44.34015018799726\n",
      "Epoch 102 | Loss = -44.1508358226103\n",
      "Epoch 103 | Loss = -44.29339972664328\n",
      "Epoch 104 | Loss = -44.49503595688764\n",
      "Epoch 105 | Loss = -44.31467434939216\n",
      "Epoch 106 | Loss = -44.537138630362115\n",
      "Epoch 107 | Loss = -44.58773862614351\n",
      "Epoch 108 | Loss = -44.45214759602266\n",
      "Epoch 109 | Loss = -44.339015652151666\n",
      "Epoch 110 | Loss = -44.66055227728451\n",
      "Epoch 111 | Loss = -44.42127766328699\n",
      "Epoch 112 | Loss = -44.52911211462582\n",
      "Epoch 113 | Loss = -44.58190457961138\n",
      "Epoch 114 | Loss = -44.549728982588825\n",
      "Epoch 115 | Loss = -44.76170138751759\n",
      "Epoch 116 | Loss = -44.790692413554474\n",
      "Epoch 117 | Loss = -44.778009442722094\n",
      "Epoch 118 | Loss = -44.88653912263758\n",
      "Epoch 119 | Loss = -44.82137469684376\n",
      "Epoch 120 | Loss = -44.87636322133682\n",
      "Epoch 121 | Loss = -44.81342102499569\n",
      "Epoch 122 | Loss = -44.89693703370936\n",
      "Epoch 123 | Loss = -44.871051255394434\n",
      "Epoch 124 | Loss = -45.082373955670526\n",
      "Epoch 125 | Loss = -44.82731154385735\n",
      "Epoch 126 | Loss = -45.01286402870627\n",
      "Epoch 127 | Loss = -45.13959879033706\n",
      "Epoch 128 | Loss = -45.18939175325281\n",
      "Epoch 129 | Loss = -45.0109915452845\n",
      "Epoch 130 | Loss = -45.03910129210528\n",
      "Epoch 131 | Loss = -45.08117549559649\n",
      "Epoch 132 | Loss = -44.739372954649085\n",
      "Epoch 133 | Loss = -45.16365783354815\n",
      "Epoch 134 | Loss = -45.00105641869938\n",
      "Epoch 135 | Loss = -45.038133537068084\n",
      "Epoch 136 | Loss = -45.04582733266494\n",
      "Epoch 137 | Loss = -45.28513529721428\n",
      "Epoch 138 | Loss = -45.29129970774931\n",
      "Epoch 139 | Loss = -45.130086702458996\n",
      "Epoch 140 | Loss = -45.2520951944239\n",
      "Epoch 141 | Loss = -45.386316159192255\n",
      "Epoch 142 | Loss = -45.39966308369356\n",
      "Epoch 143 | Loss = -45.2828631401062\n",
      "Epoch 144 | Loss = -45.25580434238209\n",
      "Epoch 145 | Loss = -45.49271373187794\n",
      "Epoch 146 | Loss = -45.40905009998995\n",
      "Epoch 147 | Loss = -45.104083229513726\n",
      "Epoch 148 | Loss = -45.327992888057935\n",
      "Epoch 149 | Loss = -45.482206905589386\n",
      "Epoch 150 | Loss = -45.222428153542914\n",
      "Epoch 151 | Loss = -45.500605162452246\n",
      "Epoch 152 | Loss = -45.52856636047363\n",
      "Epoch 153 | Loss = -45.36677607368021\n",
      "Epoch 154 | Loss = -45.34855991251328\n",
      "Epoch 155 | Loss = -45.447299368241254\n",
      "Epoch 156 | Loss = -45.34632884754854\n",
      "Epoch 157 | Loss = -45.40231954350191\n",
      "Epoch 158 | Loss = -45.416819852941174\n",
      "Epoch 159 | Loss = -45.54295803518856\n",
      "Epoch 160 | Loss = -45.36417994779699\n",
      "Epoch 161 | Loss = -45.61612796783447\n",
      "Epoch 163 | Loss = -45.750294853659234\n",
      "Epoch 164 | Loss = -45.562926292419434\n",
      "Epoch 165 | Loss = -45.412886787863336\n",
      "Epoch 166 | Loss = -45.62696125928093\n",
      "Epoch 167 | Loss = -45.68770627414479\n",
      "Epoch 168 | Loss = -45.607448044945215\n",
      "Epoch 169 | Loss = -45.667329227223114\n",
      "Epoch 170 | Loss = -45.60898068371941\n",
      "Epoch 171 | Loss = -45.778974168440875\n",
      "Epoch 172 | Loss = -45.50503554063685\n",
      "Epoch 173 | Loss = -45.74293316111845\n",
      "Epoch 174 | Loss = -45.73519400989308\n",
      "Epoch 175 | Loss = -45.6295321969425\n",
      "Epoch 176 | Loss = -45.893042648539826\n",
      "Epoch 177 | Loss = -45.711268929874194\n",
      "Epoch 178 | Loss = -45.79338887158562\n",
      "Epoch 179 | Loss = -45.5348272043116\n",
      "Epoch 180 | Loss = -45.79003785638248\n",
      "Epoch 181 | Loss = -45.818723790785846\n",
      "Epoch 182 | Loss = -45.7622670285842\n",
      "Epoch 183 | Loss = -45.725899303660675\n",
      "Epoch 184 | Loss = -45.802944632137525\n",
      "Epoch 185 | Loss = -46.09044506970574\n",
      "Epoch 186 | Loss = -45.94477154226864\n",
      "Epoch 187 | Loss = -45.71186197505278\n",
      "Epoch 188 | Loss = -45.76062732584336\n",
      "Epoch 189 | Loss = -45.50147959765266\n",
      "Epoch 190 | Loss = -45.814233359168554\n",
      "Epoch 191 | Loss = -45.762214576496795\n",
      "Epoch 192 | Loss = -45.667817508473114\n",
      "Epoch 193 | Loss = -46.01249989341287\n",
      "Epoch 194 | Loss = -45.915765706230616\n",
      "Epoch 195 | Loss = -45.952305288875806\n",
      "Epoch 196 | Loss = -45.79172316719504\n",
      "Epoch 197 | Loss = -45.88347774393418\n",
      "Epoch 198 | Loss = -45.86832105412203\n",
      "Epoch 199 | Loss = -45.93862281126135\n",
      "Epoch 200 | Loss = -46.03767229528988\n",
      "+-----------------------------------------------------------------------------+\n",
      "Training Time: 23939.49583363533 seconds.\n",
      "Get Embedding...\n",
      "(118, 64) (118,)\n",
      "SVC Accuracy: [0.7537878787878789, 0.7310606060606061]\n",
      "+-----------------------------------------------------------------------------+\n",
      "Embedding Time: 13.319809913635254 seconds.\n",
      "Experient 12\n",
      "20211221-091012 :Log the record!\n",
      "+-----------------------------------------------------------------------------+\n",
      "Experient 13\n",
      "!!!\n",
      "89\n",
      "!!!\n",
      "[553, 217, 346, 850, 1139, 72, 280, 861, 916, 589, 50, 182, 307, 1067, 172, 583, 831, 791, 1014, 246, 18, 1013, 59, 1023, 984, 69, 755, 65, 71, 742, 1153, 256, 856, 266, 427, 956, 679, 826, 541, 147, 910, 0, 922, 683, 131, 103, 91, 1001, 166, 558, 796, 1078, 811, 852, 351, 1002, 383, 145, 197, 1072, 789, 17, 606, 9, 608, 1111, 664, 414, 154, 290, 954, 667, 959, 1097, 652, 339, 184, 368, 327, 506, 933, 370, 36, 768, 67, 1158, 93, 1123, 47, 624, 311, 883, 484, 702, 379, 316, 794, 822, 818, 55, 375, 1008, 605, 584, 920, 575, 986, 675, 82, 468, 1104, 202, 428, 566, 85, 550, 914, 1142, 545, 1145, 870, 1149, 332, 732, 185, 401, 359, 540, 714, 277, 785, 56, 45, 330, 1137, 564, 941, 703, 157, 1056, 1025, 325, 394, 360, 1077, 611, 1058, 1131, 830, 1081, 857, 1105, 364, 57, 233, 291, 1065, 926, 181, 231, 230, 808, 731, 678, 588, 929, 226, 537, 680, 38, 746, 595, 733, 244, 138, 1039, 441, 216, 762, 220, 284, 570, 1166, 1103, 117, 476, 261, 312, 232, 424, 149, 1110, 183, 651, 776, 522, 898, 381, 1100, 1088, 221, 1043, 596, 1084, 719, 669, 253, 715, 498, 35, 112, 495, 524, 548, 369, 735, 574, 580, 391, 167, 293, 942, 793, 273, 204, 1049, 474, 292, 973, 129, 1006, 402, 444, 975, 137, 1071, 647, 656, 1080, 1045, 333, 648, 555, 1127, 576, 486, 602, 989, 399, 3, 1030, 1016, 782, 153, 962, 421, 620, 1036, 264, 944, 848, 736, 717, 26, 1121, 978, 338, 398, 449, 46, 778, 877, 773, 633, 287, 465, 957, 828, 462, 800, 419, 902, 709, 953, 549, 997, 724, 78, 779, 1019, 2, 1050, 546, 1048, 195, 1061, 995, 1032, 10, 25, 450, 357, 1135, 254, 559, 612, 912, 965, 179, 94, 739, 470, 616, 298, 900, 224, 598, 301, 436, 1099, 234, 1060, 1053, 839, 815, 158, 804, 945, 89, 31, 210, 365, 993, 413, 225, 115, 618, 42, 567, 320, 1125, 644, 532, 337, 513, 168, 609, 243, 494, 186, 734, 1159, 193, 439, 305, 641, 628, 300, 521, 636, 948, 198, 994, 556, 96, 1130, 568, 22, 557, 1021, 132, 938, 707, 463, 723, 1152, 915, 711, 156, 631, 931, 457, 1066, 433, 666, 464, 459, 597, 816, 471, 725, 614, 1063, 397, 716, 684, 176, 227, 577, 1004, 801, 637, 34, 761, 302, 404, 1090, 980, 1028, 262, 161, 426, 751, 617, 1000, 39, 345, 504, 880, 907, 409, 173, 515, 249, 1069, 285, 1156, 730, 350, 260, 150, 281, 923, 361, 1062, 918, 1052, 904, 282, 1095, 219, 503, 738, 88, 668, 563, 710, 946, 207, 146, 60, 340, 760, 691, 106, 827, 265, 571, 15, 1132, 1015, 985, 825, 970, 758, 223, 406, 921, 1044, 772, 120, 247, 4, 542, 101, 859, 467, 1150, 12, 871, 268, 1012, 630, 1009, 336, 322, 621, 491, 1026, 187, 629, 432, 385, 992, 809, 1024, 756, 366, 62, 188, 53, 899, 950, 939, 1175, 988, 13, 105, 692, 659, 160, 960, 134, 875, 492, 107, 505, 757, 932, 653, 533, 175, 342, 314, 1165, 936, 1114, 252, 639, 8, 695, 838, 539, 824, 458, 408, 508, 248, 148, 352, 1162, 663, 911, 578, 318, 1116, 108, 1106, 235, 844, 54, 889, 645, 586, 1068, 239, 326, 763, 689, 104, 81, 323, 48, 1031, 908, 1041, 851, 1124, 452, 961, 122, 127, 1108, 388, 215, 155, 73, 887, 475, 77, 348, 485, 84, 1148, 650, 509, 75, 299, 331, 144, 241, 288, 727, 619, 775, 625, 206, 977, 455, 934, 697, 777, 865, 245, 835, 585, 133, 70, 655, 1161, 1092, 892, 190, 7, 32, 807, 110, 693, 1134, 1064, 295, 19, 872, 729, 1154, 289, 1176, 657, 142, 890, 445, 952, 275, 866, 587, 927, 603, 943, 211, 1118, 728, 593, 701, 477, 222, 196, 440, 205, 531, 981, 354, 478, 165, 242, 958, 1115, 1075, 1074, 1007, 24, 967, 913, 49, 744, 11, 554, 201, 321, 437, 517, 999, 814, 529, 681, 236, 180, 750, 780, 135, 868, 516, 879, 820, 315, 1128, 560, 218, 845, 297, 949, 841, 1059, 309, 1167, 1155, 665, 425, 1172, 461, 812, 79, 143, 622, 983, 937, 438, 353, 832, 6, 901, 795, 990, 759, 869, 1120, 276, 686, 810, 199, 371, 174, 677, 627, 116, 1133, 435, 519, 113, 496, 1029, 706, 255, 764, 130, 1005, 525, 500, 453, 367, 720, 171, 278, 483, 1122, 754, 698, 1051, 599, 1126, 74, 867, 726, 885, 250, 191, 1027, 229, 996, 310, 209, 884, 111, 456, 1082, 76, 358, 343, 66, 356, 213, 874, 1164, 362, 813, 1138, 5, 1055, 324, 888, 480, 849, 1033, 228, 267, 410, 786, 1035, 139, 873, 972, 740, 303, 951, 304, 1140, 543, 682, 382, 393, 896, 488, 718, 447, 1054, 417, 643, 43, 964, 14, 237, 189, 662, 29, 306, 1144, 119, 654, 86, 1107, 783, 511, 876, 1136, 696, 1146, 1086, 507, 341, 649, 771, 169, 274, 37, 376, 317, 279, 395, 840, 966, 251, 687, 878, 787, 159, 600, 1087, 905, 319, 1160, 378, 1094, 263, 52, 987, 344, 308, 363, 416, 396, 924, 769, 688, 33, 615, 460, 979, 906, 121, 895, 1171, 102, 579, 526, 882, 1168, 177, 523, 380, 1076, 864, 1085, 125, 674, 83, 781, 64, 124, 1047, 51, 20, 661, 767, 258, 151, 833, 638, 1109, 590, 635, 1169, 930, 372, 448, 982, 991, 846, 538, 374, 792, 446, 708, 528, 283, 819, 1089, 858, 1038, 806, 466, 493, 1174, 347, 114, 136, 128, 863, 935, 1093, 415, 269, 510, 1129, 834, 1, 389, 68, 749, 963, 552, 487, 430, 212, 971, 847, 745, 194, 162, 713, 296, 1011, 592, 582, 917, 594, 429, 853, 1073, 784, 520, 499, 469, 451, 1079, 572, 862, 753, 737, 27, 1117, 547, 947, 123, 894, 1112, 855, 384, 1034, 422, 490, 712, 843, 192, 1173, 203, 634, 514, 928, 694, 423, 141, 420, 329, 1017, 1119, 377, 1046, 527, 998, 349, 272, 454, 1057, 501, 178, 392, 405, 774, 472, 238, 1042, 534, 699, 482, 1018, 569, 842, 164, 660, 1113, 536, 817, 95, 240, 387, 854, 126, 561, 743, 431, 672, 802, 208, 766, 690, 390, 1101, 591, 604, 109, 909, 803, 28, 286, 765, 163, 601, 1163, 722, 90, 1177, 1037, 152, 1098, 805, 334, 294, 829, 700, 974, 21]\n",
      "Number of Graphs (dataset_train): 1060\n",
      "Number of Graphs (dataset_test): 118\n",
      "+-----------------------------------------------------------------------------+\n",
      "Learning Rate: 0.001\n",
      "Epochs: 200\n",
      "Batch Size: 64\n",
      "Hidden Dimension 512\n",
      "Number of Layer in Encoder: 1\n",
      "Opimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device: cuda\n",
      "Model:\n",
      "SimCLR(\n",
      "  (encoder): Encoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=89, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      ))\n",
      "    )\n",
      "    (bns): ModuleList(\n",
      "      (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Projection_MLP(\n",
      "    (layer1): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (unilayer): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Start Training...\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss = -26.01702095480526\n",
      "Epoch 2 | Loss = -36.74648977728451\n",
      "Epoch 3 | Loss = -41.534054700066065\n",
      "Epoch 4 | Loss = -44.08020504783182\n",
      "Epoch 5 | Loss = -45.50912167044247\n",
      "Epoch 6 | Loss = -46.84841958214255\n",
      "Epoch 7 | Loss = -47.59523857341093\n",
      "Epoch 8 | Loss = -48.32154997657327\n",
      "Epoch 9 | Loss = -48.67053595711203\n",
      "Epoch 10 | Loss = -49.320900187772864\n",
      "Epoch 11 | Loss = -49.56415580300724\n",
      "Epoch 12 | Loss = -50.01561574374928\n",
      "Epoch 13 | Loss = -50.268473120296704\n",
      "Epoch 14 | Loss = -50.63704097972197\n",
      "Epoch 15 | Loss = -50.883907065672034\n",
      "Epoch 16 | Loss = -51.19917690052706\n",
      "Epoch 17 | Loss = -51.42579802344827\n",
      "Epoch 18 | Loss = -51.48309937645407\n",
      "Epoch 19 | Loss = -51.72596575232113\n",
      "Epoch 20 | Loss = -51.827175224528595\n",
      "Epoch 21 | Loss = -51.97249362047981\n",
      "Epoch 22 | Loss = -52.092828946955066\n",
      "Epoch 23 | Loss = -52.31981330759385\n",
      "Epoch 24 | Loss = -52.425529704374426\n",
      "Epoch 25 | Loss = -52.582330367144415\n",
      "Epoch 26 | Loss = -52.750231237972486\n",
      "Epoch 27 | Loss = -52.849343608407416\n",
      "Epoch 28 | Loss = -52.920329963459686\n",
      "Epoch 29 | Loss = -53.05163139455459\n",
      "Epoch 30 | Loss = -53.29246063793407\n",
      "Epoch 31 | Loss = -53.22185880997602\n",
      "Epoch 32 | Loss = -53.46070906695198\n",
      "Epoch 33 | Loss = -53.50691730835859\n",
      "Epoch 34 | Loss = -53.56319326512954\n",
      "Epoch 35 | Loss = -53.68264686360079\n",
      "Epoch 36 | Loss = -53.80148175183464\n",
      "Epoch 37 | Loss = -53.727268387289605\n",
      "Epoch 38 | Loss = -53.83526964748607\n",
      "Epoch 39 | Loss = -53.898129407097315\n",
      "Epoch 40 | Loss = -54.08807723662432\n",
      "Epoch 41 | Loss = -54.281316588906684\n",
      "Epoch 42 | Loss = -54.24696355707505\n",
      "Epoch 43 | Loss = -54.29958441678215\n",
      "Epoch 44 | Loss = -54.385264480815216\n",
      "Epoch 45 | Loss = -54.476390333736646\n",
      "Epoch 46 | Loss = -54.42789316177368\n",
      "Epoch 47 | Loss = -54.52618108076208\n",
      "Epoch 48 | Loss = -54.59682099959429\n",
      "Epoch 49 | Loss = -54.623764767366296\n",
      "Epoch 50 | Loss = -54.676783982445215\n",
      "Epoch 51 | Loss = -54.73372936248779\n",
      "Epoch 52 | Loss = -54.75368634392233\n",
      "Epoch 53 | Loss = -54.79663955464083\n",
      "Epoch 54 | Loss = -54.831120771520276\n",
      "Epoch 55 | Loss = -55.004840906928564\n",
      "Epoch 56 | Loss = -55.05063149508308\n",
      "Epoch 57 | Loss = -54.99424623040592\n",
      "Epoch 58 | Loss = -55.1806801627664\n",
      "Epoch 59 | Loss = -55.14917118409101\n",
      "Epoch 60 | Loss = -55.23444576824413\n",
      "Epoch 61 | Loss = -55.1655291669509\n",
      "Epoch 62 | Loss = -55.25151735193589\n",
      "Epoch 63 | Loss = -55.2696743852952\n",
      "Epoch 64 | Loss = -55.37968310187845\n",
      "Epoch 65 | Loss = -55.33533037410063\n",
      "Epoch 66 | Loss = -55.32200964759378\n",
      "Epoch 67 | Loss = -55.3261869654936\n",
      "Epoch 68 | Loss = -55.37228682461907\n",
      "Epoch 69 | Loss = -55.38297451243681\n",
      "Epoch 70 | Loss = -55.26786624684053\n",
      "Epoch 71 | Loss = -55.361152957467475\n",
      "Epoch 72 | Loss = -55.43841718224918\n",
      "Epoch 73 | Loss = -55.414805973277375\n",
      "Epoch 74 | Loss = -55.56858107622932\n",
      "Epoch 75 | Loss = -55.608465026406684\n",
      "Epoch 76 | Loss = -55.626374272739184\n",
      "Epoch 77 | Loss = -55.71639711716596\n",
      "Epoch 78 | Loss = -55.65396738052368\n",
      "Epoch 79 | Loss = -55.70254280987908\n",
      "Epoch 80 | Loss = -55.81617790109971\n",
      "Epoch 81 | Loss = -55.85460177589865\n",
      "Epoch 82 | Loss = -55.82807535283706\n",
      "Epoch 83 | Loss = -55.88305335886338\n",
      "Epoch 84 | Loss = -55.88606660506304\n",
      "Epoch 85 | Loss = -55.98869688370649\n",
      "Epoch 86 | Loss = -55.973900738884424\n",
      "Epoch 87 | Loss = -55.88180595285752\n",
      "Epoch 88 | Loss = -55.95233370276058\n",
      "Epoch 89 | Loss = -56.015927483053765\n",
      "Epoch 90 | Loss = -56.0643084750456\n",
      "Epoch 91 | Loss = -55.99394052168902\n",
      "Epoch 92 | Loss = -56.125335805556354\n",
      "Epoch 93 | Loss = -56.046077868517706\n",
      "Epoch 94 | Loss = -56.13875840691959\n",
      "Epoch 95 | Loss = -56.16854936936323\n",
      "Epoch 96 | Loss = -56.26290997336893\n",
      "Epoch 97 | Loss = -56.19915022569544\n",
      "Epoch 98 | Loss = -56.25963791678934\n",
      "Epoch 99 | Loss = -56.33088529811186\n",
      "Epoch 100 | Loss = -56.2239870744593\n",
      "Epoch 101 | Loss = -56.27492871003992\n",
      "Epoch 102 | Loss = -56.262534478131464\n",
      "Epoch 103 | Loss = -56.275065057417926\n",
      "Epoch 104 | Loss = -56.365243434906006\n",
      "Epoch 105 | Loss = -56.3737611209645\n",
      "Epoch 106 | Loss = -56.417621079613184\n",
      "Epoch 107 | Loss = -56.40359676585478\n",
      "Epoch 108 | Loss = -56.40244795294369\n",
      "Epoch 109 | Loss = -56.45065338471357\n",
      "Epoch 110 | Loss = -56.4211849044351\n",
      "Epoch 111 | Loss = -56.49251691032858\n",
      "Epoch 112 | Loss = -56.47237042819752\n",
      "Epoch 113 | Loss = -56.46662521362305\n",
      "Epoch 114 | Loss = -56.57637879427742\n",
      "Epoch 115 | Loss = -56.608269074383905\n",
      "Epoch 116 | Loss = -56.48202405256384\n",
      "Epoch 117 | Loss = -56.546987393323114\n",
      "Epoch 118 | Loss = -56.597864375394934\n",
      "Epoch 119 | Loss = -56.661268037908215\n",
      "Epoch 120 | Loss = -56.63665339526008\n",
      "Epoch 121 | Loss = -56.67232373181511\n",
      "Epoch 122 | Loss = -56.75313562505386\n",
      "Epoch 123 | Loss = -56.64778111962711\n",
      "Epoch 124 | Loss = -56.726595626157874\n",
      "Epoch 125 | Loss = -56.75294505848604\n",
      "Epoch 126 | Loss = -56.785761160009045\n",
      "Epoch 127 | Loss = -56.70504471834968\n",
      "Epoch 128 | Loss = -56.760657029993396\n",
      "Epoch 129 | Loss = -56.75516605377197\n",
      "Epoch 130 | Loss = -56.73846648721134\n",
      "Epoch 131 | Loss = -56.777705164516675\n",
      "Epoch 132 | Loss = -56.797350013957306\n",
      "Epoch 133 | Loss = -56.77480983734131\n",
      "Epoch 134 | Loss = -56.847344286301556\n",
      "Epoch 135 | Loss = -56.878012152279126\n",
      "Epoch 136 | Loss = -56.93151959250955\n",
      "Epoch 137 | Loss = -56.94645143957699\n",
      "Epoch 138 | Loss = -56.94586133956909\n",
      "Epoch 139 | Loss = -56.89767391541425\n",
      "Epoch 140 | Loss = -56.958348638871136\n",
      "Epoch 141 | Loss = -56.94191621331608\n",
      "Epoch 142 | Loss = -56.89613359114703\n",
      "Epoch 143 | Loss = -56.950322543873504\n",
      "Epoch 144 | Loss = -56.99190176234526\n",
      "Epoch 145 | Loss = -56.995281780467316\n",
      "Epoch 146 | Loss = -57.04809859219719\n",
      "Epoch 147 | Loss = -57.02833817986881\n",
      "Epoch 148 | Loss = -56.98360908732695\n",
      "Epoch 149 | Loss = -56.986378389246326\n",
      "Epoch 150 | Loss = -56.97845703012803\n",
      "Epoch 151 | Loss = -57.0247119735269\n",
      "Epoch 152 | Loss = -56.9900001077091\n",
      "Epoch 153 | Loss = -56.979226869695324\n",
      "Epoch 154 | Loss = -56.867899165434\n",
      "Epoch 155 | Loss = -56.880567466511444\n",
      "Epoch 156 | Loss = -56.98902416229248\n",
      "Epoch 157 | Loss = -57.06305921778959\n",
      "Epoch 158 | Loss = -57.05972761266372\n",
      "Epoch 159 | Loss = -57.102721999673285\n",
      "Epoch 160 | Loss = -57.164917188532215\n",
      "Epoch 161 | Loss = -57.1761640660903\n",
      "Epoch 162 | Loss = -57.15964202319874\n",
      "Epoch 163 | Loss = -57.11373836853925\n",
      "Epoch 164 | Loss = -57.17588046017815\n",
      "Epoch 165 | Loss = -57.2045436185949\n",
      "Epoch 166 | Loss = -57.17251382154577\n",
      "Epoch 167 | Loss = -57.20581481036018\n",
      "Epoch 168 | Loss = -57.24894245933084\n",
      "Epoch 169 | Loss = -57.24357672298656\n",
      "Epoch 170 | Loss = -57.214030013364905\n",
      "Epoch 171 | Loss = -57.243822799009436\n",
      "Epoch 172 | Loss = -57.231784736408905\n",
      "Epoch 173 | Loss = -57.32246721492094\n",
      "Epoch 174 | Loss = -57.268471128800336\n",
      "Epoch 175 | Loss = -57.38089909273035\n",
      "Epoch 176 | Loss = -57.36803851408117\n",
      "Epoch 177 | Loss = -57.27933241339291\n",
      "Epoch 178 | Loss = -57.19049622030819\n",
      "Epoch 179 | Loss = -57.15527618632597\n",
      "Epoch 180 | Loss = -57.22669719247257\n",
      "Epoch 181 | Loss = -57.24836607540355\n",
      "Epoch 182 | Loss = -57.371822721817914\n",
      "Epoch 183 | Loss = -57.34785747528076\n",
      "Epoch 184 | Loss = -57.38561375000898\n",
      "Epoch 185 | Loss = -57.45138098211849\n",
      "Epoch 186 | Loss = -57.42747688293457\n",
      "Epoch 187 | Loss = -57.383135206559125\n",
      "Epoch 188 | Loss = -57.37564375821282\n",
      "Epoch 189 | Loss = -57.34373042162727\n",
      "Epoch 190 | Loss = -57.47665755888995\n",
      "Epoch 191 | Loss = -57.450388992533966\n",
      "Epoch 192 | Loss = -57.384710115544934\n",
      "Epoch 193 | Loss = -57.40248051811667\n",
      "Epoch 194 | Loss = -57.482196050531726\n",
      "Epoch 195 | Loss = -57.47944615868961\n",
      "Epoch 196 | Loss = -57.440010267145496\n",
      "Epoch 197 | Loss = -57.48061853296616\n",
      "Epoch 198 | Loss = -57.47760999903959\n",
      "Epoch 199 | Loss = -57.44241125443403\n",
      "Epoch 200 | Loss = -57.46856229445513\n",
      "+-----------------------------------------------------------------------------+\n",
      "Training Time: 23976.75683283806 seconds.\n",
      "Get Embedding...\n",
      "(118, 512) (118,)\n",
      "SVC Accuracy: [0.762121212121212, 0.7537878787878789]\n",
      "+-----------------------------------------------------------------------------+\n",
      "Embedding Time: 13.969263315200806 seconds.\n",
      "Experient 13\n",
      "20211221-155004 :Log the record!\n",
      "+-----------------------------------------------------------------------------+\n",
      "Experient 14\n",
      "!!!\n",
      "89\n",
      "!!!\n",
      "[553, 217, 346, 850, 1139, 72, 280, 861, 916, 589, 50, 182, 307, 1067, 172, 583, 831, 791, 1014, 246, 18, 1013, 59, 1023, 984, 69, 755, 65, 71, 742, 1153, 256, 856, 266, 427, 956, 679, 826, 541, 147, 910, 0, 922, 683, 131, 103, 91, 1001, 166, 558, 796, 1078, 811, 852, 351, 1002, 383, 145, 197, 1072, 789, 17, 606, 9, 608, 1111, 664, 414, 154, 290, 954, 667, 959, 1097, 652, 339, 184, 368, 327, 506, 933, 370, 36, 768, 67, 1158, 93, 1123, 47, 624, 311, 883, 484, 702, 379, 316, 794, 822, 818, 55, 375, 1008, 605, 584, 920, 575, 986, 675, 82, 468, 1104, 202, 428, 566, 85, 550, 914, 1142, 545, 1145, 870, 1149, 332, 732, 185, 401, 359, 540, 714, 277, 785, 56, 45, 330, 1137, 564, 941, 703, 157, 1056, 1025, 325, 394, 360, 1077, 611, 1058, 1131, 830, 1081, 857, 1105, 364, 57, 233, 291, 1065, 926, 181, 231, 230, 808, 731, 678, 588, 929, 226, 537, 680, 38, 746, 595, 733, 244, 138, 1039, 441, 216, 762, 220, 284, 570, 1166, 1103, 117, 476, 261, 312, 232, 424, 149, 1110, 183, 651, 776, 522, 898, 381, 1100, 1088, 221, 1043, 596, 1084, 719, 669, 253, 715, 498, 35, 112, 495, 524, 548, 369, 735, 574, 580, 391, 167, 293, 942, 793, 273, 204, 1049, 474, 292, 973, 129, 1006, 402, 444, 975, 137, 1071, 647, 656, 1080, 1045, 333, 648, 555, 1127, 576, 486, 602, 989, 399, 3, 1030, 1016, 782, 153, 962, 421, 620, 1036, 264, 944, 848, 736, 717, 26, 1121, 978, 338, 398, 449, 46, 778, 877, 773, 633, 287, 465, 957, 828, 462, 800, 419, 902, 709, 953, 549, 997, 724, 78, 779, 1019, 2, 1050, 546, 1048, 195, 1061, 995, 1032, 10, 25, 450, 357, 1135, 254, 559, 612, 912, 965, 179, 94, 739, 470, 616, 298, 900, 224, 598, 301, 436, 1099, 234, 1060, 1053, 839, 815, 158, 804, 945, 89, 31, 210, 365, 993, 413, 225, 115, 618, 42, 567, 320, 1125, 644, 532, 337, 513, 168, 609, 243, 494, 186, 734, 1159, 193, 439, 305, 641, 628, 300, 521, 636, 948, 198, 994, 556, 96, 1130, 568, 22, 557, 1021, 132, 938, 707, 463, 723, 1152, 915, 711, 156, 631, 931, 457, 1066, 433, 666, 464, 459, 597, 816, 471, 725, 614, 1063, 397, 716, 684, 176, 227, 577, 1004, 801, 637, 34, 761, 302, 404, 1090, 980, 1028, 262, 161, 426, 751, 617, 1000, 39, 345, 504, 880, 907, 409, 173, 515, 249, 1069, 285, 1156, 730, 350, 260, 150, 281, 923, 361, 1062, 918, 1052, 904, 282, 1095, 219, 503, 738, 88, 668, 563, 710, 946, 207, 146, 60, 340, 760, 691, 106, 827, 265, 571, 15, 1132, 1015, 985, 825, 970, 758, 223, 406, 921, 1044, 772, 120, 247, 4, 542, 101, 859, 467, 1150, 12, 871, 268, 1012, 630, 1009, 336, 322, 621, 491, 1026, 187, 629, 432, 385, 992, 809, 1024, 756, 366, 62, 188, 53, 899, 950, 939, 1175, 988, 13, 105, 692, 659, 160, 960, 134, 875, 492, 107, 505, 757, 932, 653, 533, 175, 342, 314, 1165, 936, 1114, 252, 639, 8, 695, 838, 539, 824, 458, 408, 508, 248, 148, 352, 1162, 663, 911, 578, 318, 1116, 108, 1106, 235, 844, 54, 889, 645, 586, 1068, 239, 326, 763, 689, 104, 81, 323, 48, 1031, 908, 1041, 851, 1124, 452, 961, 122, 127, 1108, 388, 215, 155, 73, 887, 475, 77, 348, 485, 84, 1148, 650, 509, 75, 299, 331, 144, 241, 288, 727, 619, 775, 625, 206, 977, 455, 934, 697, 777, 865, 245, 835, 585, 133, 70, 655, 1161, 1092, 892, 190, 7, 32, 807, 110, 693, 1134, 1064, 295, 19, 872, 729, 1154, 289, 1176, 657, 142, 890, 445, 952, 275, 866, 587, 927, 603, 943, 211, 1118, 728, 593, 701, 477, 222, 196, 440, 205, 531, 981, 354, 478, 165, 242, 958, 1115, 1075, 1074, 1007, 24, 967, 913, 49, 744, 11, 554, 201, 321, 437, 517, 999, 814, 529, 681, 236, 180, 750, 780, 135, 868, 516, 879, 820, 315, 1128, 560, 218, 845, 297, 949, 841, 1059, 309, 1167, 1155, 665, 425, 1172, 461, 812, 79, 143, 622, 983, 937, 438, 353, 832, 6, 901, 795, 990, 759, 869, 1120, 276, 686, 810, 199, 371, 174, 677, 627, 116, 1133, 435, 519, 113, 496, 1029, 706, 255, 764, 130, 1005, 525, 500, 453, 367, 720, 171, 278, 483, 1122, 754, 698, 1051, 599, 1126, 74, 867, 726, 885, 250, 191, 1027, 229, 996, 310, 209, 884, 111, 456, 1082, 76, 358, 343, 66, 356, 213, 874, 1164, 362, 813, 1138, 5, 1055, 324, 888, 480, 849, 1033, 228, 267, 410, 786, 1035, 139, 873, 972, 740, 303, 951, 304, 1140, 543, 682, 382, 393, 896, 488, 718, 447, 1054, 417, 643, 43, 964, 14, 237, 189, 662, 29, 306, 1144, 119, 654, 86, 1107, 783, 511, 876, 1136, 696, 1146, 1086, 507, 341, 649, 771, 169, 274, 37, 376, 317, 279, 395, 840, 966, 251, 687, 878, 787, 159, 600, 1087, 905, 319, 1160, 378, 1094, 263, 52, 987, 344, 308, 363, 416, 396, 924, 769, 688, 33, 615, 460, 979, 906, 121, 895, 1171, 102, 579, 526, 882, 1168, 177, 523, 380, 1076, 864, 1085, 125, 674, 83, 781, 64, 124, 1047, 51, 20, 661, 767, 258, 151, 833, 638, 1109, 590, 635, 1169, 930, 372, 448, 982, 991, 846, 538, 374, 792, 446, 708, 528, 283, 819, 1089, 858, 1038, 806, 466, 493, 1174, 347, 114, 136, 128, 863, 935, 1093, 415, 269, 510, 1129, 834, 1, 389, 68, 749, 963, 552, 487, 430, 212, 971, 847, 745, 194, 162, 713, 296, 1011, 592, 582, 917, 594, 429, 853, 1073, 784, 520, 499, 469, 451, 1079, 572, 862, 753, 737, 27, 1117, 547, 947, 123, 894, 1112, 855, 384, 1034, 422, 490, 712, 843, 192, 1173, 203, 634, 514, 928, 694, 423, 141, 420, 329, 1017, 1119, 377, 1046, 527, 998, 349, 272, 454, 1057, 501, 178, 392, 405, 774, 472, 238, 1042, 534, 699, 482, 1018, 569, 842, 164, 660, 1113, 536, 817, 95, 240, 387, 854, 126, 561, 743, 431, 672, 802, 208, 766, 690, 390, 1101, 591, 604, 109, 909, 803, 28, 286, 765, 163, 601, 1163, 722, 90, 1177, 1037, 152, 1098, 805, 334, 294, 829, 700, 974, 21]\n",
      "Number of Graphs (dataset_train): 1060\n",
      "Number of Graphs (dataset_test): 118\n",
      "+-----------------------------------------------------------------------------+\n",
      "Learning Rate: 0.001\n",
      "Epochs: 200\n",
      "Batch Size: 64\n",
      "Hidden Dimension 64\n",
      "Number of Layer in Encoder: 2\n",
      "Opimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device: cuda\n",
      "Model:\n",
      "SimCLR(\n",
      "  (encoder): Encoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=89, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      ))\n",
      "      (1): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      ))\n",
      "    )\n",
      "    (bns): ModuleList(\n",
      "      (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Projection_MLP(\n",
      "    (layer1): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (unilayer): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Start Training...\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss = 7.4745956028209015\n",
      "Epoch 2 | Loss = -14.69085137984332\n",
      "Epoch 3 | Loss = -22.989131983588724\n",
      "Epoch 4 | Loss = -27.510546375723447\n",
      "Epoch 5 | Loss = -30.607957363128662\n",
      "Epoch 6 | Loss = -32.57696336858413\n",
      "Epoch 7 | Loss = -34.31185747595394\n",
      "Epoch 8 | Loss = -35.744036842794976\n",
      "Epoch 9 | Loss = -36.81435293309829\n",
      "Epoch 10 | Loss = -37.737567368675684\n",
      "Epoch 11 | Loss = -38.24898557101979\n",
      "Epoch 12 | Loss = -38.931923277237836\n",
      "Epoch 13 | Loss = -39.75949893278234\n",
      "Epoch 14 | Loss = -40.2847684691934\n",
      "Epoch 15 | Loss = -40.86909742916332\n",
      "Epoch 16 | Loss = -41.153216782738184\n",
      "Epoch 17 | Loss = -41.63889046276317\n",
      "Epoch 18 | Loss = -42.00497714210959\n",
      "Epoch 19 | Loss = -42.21233906465418\n",
      "Epoch 20 | Loss = -42.73711058672737\n",
      "Epoch 21 | Loss = -43.02944071152631\n",
      "Epoch 22 | Loss = -43.31127363092759\n",
      "Epoch 23 | Loss = -43.562892184538\n",
      "Epoch 24 | Loss = -43.45318000456866\n",
      "Epoch 25 | Loss = -44.06844060561236\n",
      "Epoch 26 | Loss = -44.32980913274429\n",
      "Epoch 27 | Loss = -44.61016231424668\n",
      "Epoch 28 | Loss = -44.74260240442612\n",
      "Epoch 29 | Loss = -45.10297587338616\n",
      "Epoch 30 | Loss = -45.24289565927842\n",
      "Epoch 31 | Loss = -45.293969350702625\n",
      "Epoch 32 | Loss = -45.421857833862305\n",
      "Epoch 33 | Loss = -45.78456354141235\n",
      "Epoch 34 | Loss = -46.00161454256843\n",
      "Epoch 35 | Loss = -45.85824814964743\n",
      "Epoch 36 | Loss = -46.098409456365246\n",
      "Epoch 37 | Loss = -46.19645814334645\n",
      "Epoch 38 | Loss = -46.24722231135649\n",
      "Epoch 39 | Loss = -46.49543234881233\n",
      "Epoch 40 | Loss = -46.7381810019998\n",
      "Epoch 41 | Loss = -46.915998458862305\n",
      "Epoch 42 | Loss = -46.77413351395551\n",
      "Epoch 43 | Loss = -47.03375833174761\n",
      "Epoch 44 | Loss = -47.1315403545604\n",
      "Epoch 45 | Loss = -47.18835614709293\n",
      "Epoch 46 | Loss = -47.21828643013449\n",
      "Epoch 47 | Loss = -47.43213622710284\n",
      "Epoch 48 | Loss = -47.451503473169666\n",
      "Epoch 49 | Loss = -47.66406090119306\n",
      "Epoch 50 | Loss = -47.633925409878\n",
      "Epoch 51 | Loss = -47.653678361107325\n",
      "Epoch 52 | Loss = -48.01262238446404\n",
      "Epoch 53 | Loss = -47.85682484682869\n",
      "Epoch 54 | Loss = -48.038760886472815\n",
      "Epoch 55 | Loss = -48.01220052382525\n",
      "Epoch 56 | Loss = -48.255790429956775\n",
      "Epoch 57 | Loss = -48.23711145625395\n",
      "Epoch 58 | Loss = -48.18022882237154\n",
      "Epoch 59 | Loss = -48.38963017744177\n",
      "Epoch 60 | Loss = -48.68175400004667\n",
      "Epoch 61 | Loss = -48.406726472518024\n",
      "Epoch 62 | Loss = -48.707477261038385\n",
      "Epoch 63 | Loss = -48.613753346835864\n",
      "Epoch 64 | Loss = -48.678896595450006\n",
      "Epoch 65 | Loss = -48.91968718697043\n",
      "Epoch 66 | Loss = -48.679208390852985\n",
      "Epoch 67 | Loss = -48.94720607645371\n",
      "Epoch 68 | Loss = -48.99015962376314\n",
      "Epoch 69 | Loss = -48.958678133347455\n",
      "Epoch 70 | Loss = -49.119895935058594\n",
      "Epoch 71 | Loss = -49.24726177664364\n",
      "Epoch 72 | Loss = -49.130171607522406\n",
      "Epoch 73 | Loss = -49.11962253907148\n",
      "Epoch 74 | Loss = -49.30856118482702\n",
      "Epoch 75 | Loss = -49.27851878895479\n",
      "Epoch 76 | Loss = -49.07997866237865\n",
      "Epoch 77 | Loss = -49.42027835284962\n",
      "Epoch 78 | Loss = -49.23702719632317\n",
      "Epoch 79 | Loss = -49.57128895030302\n",
      "Epoch 80 | Loss = -49.627434842726764\n",
      "Epoch 81 | Loss = -49.6509669528288\n",
      "Epoch 82 | Loss = -49.52612262613633\n",
      "Epoch 83 | Loss = -49.69373209336225\n",
      "Epoch 84 | Loss = -49.67714834213257\n",
      "Epoch 85 | Loss = -49.69307728374706\n",
      "Epoch 86 | Loss = -49.83741280611824\n",
      "Epoch 87 | Loss = -49.771382331848145\n",
      "Epoch 88 | Loss = -49.644176342908075\n",
      "Epoch 89 | Loss = -49.663288172553564\n",
      "Epoch 90 | Loss = -49.8167849148021\n",
      "Epoch 91 | Loss = -50.06491641437306\n",
      "Epoch 92 | Loss = -50.15683535968556\n",
      "Epoch 93 | Loss = -49.992675248314356\n",
      "Epoch 94 | Loss = -49.9518526021172\n",
      "Epoch 95 | Loss = -50.08520678912892\n",
      "Epoch 96 | Loss = -50.132884334115424\n",
      "Epoch 97 | Loss = -50.19520568847656\n",
      "Epoch 98 | Loss = -50.1860655335819\n",
      "Epoch 99 | Loss = -50.20322297601139\n",
      "Epoch 100 | Loss = -50.203117707196405\n",
      "Epoch 101 | Loss = -50.4297090698691\n",
      "Epoch 102 | Loss = -50.495783889994904\n",
      "Epoch 103 | Loss = -50.46073975282557\n",
      "Epoch 104 | Loss = -50.395925101111914\n",
      "Epoch 105 | Loss = -50.502555202035346\n",
      "Epoch 106 | Loss = -50.49856334574082\n",
      "Epoch 107 | Loss = -50.50076610901777\n",
      "Epoch 108 | Loss = -50.42046084123499\n",
      "Epoch 109 | Loss = -50.58002441069659\n",
      "Epoch 110 | Loss = -50.611691306619086\n",
      "Epoch 111 | Loss = -50.55417220732745\n",
      "Epoch 112 | Loss = -50.71111561270321\n",
      "Epoch 113 | Loss = -50.587242266711065\n",
      "Epoch 114 | Loss = -50.533170223236084\n",
      "Epoch 115 | Loss = -50.5657125080333\n",
      "Epoch 116 | Loss = -50.894805291119745\n",
      "Epoch 117 | Loss = -50.65968785566442\n",
      "Epoch 118 | Loss = -50.731940521913415\n",
      "Epoch 119 | Loss = -50.71361667969648\n",
      "Epoch 120 | Loss = -50.935085100286145\n",
      "Epoch 121 | Loss = -50.784159043255976\n",
      "Epoch 122 | Loss = -50.72430621876436\n",
      "Epoch 123 | Loss = -50.90254441429587\n",
      "Epoch 124 | Loss = -51.07738449994255\n",
      "Epoch 125 | Loss = -51.05345518448774\n",
      "Epoch 126 | Loss = -51.00791140163646\n",
      "Epoch 127 | Loss = -50.91711437000948\n",
      "Epoch 128 | Loss = -50.98517995722153\n",
      "Epoch 129 | Loss = -51.13967438305126\n",
      "Epoch 130 | Loss = -51.08742119284237\n",
      "Epoch 131 | Loss = -51.21725247888004\n",
      "Epoch 132 | Loss = -51.199939755832446\n",
      "Epoch 133 | Loss = -51.102784998276654\n",
      "Epoch 134 | Loss = -51.2217220138101\n",
      "Epoch 135 | Loss = -51.228378408095416\n",
      "Epoch 136 | Loss = -51.29589055566227\n",
      "Epoch 137 | Loss = -51.30621811922859\n",
      "Epoch 138 | Loss = -51.341930754044476\n",
      "Epoch 139 | Loss = -51.298192753511316\n",
      "Epoch 140 | Loss = -51.51176130070406\n",
      "Epoch 141 | Loss = -51.36007833480835\n",
      "Epoch 142 | Loss = -51.45254550260656\n",
      "Epoch 143 | Loss = -51.432214316199804\n",
      "Epoch 144 | Loss = -51.336564148173615\n",
      "Epoch 145 | Loss = -51.45920966653263\n",
      "Epoch 146 | Loss = -51.49675745122573\n",
      "Epoch 147 | Loss = -51.5198704775642\n",
      "Epoch 148 | Loss = -51.569667872260595\n",
      "Epoch 149 | Loss = -51.40675572788014\n",
      "Epoch 150 | Loss = -51.570881478926715\n",
      "Epoch 152 | Loss = -51.56563040789436\n",
      "Epoch 153 | Loss = -51.566860367270074\n",
      "Epoch 154 | Loss = -51.4892107739168\n",
      "Epoch 155 | Loss = -51.47801974240471\n",
      "Epoch 156 | Loss = -51.544128333821014\n",
      "Epoch 157 | Loss = -51.67581555422615\n",
      "Epoch 158 | Loss = -51.695891352260816\n",
      "Epoch 159 | Loss = -51.634783941156726\n",
      "Epoch 160 | Loss = -51.64918223549338\n",
      "Epoch 161 | Loss = -51.77998253878425\n",
      "Epoch 162 | Loss = -51.68206172830918\n",
      "Epoch 163 | Loss = -51.7231481776518\n",
      "Epoch 164 | Loss = -51.65635431514067\n",
      "Epoch 165 | Loss = -51.865088154287896\n",
      "Epoch 166 | Loss = -51.87361815396477\n",
      "Epoch 167 | Loss = -51.78582800135893\n",
      "Epoch 168 | Loss = -51.75848391476799\n",
      "Epoch 169 | Loss = -51.710974889643055\n",
      "Epoch 170 | Loss = -51.89660291110768\n",
      "Epoch 171 | Loss = -51.872685853172754\n",
      "Epoch 172 | Loss = -52.01965612523696\n",
      "Epoch 173 | Loss = -51.862886148340564\n",
      "Epoch 174 | Loss = -51.97689325669233\n",
      "Epoch 175 | Loss = -51.903853248147406\n",
      "Epoch 176 | Loss = -51.90392314686495\n",
      "Epoch 177 | Loss = -51.95034307592056\n",
      "Epoch 178 | Loss = -51.92978328817031\n",
      "Epoch 179 | Loss = -52.078759361715875\n",
      "Epoch 180 | Loss = -51.98620790593765\n",
      "Epoch 181 | Loss = -52.17354325687184\n",
      "Epoch 182 | Loss = -52.023881631739\n",
      "Epoch 183 | Loss = -52.11902250963099\n",
      "Epoch 184 | Loss = -52.2734122837291\n",
      "Epoch 185 | Loss = -52.03958146712359\n",
      "Epoch 186 | Loss = -52.07987288867726\n",
      "Epoch 187 | Loss = -52.07907499986536\n",
      "Epoch 188 | Loss = -52.08680887783275\n",
      "Epoch 189 | Loss = -52.284678010379565\n",
      "Epoch 190 | Loss = -52.308071220622345\n",
      "Epoch 191 | Loss = -52.21135711669922\n",
      "Epoch 192 | Loss = -52.2346952382256\n",
      "Epoch 193 | Loss = -52.21804652494543\n",
      "Epoch 194 | Loss = -52.22154920241412\n",
      "Epoch 195 | Loss = -52.322092056274414\n",
      "Epoch 196 | Loss = -52.36032859016867\n",
      "Epoch 197 | Loss = -52.34913910136503\n",
      "Epoch 198 | Loss = -52.28336278130026\n",
      "Epoch 199 | Loss = -52.35249962526209\n",
      "Epoch 200 | Loss = -52.43441116108614\n",
      "+-----------------------------------------------------------------------------+\n",
      "Training Time: 23908.212022066116 seconds.\n",
      "Get Embedding...\n",
      "(118, 128) (118,)\n",
      "SVC Accuracy: [0.7545454545454546, 0.703030303030303]\n",
      "+-----------------------------------------------------------------------------+\n",
      "Embedding Time: 13.52407693862915 seconds.\n",
      "Experient 14\n",
      "20211221-222851 :Log the record!\n",
      "+-----------------------------------------------------------------------------+\n",
      "Experient 15\n",
      "!!!\n",
      "89\n",
      "!!!\n",
      "[553, 217, 346, 850, 1139, 72, 280, 861, 916, 589, 50, 182, 307, 1067, 172, 583, 831, 791, 1014, 246, 18, 1013, 59, 1023, 984, 69, 755, 65, 71, 742, 1153, 256, 856, 266, 427, 956, 679, 826, 541, 147, 910, 0, 922, 683, 131, 103, 91, 1001, 166, 558, 796, 1078, 811, 852, 351, 1002, 383, 145, 197, 1072, 789, 17, 606, 9, 608, 1111, 664, 414, 154, 290, 954, 667, 959, 1097, 652, 339, 184, 368, 327, 506, 933, 370, 36, 768, 67, 1158, 93, 1123, 47, 624, 311, 883, 484, 702, 379, 316, 794, 822, 818, 55, 375, 1008, 605, 584, 920, 575, 986, 675, 82, 468, 1104, 202, 428, 566, 85, 550, 914, 1142, 545, 1145, 870, 1149, 332, 732, 185, 401, 359, 540, 714, 277, 785, 56, 45, 330, 1137, 564, 941, 703, 157, 1056, 1025, 325, 394, 360, 1077, 611, 1058, 1131, 830, 1081, 857, 1105, 364, 57, 233, 291, 1065, 926, 181, 231, 230, 808, 731, 678, 588, 929, 226, 537, 680, 38, 746, 595, 733, 244, 138, 1039, 441, 216, 762, 220, 284, 570, 1166, 1103, 117, 476, 261, 312, 232, 424, 149, 1110, 183, 651, 776, 522, 898, 381, 1100, 1088, 221, 1043, 596, 1084, 719, 669, 253, 715, 498, 35, 112, 495, 524, 548, 369, 735, 574, 580, 391, 167, 293, 942, 793, 273, 204, 1049, 474, 292, 973, 129, 1006, 402, 444, 975, 137, 1071, 647, 656, 1080, 1045, 333, 648, 555, 1127, 576, 486, 602, 989, 399, 3, 1030, 1016, 782, 153, 962, 421, 620, 1036, 264, 944, 848, 736, 717, 26, 1121, 978, 338, 398, 449, 46, 778, 877, 773, 633, 287, 465, 957, 828, 462, 800, 419, 902, 709, 953, 549, 997, 724, 78, 779, 1019, 2, 1050, 546, 1048, 195, 1061, 995, 1032, 10, 25, 450, 357, 1135, 254, 559, 612, 912, 965, 179, 94, 739, 470, 616, 298, 900, 224, 598, 301, 436, 1099, 234, 1060, 1053, 839, 815, 158, 804, 945, 89, 31, 210, 365, 993, 413, 225, 115, 618, 42, 567, 320, 1125, 644, 532, 337, 513, 168, 609, 243, 494, 186, 734, 1159, 193, 439, 305, 641, 628, 300, 521, 636, 948, 198, 994, 556, 96, 1130, 568, 22, 557, 1021, 132, 938, 707, 463, 723, 1152, 915, 711, 156, 631, 931, 457, 1066, 433, 666, 464, 459, 597, 816, 471, 725, 614, 1063, 397, 716, 684, 176, 227, 577, 1004, 801, 637, 34, 761, 302, 404, 1090, 980, 1028, 262, 161, 426, 751, 617, 1000, 39, 345, 504, 880, 907, 409, 173, 515, 249, 1069, 285, 1156, 730, 350, 260, 150, 281, 923, 361, 1062, 918, 1052, 904, 282, 1095, 219, 503, 738, 88, 668, 563, 710, 946, 207, 146, 60, 340, 760, 691, 106, 827, 265, 571, 15, 1132, 1015, 985, 825, 970, 758, 223, 406, 921, 1044, 772, 120, 247, 4, 542, 101, 859, 467, 1150, 12, 871, 268, 1012, 630, 1009, 336, 322, 621, 491, 1026, 187, 629, 432, 385, 992, 809, 1024, 756, 366, 62, 188, 53, 899, 950, 939, 1175, 988, 13, 105, 692, 659, 160, 960, 134, 875, 492, 107, 505, 757, 932, 653, 533, 175, 342, 314, 1165, 936, 1114, 252, 639, 8, 695, 838, 539, 824, 458, 408, 508, 248, 148, 352, 1162, 663, 911, 578, 318, 1116, 108, 1106, 235, 844, 54, 889, 645, 586, 1068, 239, 326, 763, 689, 104, 81, 323, 48, 1031, 908, 1041, 851, 1124, 452, 961, 122, 127, 1108, 388, 215, 155, 73, 887, 475, 77, 348, 485, 84, 1148, 650, 509, 75, 299, 331, 144, 241, 288, 727, 619, 775, 625, 206, 977, 455, 934, 697, 777, 865, 245, 835, 585, 133, 70, 655, 1161, 1092, 892, 190, 7, 32, 807, 110, 693, 1134, 1064, 295, 19, 872, 729, 1154, 289, 1176, 657, 142, 890, 445, 952, 275, 866, 587, 927, 603, 943, 211, 1118, 728, 593, 701, 477, 222, 196, 440, 205, 531, 981, 354, 478, 165, 242, 958, 1115, 1075, 1074, 1007, 24, 967, 913, 49, 744, 11, 554, 201, 321, 437, 517, 999, 814, 529, 681, 236, 180, 750, 780, 135, 868, 516, 879, 820, 315, 1128, 560, 218, 845, 297, 949, 841, 1059, 309, 1167, 1155, 665, 425, 1172, 461, 812, 79, 143, 622, 983, 937, 438, 353, 832, 6, 901, 795, 990, 759, 869, 1120, 276, 686, 810, 199, 371, 174, 677, 627, 116, 1133, 435, 519, 113, 496, 1029, 706, 255, 764, 130, 1005, 525, 500, 453, 367, 720, 171, 278, 483, 1122, 754, 698, 1051, 599, 1126, 74, 867, 726, 885, 250, 191, 1027, 229, 996, 310, 209, 884, 111, 456, 1082, 76, 358, 343, 66, 356, 213, 874, 1164, 362, 813, 1138, 5, 1055, 324, 888, 480, 849, 1033, 228, 267, 410, 786, 1035, 139, 873, 972, 740, 303, 951, 304, 1140, 543, 682, 382, 393, 896, 488, 718, 447, 1054, 417, 643, 43, 964, 14, 237, 189, 662, 29, 306, 1144, 119, 654, 86, 1107, 783, 511, 876, 1136, 696, 1146, 1086, 507, 341, 649, 771, 169, 274, 37, 376, 317, 279, 395, 840, 966, 251, 687, 878, 787, 159, 600, 1087, 905, 319, 1160, 378, 1094, 263, 52, 987, 344, 308, 363, 416, 396, 924, 769, 688, 33, 615, 460, 979, 906, 121, 895, 1171, 102, 579, 526, 882, 1168, 177, 523, 380, 1076, 864, 1085, 125, 674, 83, 781, 64, 124, 1047, 51, 20, 661, 767, 258, 151, 833, 638, 1109, 590, 635, 1169, 930, 372, 448, 982, 991, 846, 538, 374, 792, 446, 708, 528, 283, 819, 1089, 858, 1038, 806, 466, 493, 1174, 347, 114, 136, 128, 863, 935, 1093, 415, 269, 510, 1129, 834, 1, 389, 68, 749, 963, 552, 487, 430, 212, 971, 847, 745, 194, 162, 713, 296, 1011, 592, 582, 917, 594, 429, 853, 1073, 784, 520, 499, 469, 451, 1079, 572, 862, 753, 737, 27, 1117, 547, 947, 123, 894, 1112, 855, 384, 1034, 422, 490, 712, 843, 192, 1173, 203, 634, 514, 928, 694, 423, 141, 420, 329, 1017, 1119, 377, 1046, 527, 998, 349, 272, 454, 1057, 501, 178, 392, 405, 774, 472, 238, 1042, 534, 699, 482, 1018, 569, 842, 164, 660, 1113, 536, 817, 95, 240, 387, 854, 126, 561, 743, 431, 672, 802, 208, 766, 690, 390, 1101, 591, 604, 109, 909, 803, 28, 286, 765, 163, 601, 1163, 722, 90, 1177, 1037, 152, 1098, 805, 334, 294, 829, 700, 974, 21]\n",
      "Number of Graphs (dataset_train): 1060\n",
      "Number of Graphs (dataset_test): 118\n",
      "+-----------------------------------------------------------------------------+\n",
      "Learning Rate: 0.001\n",
      "Epochs: 200\n",
      "Batch Size: 64\n",
      "Hidden Dimension 512\n",
      "Number of Layer in Encoder: 2\n",
      "Opimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device: cuda\n",
      "Model:\n",
      "SimCLR(\n",
      "  (encoder): Encoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=89, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      ))\n",
      "      (1): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      ))\n",
      "    )\n",
      "    (bns): ModuleList(\n",
      "      (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Projection_MLP(\n",
      "    (layer1): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (unilayer): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Start Training...\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss = -21.33525538444519\n",
      "Epoch 2 | Loss = -32.27556402543012\n",
      "Epoch 3 | Loss = -39.037401507882514\n",
      "Epoch 4 | Loss = -42.74185250787174\n",
      "Epoch 5 | Loss = -45.00877994649551\n",
      "Epoch 6 | Loss = -46.633804826175464\n",
      "Epoch 7 | Loss = -47.79753951465382\n",
      "Epoch 8 | Loss = -48.87936751982745\n",
      "Epoch 9 | Loss = -49.60734499202055\n",
      "Epoch 10 | Loss = -50.308028417475086\n",
      "Epoch 11 | Loss = -50.98012197718901\n",
      "Epoch 12 | Loss = -51.26291353562299\n",
      "Epoch 13 | Loss = -51.542549049153045\n",
      "Epoch 14 | Loss = -51.934373406802905\n",
      "Epoch 15 | Loss = -52.3657826535842\n",
      "Epoch 16 | Loss = -52.5655398088343\n",
      "Epoch 17 | Loss = -53.00628308688893\n",
      "Epoch 18 | Loss = -53.266807808595544\n",
      "Epoch 19 | Loss = -53.62999077404247\n",
      "Epoch 20 | Loss = -53.773217734168554\n",
      "Epoch 21 | Loss = -53.94652380662806\n",
      "Epoch 22 | Loss = -54.26585968802957\n",
      "Epoch 23 | Loss = -54.366346190957465\n",
      "Epoch 24 | Loss = -54.57139693989473\n",
      "Epoch 25 | Loss = -54.66268449671128\n",
      "Epoch 26 | Loss = -54.79068026823156\n",
      "Epoch 27 | Loss = -54.88411235809326\n",
      "Epoch 28 | Loss = -54.99636574352489\n",
      "Epoch 29 | Loss = -55.025523269877716\n",
      "Epoch 30 | Loss = -55.21206586501177\n",
      "Epoch 31 | Loss = -55.31350893132827\n",
      "Epoch 32 | Loss = -55.45296764373779\n",
      "Epoch 33 | Loss = -55.549226732815015\n",
      "Epoch 34 | Loss = -55.73009558284984\n",
      "Epoch 35 | Loss = -55.72354628058041\n",
      "Epoch 36 | Loss = -55.77407132878023\n",
      "Epoch 37 | Loss = -55.894626533283905\n",
      "Epoch 38 | Loss = -55.92329476861393\n",
      "Epoch 39 | Loss = -56.094603173873004\n",
      "Epoch 40 | Loss = -56.11533375347362\n",
      "Epoch 41 | Loss = -56.14820629007676\n",
      "Epoch 42 | Loss = -56.30846009534948\n",
      "Epoch 43 | Loss = -56.32489684048821\n",
      "Epoch 44 | Loss = -56.43197811351103\n",
      "Epoch 45 | Loss = -56.47543040443869\n",
      "Epoch 46 | Loss = -56.51972285438986\n",
      "Epoch 47 | Loss = -56.578187830307904\n",
      "Epoch 48 | Loss = -56.658310273114374\n",
      "Epoch 49 | Loss = -56.64358542947208\n",
      "Epoch 50 | Loss = -56.71239123624914\n",
      "Epoch 51 | Loss = -56.77752750060137\n",
      "Epoch 52 | Loss = -56.842206954956055\n",
      "Epoch 53 | Loss = -56.83818732990938\n",
      "Epoch 54 | Loss = -56.870996447170484\n",
      "Epoch 55 | Loss = -56.756475897396314\n",
      "Epoch 56 | Loss = -56.68637435576495\n",
      "Epoch 57 | Loss = -56.78452359928804\n",
      "Epoch 58 | Loss = -56.878968351027545\n",
      "Epoch 59 | Loss = -56.998763028313135\n",
      "Epoch 60 | Loss = -57.09059639538036\n",
      "Epoch 61 | Loss = -57.12038163577809\n",
      "Epoch 62 | Loss = -57.24080492468441\n",
      "Epoch 63 | Loss = -57.25829707874971\n",
      "Epoch 64 | Loss = -57.291569373186896\n",
      "Epoch 65 | Loss = -57.30542076335234\n",
      "Epoch 66 | Loss = -57.29689062342924\n",
      "Epoch 67 | Loss = -57.32691254335291\n",
      "Epoch 68 | Loss = -57.37891850751989\n",
      "Epoch 69 | Loss = -57.42120703528909\n",
      "Epoch 70 | Loss = -57.41220827663646\n",
      "Epoch 71 | Loss = -57.41635302936329\n",
      "Epoch 72 | Loss = -57.483151099261114\n",
      "Epoch 73 | Loss = -57.49973804810468\n",
      "Epoch 74 | Loss = -57.44756140428431\n",
      "Epoch 75 | Loss = -57.478233758141016\n",
      "Epoch 76 | Loss = -57.610378097085395\n",
      "Epoch 77 | Loss = -57.58819507150089\n",
      "Epoch 78 | Loss = -57.648029804229736\n",
      "Epoch 79 | Loss = -57.67380425509285\n",
      "Epoch 80 | Loss = -57.69055402980131\n",
      "Epoch 81 | Loss = -57.72313995922313\n",
      "Epoch 82 | Loss = -57.71443869085873\n",
      "Epoch 83 | Loss = -57.72614997975967\n",
      "Epoch 84 | Loss = -57.76360163969152\n",
      "Epoch 85 | Loss = -57.758697509765625\n",
      "Epoch 86 | Loss = -57.79934139812694\n",
      "Epoch 87 | Loss = -57.80606886919807\n",
      "Epoch 88 | Loss = -57.823465796077954\n",
      "Epoch 89 | Loss = -57.872602827408734\n",
      "Epoch 90 | Loss = -57.87255402172313\n",
      "Epoch 91 | Loss = -57.928315471200385\n",
      "Epoch 92 | Loss = -57.911805377287024\n",
      "Epoch 93 | Loss = -57.901465864742505\n",
      "Epoch 94 | Loss = -57.972200477824494\n",
      "Epoch 95 | Loss = -57.90867443645702\n",
      "Epoch 96 | Loss = -57.77723390915815\n",
      "Epoch 97 | Loss = -57.73908152299769\n",
      "Epoch 98 | Loss = -57.8204779624939\n",
      "Epoch 99 | Loss = -57.95054418900434\n",
      "Epoch 100 | Loss = -58.00760266360115\n",
      "Epoch 101 | Loss = -58.03155405381147\n",
      "Epoch 102 | Loss = -58.079807085149426\n",
      "Epoch 103 | Loss = -58.03947042016422\n",
      "Epoch 104 | Loss = -57.973612168255976\n",
      "Epoch 105 | Loss = -57.86234852846931\n",
      "Epoch 106 | Loss = -58.02160857705509\n",
      "Epoch 107 | Loss = -58.079019714804254\n",
      "Epoch 108 | Loss = -58.12708677965052\n",
      "Epoch 109 | Loss = -58.16731321110445\n",
      "Epoch 110 | Loss = -58.21058391122257\n",
      "Epoch 111 | Loss = -58.23072338104248\n",
      "Epoch 112 | Loss = -58.27279219907873\n",
      "Epoch 113 | Loss = -58.28337178510778\n",
      "Epoch 114 | Loss = -58.28101668638342\n",
      "Epoch 115 | Loss = -58.325484303867114\n",
      "Epoch 116 | Loss = -58.311825163224164\n",
      "Epoch 117 | Loss = -58.329220799838794\n",
      "Epoch 118 | Loss = -58.33486986160278\n",
      "Epoch 119 | Loss = -58.34057381573845\n",
      "Epoch 120 | Loss = -58.35405043994679\n",
      "Epoch 121 | Loss = -58.37532265046064\n",
      "Epoch 122 | Loss = -58.32942999110502\n",
      "Epoch 123 | Loss = -58.35140548032873\n",
      "Epoch 124 | Loss = -58.380324111265296\n",
      "Epoch 125 | Loss = -58.377707453335034\n",
      "Epoch 126 | Loss = -58.380821115830365\n",
      "Epoch 127 | Loss = -58.39404779322007\n",
      "Epoch 128 | Loss = -58.369310182683606\n",
      "Epoch 129 | Loss = -58.39896182452931\n",
      "Epoch 130 | Loss = -58.42677029441385\n",
      "Epoch 131 | Loss = -58.451352063347315\n",
      "Epoch 132 | Loss = -58.47238361134249\n",
      "Epoch 133 | Loss = -58.464049227097455\n",
      "Epoch 134 | Loss = -58.472280979156494\n",
      "Epoch 135 | Loss = -58.46594342063455\n",
      "Epoch 136 | Loss = -58.460228471194995\n",
      "Epoch 137 | Loss = -58.46078897924984\n",
      "Epoch 138 | Loss = -58.49192840912763\n",
      "Epoch 139 | Loss = -58.52113530215095\n",
      "Epoch 140 | Loss = -58.565005134133735\n",
      "Epoch 141 | Loss = -58.555817632114184\n",
      "Epoch 142 | Loss = -58.54517947926241\n",
      "Epoch 143 | Loss = -58.51981325710521\n",
      "Epoch 144 | Loss = -58.300603922675634\n",
      "Epoch 145 | Loss = -58.27637495714075\n",
      "Epoch 146 | Loss = -58.3339952581069\n",
      "Epoch 147 | Loss = -58.44610298381132\n",
      "Epoch 148 | Loss = -58.50860458261826\n",
      "Epoch 149 | Loss = -58.553504691404456\n",
      "Epoch 150 | Loss = -58.525366474600396\n",
      "Epoch 151 | Loss = -58.4156241977916\n",
      "Epoch 152 | Loss = -58.328458729912256\n",
      "Epoch 153 | Loss = -58.50943363414091\n",
      "Epoch 154 | Loss = -58.54746779273538\n",
      "Epoch 155 | Loss = -58.58603637358721\n",
      "Epoch 156 | Loss = -58.61103217742022\n",
      "Epoch 157 | Loss = -58.63891590342802\n",
      "Epoch 158 | Loss = -58.65349354463465\n",
      "Epoch 159 | Loss = -58.676927847020764\n",
      "Epoch 160 | Loss = -58.68090699700748\n",
      "Epoch 161 | Loss = -58.69218237259809\n",
      "Epoch 162 | Loss = -58.70853856030632\n",
      "Epoch 163 | Loss = -58.70692174574908\n",
      "Epoch 164 | Loss = -58.71960564220653\n",
      "Epoch 165 | Loss = -58.71427493936875\n",
      "Epoch 166 | Loss = -58.72325745750876\n",
      "Epoch 167 | Loss = -58.71885162241318\n",
      "Epoch 168 | Loss = -58.699036457959345\n",
      "Epoch 169 | Loss = -58.699254400589886\n",
      "Epoch 170 | Loss = -58.719204650205725\n",
      "Epoch 171 | Loss = -58.727617993074304\n",
      "Epoch 172 | Loss = -58.74807024002075\n",
      "Epoch 173 | Loss = -58.76240747115191\n",
      "Epoch 174 | Loss = -58.7487800542046\n",
      "Epoch 175 | Loss = -58.77446786095114\n",
      "Epoch 176 | Loss = -58.763165698331946\n",
      "Epoch 177 | Loss = -58.771126354441925\n",
      "Epoch 178 | Loss = -58.7689795774572\n",
      "Epoch 179 | Loss = -58.629986594705024\n",
      "Epoch 180 | Loss = -58.546301645391125\n",
      "Epoch 181 | Loss = -58.69674362855799\n",
      "Epoch 182 | Loss = -58.74324052474078\n",
      "Epoch 183 | Loss = -58.7561450846055\n",
      "Epoch 184 | Loss = -58.794425571666046\n",
      "Epoch 185 | Loss = -58.818878173828125\n",
      "Epoch 186 | Loss = -58.81869857451495\n",
      "Epoch 187 | Loss = -58.8344131638022\n",
      "Epoch 188 | Loss = -58.84338000241448\n",
      "Epoch 189 | Loss = -58.86111478244557\n",
      "Epoch 190 | Loss = -58.88160136166741\n",
      "Epoch 191 | Loss = -58.857974753660315\n",
      "Epoch 192 | Loss = -58.84626868191887\n",
      "Epoch 193 | Loss = -58.87494334052591\n",
      "Epoch 194 | Loss = -58.86846800411449\n",
      "Epoch 195 | Loss = -58.82534456253052\n",
      "Epoch 196 | Loss = -58.6066432279699\n",
      "Epoch 197 | Loss = -58.57648627898272\n",
      "Epoch 198 | Loss = -58.62431983386769\n",
      "Epoch 199 | Loss = -58.69009085262523\n",
      "Epoch 200 | Loss = -58.79419256659115\n",
      "+-----------------------------------------------------------------------------+\n",
      "Training Time: 23917.008272647858 seconds.\n",
      "Get Embedding...\n",
      "(118, 1024) (118,)\n",
      "SVC Accuracy: [0.7393939393939394, 0.7037878787878789]\n",
      "+-----------------------------------------------------------------------------+\n",
      "Embedding Time: 14.617863655090332 seconds.\n",
      "Experient 15\n",
      "20211222-050743 :Log the record!\n",
      "+-----------------------------------------------------------------------------+\n",
      "Experient 16\n",
      "!!!\n",
      "89\n",
      "!!!\n",
      "[553, 217, 346, 850, 1139, 72, 280, 861, 916, 589, 50, 182, 307, 1067, 172, 583, 831, 791, 1014, 246, 18, 1013, 59, 1023, 984, 69, 755, 65, 71, 742, 1153, 256, 856, 266, 427, 956, 679, 826, 541, 147, 910, 0, 922, 683, 131, 103, 91, 1001, 166, 558, 796, 1078, 811, 852, 351, 1002, 383, 145, 197, 1072, 789, 17, 606, 9, 608, 1111, 664, 414, 154, 290, 954, 667, 959, 1097, 652, 339, 184, 368, 327, 506, 933, 370, 36, 768, 67, 1158, 93, 1123, 47, 624, 311, 883, 484, 702, 379, 316, 794, 822, 818, 55, 375, 1008, 605, 584, 920, 575, 986, 675, 82, 468, 1104, 202, 428, 566, 85, 550, 914, 1142, 545, 1145, 870, 1149, 332, 732, 185, 401, 359, 540, 714, 277, 785, 56, 45, 330, 1137, 564, 941, 703, 157, 1056, 1025, 325, 394, 360, 1077, 611, 1058, 1131, 830, 1081, 857, 1105, 364, 57, 233, 291, 1065, 926, 181, 231, 230, 808, 731, 678, 588, 929, 226, 537, 680, 38, 746, 595, 733, 244, 138, 1039, 441, 216, 762, 220, 284, 570, 1166, 1103, 117, 476, 261, 312, 232, 424, 149, 1110, 183, 651, 776, 522, 898, 381, 1100, 1088, 221, 1043, 596, 1084, 719, 669, 253, 715, 498, 35, 112, 495, 524, 548, 369, 735, 574, 580, 391, 167, 293, 942, 793, 273, 204, 1049, 474, 292, 973, 129, 1006, 402, 444, 975, 137, 1071, 647, 656, 1080, 1045, 333, 648, 555, 1127, 576, 486, 602, 989, 399, 3, 1030, 1016, 782, 153, 962, 421, 620, 1036, 264, 944, 848, 736, 717, 26, 1121, 978, 338, 398, 449, 46, 778, 877, 773, 633, 287, 465, 957, 828, 462, 800, 419, 902, 709, 953, 549, 997, 724, 78, 779, 1019, 2, 1050, 546, 1048, 195, 1061, 995, 1032, 10, 25, 450, 357, 1135, 254, 559, 612, 912, 965, 179, 94, 739, 470, 616, 298, 900, 224, 598, 301, 436, 1099, 234, 1060, 1053, 839, 815, 158, 804, 945, 89, 31, 210, 365, 993, 413, 225, 115, 618, 42, 567, 320, 1125, 644, 532, 337, 513, 168, 609, 243, 494, 186, 734, 1159, 193, 439, 305, 641, 628, 300, 521, 636, 948, 198, 994, 556, 96, 1130, 568, 22, 557, 1021, 132, 938, 707, 463, 723, 1152, 915, 711, 156, 631, 931, 457, 1066, 433, 666, 464, 459, 597, 816, 471, 725, 614, 1063, 397, 716, 684, 176, 227, 577, 1004, 801, 637, 34, 761, 302, 404, 1090, 980, 1028, 262, 161, 426, 751, 617, 1000, 39, 345, 504, 880, 907, 409, 173, 515, 249, 1069, 285, 1156, 730, 350, 260, 150, 281, 923, 361, 1062, 918, 1052, 904, 282, 1095, 219, 503, 738, 88, 668, 563, 710, 946, 207, 146, 60, 340, 760, 691, 106, 827, 265, 571, 15, 1132, 1015, 985, 825, 970, 758, 223, 406, 921, 1044, 772, 120, 247, 4, 542, 101, 859, 467, 1150, 12, 871, 268, 1012, 630, 1009, 336, 322, 621, 491, 1026, 187, 629, 432, 385, 992, 809, 1024, 756, 366, 62, 188, 53, 899, 950, 939, 1175, 988, 13, 105, 692, 659, 160, 960, 134, 875, 492, 107, 505, 757, 932, 653, 533, 175, 342, 314, 1165, 936, 1114, 252, 639, 8, 695, 838, 539, 824, 458, 408, 508, 248, 148, 352, 1162, 663, 911, 578, 318, 1116, 108, 1106, 235, 844, 54, 889, 645, 586, 1068, 239, 326, 763, 689, 104, 81, 323, 48, 1031, 908, 1041, 851, 1124, 452, 961, 122, 127, 1108, 388, 215, 155, 73, 887, 475, 77, 348, 485, 84, 1148, 650, 509, 75, 299, 331, 144, 241, 288, 727, 619, 775, 625, 206, 977, 455, 934, 697, 777, 865, 245, 835, 585, 133, 70, 655, 1161, 1092, 892, 190, 7, 32, 807, 110, 693, 1134, 1064, 295, 19, 872, 729, 1154, 289, 1176, 657, 142, 890, 445, 952, 275, 866, 587, 927, 603, 943, 211, 1118, 728, 593, 701, 477, 222, 196, 440, 205, 531, 981, 354, 478, 165, 242, 958, 1115, 1075, 1074, 1007, 24, 967, 913, 49, 744, 11, 554, 201, 321, 437, 517, 999, 814, 529, 681, 236, 180, 750, 780, 135, 868, 516, 879, 820, 315, 1128, 560, 218, 845, 297, 949, 841, 1059, 309, 1167, 1155, 665, 425, 1172, 461, 812, 79, 143, 622, 983, 937, 438, 353, 832, 6, 901, 795, 990, 759, 869, 1120, 276, 686, 810, 199, 371, 174, 677, 627, 116, 1133, 435, 519, 113, 496, 1029, 706, 255, 764, 130, 1005, 525, 500, 453, 367, 720, 171, 278, 483, 1122, 754, 698, 1051, 599, 1126, 74, 867, 726, 885, 250, 191, 1027, 229, 996, 310, 209, 884, 111, 456, 1082, 76, 358, 343, 66, 356, 213, 874, 1164, 362, 813, 1138, 5, 1055, 324, 888, 480, 849, 1033, 228, 267, 410, 786, 1035, 139, 873, 972, 740, 303, 951, 304, 1140, 543, 682, 382, 393, 896, 488, 718, 447, 1054, 417, 643, 43, 964, 14, 237, 189, 662, 29, 306, 1144, 119, 654, 86, 1107, 783, 511, 876, 1136, 696, 1146, 1086, 507, 341, 649, 771, 169, 274, 37, 376, 317, 279, 395, 840, 966, 251, 687, 878, 787, 159, 600, 1087, 905, 319, 1160, 378, 1094, 263, 52, 987, 344, 308, 363, 416, 396, 924, 769, 688, 33, 615, 460, 979, 906, 121, 895, 1171, 102, 579, 526, 882, 1168, 177, 523, 380, 1076, 864, 1085, 125, 674, 83, 781, 64, 124, 1047, 51, 20, 661, 767, 258, 151, 833, 638, 1109, 590, 635, 1169, 930, 372, 448, 982, 991, 846, 538, 374, 792, 446, 708, 528, 283, 819, 1089, 858, 1038, 806, 466, 493, 1174, 347, 114, 136, 128, 863, 935, 1093, 415, 269, 510, 1129, 834, 1, 389, 68, 749, 963, 552, 487, 430, 212, 971, 847, 745, 194, 162, 713, 296, 1011, 592, 582, 917, 594, 429, 853, 1073, 784, 520, 499, 469, 451, 1079, 572, 862, 753, 737, 27, 1117, 547, 947, 123, 894, 1112, 855, 384, 1034, 422, 490, 712, 843, 192, 1173, 203, 634, 514, 928, 694, 423, 141, 420, 329, 1017, 1119, 377, 1046, 527, 998, 349, 272, 454, 1057, 501, 178, 392, 405, 774, 472, 238, 1042, 534, 699, 482, 1018, 569, 842, 164, 660, 1113, 536, 817, 95, 240, 387, 854, 126, 561, 743, 431, 672, 802, 208, 766, 690, 390, 1101, 591, 604, 109, 909, 803, 28, 286, 765, 163, 601, 1163, 722, 90, 1177, 1037, 152, 1098, 805, 334, 294, 829, 700, 974, 21]\n",
      "Number of Graphs (dataset_train): 1060\n",
      "Number of Graphs (dataset_test): 118\n",
      "+-----------------------------------------------------------------------------+\n",
      "Learning Rate: 0.001\n",
      "Epochs: 200\n",
      "Batch Size: 64\n",
      "Hidden Dimension 64\n",
      "Number of Layer in Encoder: 3\n",
      "Opimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device: cuda\n",
      "Model:\n",
      "SimCLR(\n",
      "  (encoder): Encoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=89, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      ))\n",
      "      (1): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      ))\n",
      "      (2): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      ))\n",
      "    )\n",
      "    (bns): ModuleList(\n",
      "      (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Projection_MLP(\n",
      "    (layer1): Sequential(\n",
      "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (1): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (1): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (1): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (unilayer): Sequential(\n",
      "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=192, out_features=192, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Start Training...\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss = 6.0500087667914\n",
      "Epoch 2 | Loss = -16.394766555112952\n",
      "Epoch 4 | Loss = -29.043578624725342\n",
      "Epoch 5 | Loss = -32.212020986220416\n",
      "Epoch 6 | Loss = -34.04614137200748\n",
      "Epoch 7 | Loss = -35.72611256206737\n",
      "Epoch 8 | Loss = -36.910525910994586\n",
      "Epoch 9 | Loss = -38.060208180371454\n",
      "Epoch 10 | Loss = -39.09279377320234\n",
      "Epoch 11 | Loss = -39.76475791370167\n",
      "Epoch 12 | Loss = -40.59104035882389\n",
      "Epoch 13 | Loss = -41.12687408222872\n",
      "Epoch 14 | Loss = -41.967237388386444\n",
      "Epoch 15 | Loss = -42.39585164014031\n",
      "Epoch 16 | Loss = -42.870537757873535\n",
      "Epoch 17 | Loss = -43.41822612986845\n",
      "Epoch 18 | Loss = -43.76014880573048\n",
      "Epoch 19 | Loss = -44.05988757750567\n",
      "Epoch 20 | Loss = -44.18705474629122\n",
      "Epoch 21 | Loss = -44.66196859584135\n",
      "Epoch 22 | Loss = -45.41771409090828\n",
      "Epoch 23 | Loss = -45.60722208023071\n",
      "Epoch 24 | Loss = -45.74646879644955\n",
      "Epoch 25 | Loss = -45.97409969217637\n",
      "Epoch 26 | Loss = -46.19031561122221\n",
      "Epoch 27 | Loss = -46.30814381206737\n",
      "Epoch 28 | Loss = -46.47629176869112\n",
      "Epoch 29 | Loss = -46.75912857055664\n",
      "Epoch 30 | Loss = -46.91404191185446\n",
      "Epoch 31 | Loss = -47.06680861641379\n",
      "Epoch 32 | Loss = -47.261358962339514\n",
      "Epoch 33 | Loss = -47.556547557606415\n",
      "Epoch 34 | Loss = -47.81851726419785\n",
      "Epoch 35 | Loss = -47.801104461445526\n",
      "Epoch 36 | Loss = -47.867021308225745\n",
      "Epoch 37 | Loss = -48.170420927159924\n",
      "Epoch 38 | Loss = -48.07869498869952\n",
      "Epoch 39 | Loss = -48.154233006870044\n",
      "Epoch 40 | Loss = -48.54905507143806\n",
      "Epoch 41 | Loss = -48.52919469160192\n",
      "Epoch 42 | Loss = -48.71717708251055\n",
      "Epoch 43 | Loss = -49.06141945895027\n",
      "Epoch 44 | Loss = -49.06852413626278\n",
      "Epoch 45 | Loss = -48.95271750057445\n",
      "Epoch 46 | Loss = -49.22176321815042\n",
      "Epoch 47 | Loss = -49.14104500938864\n",
      "Epoch 48 | Loss = -49.284592011395624\n",
      "Epoch 49 | Loss = -49.47978190814747\n",
      "Epoch 50 | Loss = -49.520962462705725\n",
      "Epoch 51 | Loss = -49.50104256237255\n",
      "Epoch 52 | Loss = -49.67547559738159\n",
      "Epoch 53 | Loss = -49.89766836166382\n",
      "Epoch 54 | Loss = -49.81917291529038\n",
      "Epoch 55 | Loss = -49.96329467436846\n",
      "Epoch 56 | Loss = -50.10822585049797\n",
      "Epoch 57 | Loss = -50.05807753170238\n",
      "Epoch 58 | Loss = -50.273730502409094\n",
      "Epoch 59 | Loss = -50.37560423682718\n",
      "Epoch 61 | Loss = -50.42589431650498\n",
      "Epoch 62 | Loss = -50.527457517736096\n",
      "Epoch 63 | Loss = -50.62392145044663\n",
      "Epoch 64 | Loss = -50.57920576544369\n",
      "Epoch 65 | Loss = -50.5332651979783\n",
      "Epoch 66 | Loss = -50.82441021414364\n",
      "Epoch 67 | Loss = -50.711235495174634\n",
      "Epoch 68 | Loss = -50.91730782564949\n",
      "Epoch 69 | Loss = -50.99556213266709\n",
      "Epoch 70 | Loss = -51.13183820948881\n",
      "Epoch 71 | Loss = -51.19537328271305\n",
      "Epoch 72 | Loss = -51.179461759679455\n",
      "Epoch 73 | Loss = -51.2623449493857\n",
      "Epoch 74 | Loss = -51.220475028542914\n",
      "Epoch 75 | Loss = -51.334750175476074\n",
      "Epoch 76 | Loss = -51.40201588237987\n",
      "Epoch 77 | Loss = -51.46470844044405\n",
      "Epoch 78 | Loss = -51.537183565251965\n",
      "Epoch 79 | Loss = -51.482686772066\n",
      "Epoch 80 | Loss = -51.51409398808199\n",
      "Epoch 81 | Loss = -51.661533299614405\n",
      "Epoch 82 | Loss = -51.6747672698077\n",
      "Epoch 83 | Loss = -51.693129287046546\n",
      "Epoch 84 | Loss = -51.71439644869636\n",
      "Epoch 85 | Loss = -51.72556484446806\n",
      "Epoch 86 | Loss = -51.83896078782923\n",
      "Epoch 87 | Loss = -51.909367224749396\n",
      "Epoch 88 | Loss = -51.842646823209876\n",
      "Epoch 89 | Loss = -51.887878333821014\n",
      "Epoch 90 | Loss = -51.976041092592126\n",
      "Epoch 91 | Loss = -51.96753246643964\n",
      "Epoch 92 | Loss = -52.20849225100349\n",
      "Epoch 93 | Loss = -52.09796768076279\n",
      "Epoch 94 | Loss = -52.27197728437536\n",
      "Epoch 95 | Loss = -52.22273153417251\n",
      "Epoch 96 | Loss = -52.11915832407334\n",
      "Epoch 97 | Loss = -52.2516837681041\n",
      "Epoch 98 | Loss = -52.244175406063306\n",
      "Epoch 99 | Loss = -52.28378304313211\n",
      "Epoch 100 | Loss = -52.417359211865595\n",
      "Epoch 101 | Loss = -52.31365509594188\n",
      "Epoch 102 | Loss = -52.3227645369137\n",
      "Epoch 103 | Loss = -52.36895275115967\n",
      "Epoch 104 | Loss = -52.34701782114365\n",
      "Epoch 105 | Loss = -52.52703515221091\n",
      "Epoch 106 | Loss = -52.70337500291712\n",
      "Epoch 107 | Loss = -52.48942694944494\n",
      "Epoch 108 | Loss = -52.61345484677483\n",
      "Epoch 109 | Loss = -52.5766824834487\n",
      "Epoch 110 | Loss = -52.55582579444437\n",
      "Epoch 111 | Loss = -52.76542893578024\n",
      "Epoch 112 | Loss = -52.69021727057064\n",
      "Epoch 113 | Loss = -52.67617856754976\n",
      "Epoch 114 | Loss = -52.68845336577471\n",
      "Epoch 115 | Loss = -52.74628420437084\n",
      "Epoch 116 | Loss = -52.843223712023565\n",
      "Epoch 117 | Loss = -52.899490132051355\n",
      "Epoch 118 | Loss = -52.816804941962744\n",
      "Epoch 119 | Loss = -52.910368526683136\n",
      "Epoch 120 | Loss = -52.878144292270434\n",
      "Epoch 121 | Loss = -52.912448798908905\n",
      "Epoch 122 | Loss = -52.963681894190174\n",
      "Epoch 123 | Loss = -53.09919677061193\n",
      "Epoch 124 | Loss = -53.149208293241614\n",
      "Epoch 125 | Loss = -53.154102213242474\n",
      "Epoch 126 | Loss = -53.169711898354926\n",
      "Epoch 127 | Loss = -53.15080250010771\n",
      "Epoch 128 | Loss = -53.11701278125538\n",
      "Epoch 129 | Loss = -53.22511515897863\n",
      "Epoch 130 | Loss = -53.309800652896655\n",
      "Epoch 131 | Loss = -53.298961218665625\n",
      "Epoch 132 | Loss = -53.35592171725105\n",
      "Epoch 133 | Loss = -53.31139222313376\n",
      "Epoch 134 | Loss = -53.4546119185055\n",
      "Epoch 135 | Loss = -53.46233165965361\n",
      "Epoch 136 | Loss = -53.36173725128174\n",
      "Epoch 137 | Loss = -53.33922630197861\n",
      "Epoch 138 | Loss = -53.409151918747845\n",
      "Epoch 139 | Loss = -53.389358408310834\n",
      "Epoch 140 | Loss = -53.439560048720416\n",
      "Epoch 141 | Loss = -53.39769621456371\n",
      "Epoch 142 | Loss = -53.4642533975489\n",
      "Epoch 143 | Loss = -53.5538119428298\n",
      "Epoch 144 | Loss = -53.51993423349717\n",
      "Epoch 145 | Loss = -53.59017503962797\n",
      "Epoch 146 | Loss = -53.564449310302734\n",
      "Epoch 147 | Loss = -53.460586940540985\n",
      "Epoch 148 | Loss = -53.67653187583475\n",
      "Epoch 149 | Loss = -53.700147797079644\n",
      "Epoch 150 | Loss = -53.660812854766846\n",
      "Epoch 151 | Loss = -53.56008266000187\n",
      "Epoch 152 | Loss = -53.67966755698709\n",
      "Epoch 153 | Loss = -53.736495214350086\n",
      "Epoch 154 | Loss = -53.701949428109565\n",
      "Epoch 155 | Loss = -53.80218494639677\n",
      "Epoch 156 | Loss = -53.78480992597692\n",
      "Epoch 157 | Loss = -53.73020242242252\n",
      "Epoch 158 | Loss = -53.800160323872284\n",
      "Epoch 159 | Loss = -53.85669147267061\n",
      "Epoch 160 | Loss = -53.8068780338063\n",
      "Epoch 161 | Loss = -53.8634129412034\n",
      "Epoch 162 | Loss = -53.93064857931698\n",
      "Epoch 163 | Loss = -54.00338405721328\n",
      "Epoch 164 | Loss = -53.854779327616974\n",
      "Epoch 165 | Loss = -54.01771598703721\n",
      "Epoch 166 | Loss = -53.93052586387186\n",
      "Epoch 167 | Loss = -54.01249975316665\n",
      "Epoch 168 | Loss = -53.952306523042566\n",
      "Epoch 169 | Loss = -53.98347184237312\n",
      "Epoch 170 | Loss = -54.007592257331396\n",
      "Epoch 171 | Loss = -53.8165059931138\n",
      "Epoch 172 | Loss = -54.009780154508704\n",
      "Epoch 173 | Loss = -54.00061722362743\n",
      "Epoch 174 | Loss = -54.14822755140417\n",
      "Epoch 175 | Loss = -54.084175530601954\n",
      "Epoch 176 | Loss = -54.09722760144402\n",
      "Epoch 177 | Loss = -54.09806027131922\n",
      "Epoch 178 | Loss = -54.20920739454382\n",
      "Epoch 179 | Loss = -54.16775120005888\n",
      "Epoch 180 | Loss = -54.094872530768896\n",
      "Epoch 181 | Loss = -54.10858008440803\n",
      "Epoch 182 | Loss = -54.18153849770041\n",
      "Epoch 183 | Loss = -54.08847806032966\n",
      "Epoch 184 | Loss = -54.159372946795294\n",
      "Epoch 185 | Loss = -54.14946034375359\n",
      "Epoch 186 | Loss = -54.18425416946411\n",
      "Epoch 187 | Loss = -54.20356899149277\n",
      "Epoch 188 | Loss = -54.33279385286219\n",
      "Epoch 189 | Loss = -54.28562458823709\n",
      "Epoch 190 | Loss = -54.267653212827796\n",
      "Epoch 191 | Loss = -54.36308288574219\n",
      "Epoch 192 | Loss = -54.323130271014044\n",
      "Epoch 193 | Loss = -54.32233689813053\n",
      "Epoch 194 | Loss = -54.2935214323156\n",
      "Epoch 195 | Loss = -54.37165532392614\n",
      "Epoch 196 | Loss = -54.30293400147382\n",
      "Epoch 197 | Loss = -54.33700480180628\n",
      "Epoch 198 | Loss = -54.40769122628605\n",
      "Epoch 199 | Loss = -54.38130044937134\n",
      "Epoch 200 | Loss = -54.351817439584174\n",
      "+-----------------------------------------------------------------------------+\n",
      "Training Time: 23706.89117383957 seconds.\n",
      "Get Embedding...\n",
      "(118, 192) (118,)\n",
      "SVC Accuracy: [0.7371212121212121, 0.6431818181818182]\n",
      "+-----------------------------------------------------------------------------+\n",
      "Embedding Time: 13.495413541793823 seconds.\n",
      "Experient 16\n",
      "20211222-114304 :Log the record!\n",
      "+-----------------------------------------------------------------------------+\n",
      "Experient 17\n",
      "!!!\n",
      "89\n",
      "!!!\n",
      "[553, 217, 346, 850, 1139, 72, 280, 861, 916, 589, 50, 182, 307, 1067, 172, 583, 831, 791, 1014, 246, 18, 1013, 59, 1023, 984, 69, 755, 65, 71, 742, 1153, 256, 856, 266, 427, 956, 679, 826, 541, 147, 910, 0, 922, 683, 131, 103, 91, 1001, 166, 558, 796, 1078, 811, 852, 351, 1002, 383, 145, 197, 1072, 789, 17, 606, 9, 608, 1111, 664, 414, 154, 290, 954, 667, 959, 1097, 652, 339, 184, 368, 327, 506, 933, 370, 36, 768, 67, 1158, 93, 1123, 47, 624, 311, 883, 484, 702, 379, 316, 794, 822, 818, 55, 375, 1008, 605, 584, 920, 575, 986, 675, 82, 468, 1104, 202, 428, 566, 85, 550, 914, 1142, 545, 1145, 870, 1149, 332, 732, 185, 401, 359, 540, 714, 277, 785, 56, 45, 330, 1137, 564, 941, 703, 157, 1056, 1025, 325, 394, 360, 1077, 611, 1058, 1131, 830, 1081, 857, 1105, 364, 57, 233, 291, 1065, 926, 181, 231, 230, 808, 731, 678, 588, 929, 226, 537, 680, 38, 746, 595, 733, 244, 138, 1039, 441, 216, 762, 220, 284, 570, 1166, 1103, 117, 476, 261, 312, 232, 424, 149, 1110, 183, 651, 776, 522, 898, 381, 1100, 1088, 221, 1043, 596, 1084, 719, 669, 253, 715, 498, 35, 112, 495, 524, 548, 369, 735, 574, 580, 391, 167, 293, 942, 793, 273, 204, 1049, 474, 292, 973, 129, 1006, 402, 444, 975, 137, 1071, 647, 656, 1080, 1045, 333, 648, 555, 1127, 576, 486, 602, 989, 399, 3, 1030, 1016, 782, 153, 962, 421, 620, 1036, 264, 944, 848, 736, 717, 26, 1121, 978, 338, 398, 449, 46, 778, 877, 773, 633, 287, 465, 957, 828, 462, 800, 419, 902, 709, 953, 549, 997, 724, 78, 779, 1019, 2, 1050, 546, 1048, 195, 1061, 995, 1032, 10, 25, 450, 357, 1135, 254, 559, 612, 912, 965, 179, 94, 739, 470, 616, 298, 900, 224, 598, 301, 436, 1099, 234, 1060, 1053, 839, 815, 158, 804, 945, 89, 31, 210, 365, 993, 413, 225, 115, 618, 42, 567, 320, 1125, 644, 532, 337, 513, 168, 609, 243, 494, 186, 734, 1159, 193, 439, 305, 641, 628, 300, 521, 636, 948, 198, 994, 556, 96, 1130, 568, 22, 557, 1021, 132, 938, 707, 463, 723, 1152, 915, 711, 156, 631, 931, 457, 1066, 433, 666, 464, 459, 597, 816, 471, 725, 614, 1063, 397, 716, 684, 176, 227, 577, 1004, 801, 637, 34, 761, 302, 404, 1090, 980, 1028, 262, 161, 426, 751, 617, 1000, 39, 345, 504, 880, 907, 409, 173, 515, 249, 1069, 285, 1156, 730, 350, 260, 150, 281, 923, 361, 1062, 918, 1052, 904, 282, 1095, 219, 503, 738, 88, 668, 563, 710, 946, 207, 146, 60, 340, 760, 691, 106, 827, 265, 571, 15, 1132, 1015, 985, 825, 970, 758, 223, 406, 921, 1044, 772, 120, 247, 4, 542, 101, 859, 467, 1150, 12, 871, 268, 1012, 630, 1009, 336, 322, 621, 491, 1026, 187, 629, 432, 385, 992, 809, 1024, 756, 366, 62, 188, 53, 899, 950, 939, 1175, 988, 13, 105, 692, 659, 160, 960, 134, 875, 492, 107, 505, 757, 932, 653, 533, 175, 342, 314, 1165, 936, 1114, 252, 639, 8, 695, 838, 539, 824, 458, 408, 508, 248, 148, 352, 1162, 663, 911, 578, 318, 1116, 108, 1106, 235, 844, 54, 889, 645, 586, 1068, 239, 326, 763, 689, 104, 81, 323, 48, 1031, 908, 1041, 851, 1124, 452, 961, 122, 127, 1108, 388, 215, 155, 73, 887, 475, 77, 348, 485, 84, 1148, 650, 509, 75, 299, 331, 144, 241, 288, 727, 619, 775, 625, 206, 977, 455, 934, 697, 777, 865, 245, 835, 585, 133, 70, 655, 1161, 1092, 892, 190, 7, 32, 807, 110, 693, 1134, 1064, 295, 19, 872, 729, 1154, 289, 1176, 657, 142, 890, 445, 952, 275, 866, 587, 927, 603, 943, 211, 1118, 728, 593, 701, 477, 222, 196, 440, 205, 531, 981, 354, 478, 165, 242, 958, 1115, 1075, 1074, 1007, 24, 967, 913, 49, 744, 11, 554, 201, 321, 437, 517, 999, 814, 529, 681, 236, 180, 750, 780, 135, 868, 516, 879, 820, 315, 1128, 560, 218, 845, 297, 949, 841, 1059, 309, 1167, 1155, 665, 425, 1172, 461, 812, 79, 143, 622, 983, 937, 438, 353, 832, 6, 901, 795, 990, 759, 869, 1120, 276, 686, 810, 199, 371, 174, 677, 627, 116, 1133, 435, 519, 113, 496, 1029, 706, 255, 764, 130, 1005, 525, 500, 453, 367, 720, 171, 278, 483, 1122, 754, 698, 1051, 599, 1126, 74, 867, 726, 885, 250, 191, 1027, 229, 996, 310, 209, 884, 111, 456, 1082, 76, 358, 343, 66, 356, 213, 874, 1164, 362, 813, 1138, 5, 1055, 324, 888, 480, 849, 1033, 228, 267, 410, 786, 1035, 139, 873, 972, 740, 303, 951, 304, 1140, 543, 682, 382, 393, 896, 488, 718, 447, 1054, 417, 643, 43, 964, 14, 237, 189, 662, 29, 306, 1144, 119, 654, 86, 1107, 783, 511, 876, 1136, 696, 1146, 1086, 507, 341, 649, 771, 169, 274, 37, 376, 317, 279, 395, 840, 966, 251, 687, 878, 787, 159, 600, 1087, 905, 319, 1160, 378, 1094, 263, 52, 987, 344, 308, 363, 416, 396, 924, 769, 688, 33, 615, 460, 979, 906, 121, 895, 1171, 102, 579, 526, 882, 1168, 177, 523, 380, 1076, 864, 1085, 125, 674, 83, 781, 64, 124, 1047, 51, 20, 661, 767, 258, 151, 833, 638, 1109, 590, 635, 1169, 930, 372, 448, 982, 991, 846, 538, 374, 792, 446, 708, 528, 283, 819, 1089, 858, 1038, 806, 466, 493, 1174, 347, 114, 136, 128, 863, 935, 1093, 415, 269, 510, 1129, 834, 1, 389, 68, 749, 963, 552, 487, 430, 212, 971, 847, 745, 194, 162, 713, 296, 1011, 592, 582, 917, 594, 429, 853, 1073, 784, 520, 499, 469, 451, 1079, 572, 862, 753, 737, 27, 1117, 547, 947, 123, 894, 1112, 855, 384, 1034, 422, 490, 712, 843, 192, 1173, 203, 634, 514, 928, 694, 423, 141, 420, 329, 1017, 1119, 377, 1046, 527, 998, 349, 272, 454, 1057, 501, 178, 392, 405, 774, 472, 238, 1042, 534, 699, 482, 1018, 569, 842, 164, 660, 1113, 536, 817, 95, 240, 387, 854, 126, 561, 743, 431, 672, 802, 208, 766, 690, 390, 1101, 591, 604, 109, 909, 803, 28, 286, 765, 163, 601, 1163, 722, 90, 1177, 1037, 152, 1098, 805, 334, 294, 829, 700, 974, 21]\n",
      "Number of Graphs (dataset_train): 1060\n",
      "Number of Graphs (dataset_test): 118\n",
      "+-----------------------------------------------------------------------------+\n",
      "Learning Rate: 0.001\n",
      "Epochs: 200\n",
      "Batch Size: 64\n",
      "Hidden Dimension 512\n",
      "Number of Layer in Encoder: 3\n",
      "Opimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device: cuda\n",
      "Model:\n",
      "SimCLR(\n",
      "  (encoder): Encoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=89, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      ))\n",
      "      (1): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      ))\n",
      "      (2): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      ))\n",
      "    )\n",
      "    (bns): ModuleList(\n",
      "      (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Projection_MLP(\n",
      "    (layer1): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (unilayer): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Start Training...\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss = -14.95763258373036\n",
      "Epoch 2 | Loss = -27.746570699355182\n",
      "Epoch 3 | Loss = -34.981600649216595\n",
      "Epoch 4 | Loss = -39.89585657680736\n",
      "Epoch 5 | Loss = -42.99675677804386\n",
      "Epoch 6 | Loss = -45.239060542162726\n",
      "Epoch 7 | Loss = -46.903824609868664\n",
      "Epoch 8 | Loss = -48.329532875734216\n",
      "Epoch 9 | Loss = -49.52106271070593\n",
      "Epoch 10 | Loss = -50.36617912965662\n",
      "Epoch 11 | Loss = -50.961020076976105\n",
      "Epoch 12 | Loss = -51.569300679599536\n",
      "Epoch 13 | Loss = -52.074072669534125\n",
      "Epoch 14 | Loss = -52.59847158544204\n",
      "Epoch 15 | Loss = -52.75240696177763\n",
      "Epoch 16 | Loss = -53.29231318305521\n",
      "Epoch 17 | Loss = -53.51298940882963\n",
      "Epoch 18 | Loss = -53.73595832375919\n",
      "Epoch 19 | Loss = -54.01335923811968\n",
      "Epoch 20 | Loss = -54.156758925494024\n",
      "Epoch 21 | Loss = -54.300149721257824\n",
      "Epoch 22 | Loss = -54.59567005494062\n",
      "Epoch 23 | Loss = -54.84680461883545\n",
      "Epoch 24 | Loss = -55.059564253863165\n",
      "Epoch 25 | Loss = -55.27432722203872\n",
      "Epoch 26 | Loss = -55.39358293308931\n",
      "Epoch 27 | Loss = -55.51027797250187\n",
      "Epoch 28 | Loss = -55.668531473945166\n",
      "Epoch 29 | Loss = -55.77037738351261\n",
      "Epoch 30 | Loss = -55.945162156048944\n",
      "Epoch 31 | Loss = -55.97891628040987\n",
      "Epoch 32 | Loss = -56.02964819178862\n",
      "Epoch 33 | Loss = -56.04499530792236\n",
      "Epoch 34 | Loss = -56.07688407336964\n",
      "Epoch 35 | Loss = -56.05748384139117\n",
      "Epoch 36 | Loss = -56.19168985591215\n",
      "Epoch 37 | Loss = -56.31966231851017\n",
      "Epoch 38 | Loss = -56.48034931631649\n",
      "Epoch 39 | Loss = -56.65618618796854\n",
      "Epoch 40 | Loss = -56.772667800678924\n",
      "Epoch 41 | Loss = -56.840439431807575\n",
      "Epoch 42 | Loss = -56.81999635696411\n",
      "Epoch 43 | Loss = -56.89590288611019\n",
      "Epoch 44 | Loss = -56.950066818910486\n",
      "Epoch 45 | Loss = -57.050504712497485\n",
      "Epoch 46 | Loss = -57.103537250967584\n",
      "Epoch 47 | Loss = -57.19357456880457\n",
      "Epoch 48 | Loss = -57.1986237413743\n",
      "Epoch 49 | Loss = -57.213228085461786\n",
      "Epoch 50 | Loss = -57.308320858899286\n",
      "Epoch 51 | Loss = -57.33870545555563\n",
      "Epoch 52 | Loss = -57.42398354586433\n",
      "Epoch 53 | Loss = -57.43746465795181\n",
      "Epoch 54 | Loss = -57.49040258631987\n",
      "Epoch 55 | Loss = -57.42747449874878\n",
      "Epoch 56 | Loss = -57.406650711508355\n",
      "Epoch 57 | Loss = -57.32604217529297\n",
      "Epoch 58 | Loss = -57.410872992347265\n",
      "Epoch 59 | Loss = -57.5426926612854\n",
      "Epoch 60 | Loss = -57.625187901889575\n",
      "Epoch 61 | Loss = -57.65117510627298\n",
      "Epoch 62 | Loss = -57.69530610477223\n",
      "Epoch 63 | Loss = -57.753279349383185\n",
      "Epoch 64 | Loss = -57.77762292413151\n",
      "Epoch 65 | Loss = -57.7884539716384\n",
      "Epoch 66 | Loss = -57.84894096150118\n",
      "Epoch 67 | Loss = -57.89941291248097\n",
      "Epoch 68 | Loss = -57.90410350350773\n",
      "Epoch 69 | Loss = -57.91709728801952\n",
      "Epoch 70 | Loss = -57.954755923327276\n",
      "Epoch 71 | Loss = -58.032378505258\n",
      "Epoch 72 | Loss = -58.02706825031954\n",
      "Epoch 73 | Loss = -58.07849541832419\n",
      "Epoch 74 | Loss = -58.06142910789041\n",
      "Epoch 75 | Loss = -58.09548765070298\n",
      "Epoch 76 | Loss = -58.110494894139904\n",
      "Epoch 77 | Loss = -58.15741115457871\n",
      "Epoch 78 | Loss = -58.17802030899946\n",
      "Epoch 79 | Loss = -58.20630522335277\n",
      "Epoch 80 | Loss = -58.10688756493961\n",
      "Epoch 81 | Loss = -58.102329198051905\n",
      "Epoch 82 | Loss = -58.17258498247932\n",
      "Epoch 83 | Loss = -58.15789679919972\n",
      "Epoch 84 | Loss = -58.243457429549274\n",
      "Epoch 85 | Loss = -58.25460739696727\n",
      "Epoch 86 | Loss = -58.25941313014311\n",
      "Epoch 87 | Loss = -58.2654065805323\n",
      "Epoch 88 | Loss = -58.301582364475024\n",
      "Epoch 89 | Loss = -58.35050142512602\n",
      "Epoch 90 | Loss = -58.35379059174482\n",
      "Epoch 91 | Loss = -58.34858122993918\n",
      "Epoch 92 | Loss = -58.38695573806763\n",
      "Epoch 93 | Loss = -58.44180755054249\n",
      "Epoch 94 | Loss = -58.430225793053125\n",
      "Epoch 95 | Loss = -58.43409849615658\n",
      "Epoch 96 | Loss = -58.209615370806524\n",
      "Epoch 97 | Loss = -58.118811326868396\n",
      "Epoch 98 | Loss = -58.18279639412375\n",
      "Epoch 99 | Loss = -58.28057746326222\n",
      "Epoch 100 | Loss = -58.381631318260645\n",
      "Epoch 101 | Loss = -58.43098357144524\n",
      "Epoch 102 | Loss = -58.49116552577299\n",
      "Epoch 103 | Loss = -58.52245417763205\n",
      "Epoch 104 | Loss = -58.550035925472486\n",
      "Epoch 105 | Loss = -58.57962490530575\n",
      "Epoch 106 | Loss = -58.60689205281875\n",
      "Epoch 107 | Loss = -58.60674280278823\n",
      "Epoch 108 | Loss = -58.63771719091079\n",
      "Epoch 109 | Loss = -58.64794397354126\n",
      "Epoch 110 | Loss = -58.673757553100586\n",
      "Epoch 111 | Loss = -58.652272925657385\n",
      "Epoch 112 | Loss = -58.68648458929623\n",
      "Epoch 113 | Loss = -58.711011073168585\n",
      "Epoch 114 | Loss = -58.728490100187415\n",
      "Epoch 115 | Loss = -58.730180740356445\n",
      "Epoch 116 | Loss = -58.731791243833655\n",
      "Epoch 117 | Loss = -58.74356856065638\n",
      "Epoch 118 | Loss = -58.74887690824621\n",
      "Epoch 119 | Loss = -58.763357779558966\n",
      "Epoch 120 | Loss = -58.77769349603092\n",
      "Epoch 121 | Loss = -58.777752567740045\n",
      "Epoch 122 | Loss = -58.781547995174634\n",
      "Epoch 123 | Loss = -58.782757366404816\n",
      "Epoch 124 | Loss = -58.794735095080206\n",
      "Epoch 125 | Loss = -58.797394668354706\n",
      "Epoch 126 | Loss = -58.82507411171408\n",
      "Epoch 127 | Loss = -58.83560132980347\n",
      "Epoch 128 | Loss = -58.83862742255716\n",
      "Epoch 129 | Loss = -58.843219336341406\n",
      "Epoch 130 | Loss = -58.864485151627484\n",
      "Epoch 131 | Loss = -58.868916876175824\n",
      "Epoch 132 | Loss = -58.865903209237494\n",
      "Epoch 133 | Loss = -58.87839923185461\n",
      "Epoch 134 | Loss = -58.88882465923534\n",
      "Epoch 135 | Loss = -58.907441784353814\n",
      "Epoch 136 | Loss = -58.904509123633886\n",
      "Epoch 137 | Loss = -58.934352594263416\n",
      "Epoch 138 | Loss = -58.872382051804486\n",
      "Epoch 139 | Loss = -58.624026130227485\n",
      "Epoch 140 | Loss = -58.53164613948149\n",
      "Epoch 141 | Loss = -58.66259518791647\n",
      "Epoch 142 | Loss = -58.708767133600574\n",
      "Epoch 143 | Loss = -58.75342085782219\n",
      "Epoch 144 | Loss = -58.79867545296164\n",
      "Epoch 145 | Loss = -58.803502671858844\n",
      "Epoch 146 | Loss = -58.849301366245044\n",
      "Epoch 147 | Loss = -58.89149309607113\n",
      "Epoch 148 | Loss = -58.89868624070112\n",
      "Epoch 149 | Loss = -58.832909668193146\n",
      "Epoch 150 | Loss = -58.78643776388729\n",
      "Epoch 151 | Loss = -58.844548982732434\n",
      "Epoch 152 | Loss = -58.89053900101606\n",
      "Epoch 153 | Loss = -58.922660883735205\n",
      "Epoch 154 | Loss = -58.93871046515072\n",
      "Epoch 155 | Loss = -58.97475489448099\n",
      "Epoch 156 | Loss = -58.984364593730255\n",
      "Epoch 157 | Loss = -58.99589106615852\n",
      "Epoch 158 | Loss = -59.007612480836755\n",
      "Epoch 159 | Loss = -59.021097267375275\n",
      "Epoch 160 | Loss = -59.02548186919268\n",
      "Epoch 161 | Loss = -59.03165836895214\n",
      "Epoch 162 | Loss = -59.03653792773976\n",
      "Epoch 163 | Loss = -59.04049475052778\n",
      "Epoch 164 | Loss = -59.049500830033246\n",
      "Epoch 165 | Loss = -59.053122380200556\n",
      "Epoch 166 | Loss = -59.06082624547622\n",
      "Epoch 167 | Loss = -59.0293285706464\n",
      "Epoch 168 | Loss = -58.98437211092781\n",
      "Epoch 169 | Loss = -58.958290324491614\n",
      "Epoch 170 | Loss = -59.01623495887308\n",
      "Epoch 171 | Loss = -59.03843231762157\n",
      "Epoch 172 | Loss = -59.06347019532148\n",
      "Epoch 173 | Loss = -59.072885401108685\n",
      "Epoch 174 | Loss = -59.03119269539328\n",
      "Epoch 175 | Loss = -58.844335808473474\n",
      "Epoch 176 | Loss = -58.95850818297442\n",
      "Epoch 177 | Loss = -59.01141166687012\n",
      "Epoch 178 | Loss = -59.06634619656731\n",
      "Epoch 179 | Loss = -59.07967732934391\n",
      "Epoch 180 | Loss = -59.06074456607594\n",
      "Epoch 181 | Loss = -58.95653132831349\n",
      "Epoch 182 | Loss = -58.804671203388885\n",
      "Epoch 183 | Loss = -58.90352846594418\n",
      "Epoch 184 | Loss = -58.952960210687976\n",
      "Epoch 185 | Loss = -59.01666408426621\n",
      "Epoch 186 | Loss = -59.05773524677052\n",
      "Epoch 187 | Loss = -59.07793544320499\n",
      "Epoch 188 | Loss = -59.12502810534309\n",
      "Epoch 189 | Loss = -59.14423331092386\n",
      "Epoch 190 | Loss = -59.16090766121359\n",
      "Epoch 191 | Loss = -59.164851160610425\n",
      "Epoch 192 | Loss = -59.19390285716337\n",
      "Epoch 193 | Loss = -59.19871321846457\n",
      "Epoch 194 | Loss = -59.19214080361759\n",
      "Epoch 195 | Loss = -59.21115389992209\n",
      "Epoch 196 | Loss = -59.22216970780317\n",
      "Epoch 197 | Loss = -59.23543767368092\n",
      "Epoch 198 | Loss = -59.236676440519446\n",
      "Epoch 199 | Loss = -59.230943623711084\n",
      "Epoch 200 | Loss = -59.225212181315705\n",
      "+-----------------------------------------------------------------------------+\n",
      "Training Time: 23786.819997787476 seconds.\n",
      "Get Embedding...\n",
      "(118, 1536) (118,)\n",
      "SVC Accuracy: [0.7356060606060607, 0.8553030303030302]\n",
      "+-----------------------------------------------------------------------------+\n",
      "Embedding Time: 15.142622947692871 seconds.\n",
      "Experient 17\n",
      "20211222-181947 :Log the record!\n",
      "+-----------------------------------------------------------------------------+\n",
      "Experient 18\n",
      "!!!\n",
      "89\n",
      "!!!\n",
      "[553, 217, 346, 850, 1139, 72, 280, 861, 916, 589, 50, 182, 307, 1067, 172, 583, 831, 791, 1014, 246, 18, 1013, 59, 1023, 984, 69, 755, 65, 71, 742, 1153, 256, 856, 266, 427, 956, 679, 826, 541, 147, 910, 0, 922, 683, 131, 103, 91, 1001, 166, 558, 796, 1078, 811, 852, 351, 1002, 383, 145, 197, 1072, 789, 17, 606, 9, 608, 1111, 664, 414, 154, 290, 954, 667, 959, 1097, 652, 339, 184, 368, 327, 506, 933, 370, 36, 768, 67, 1158, 93, 1123, 47, 624, 311, 883, 484, 702, 379, 316, 794, 822, 818, 55, 375, 1008, 605, 584, 920, 575, 986, 675, 82, 468, 1104, 202, 428, 566, 85, 550, 914, 1142, 545, 1145, 870, 1149, 332, 732, 185, 401, 359, 540, 714, 277, 785, 56, 45, 330, 1137, 564, 941, 703, 157, 1056, 1025, 325, 394, 360, 1077, 611, 1058, 1131, 830, 1081, 857, 1105, 364, 57, 233, 291, 1065, 926, 181, 231, 230, 808, 731, 678, 588, 929, 226, 537, 680, 38, 746, 595, 733, 244, 138, 1039, 441, 216, 762, 220, 284, 570, 1166, 1103, 117, 476, 261, 312, 232, 424, 149, 1110, 183, 651, 776, 522, 898, 381, 1100, 1088, 221, 1043, 596, 1084, 719, 669, 253, 715, 498, 35, 112, 495, 524, 548, 369, 735, 574, 580, 391, 167, 293, 942, 793, 273, 204, 1049, 474, 292, 973, 129, 1006, 402, 444, 975, 137, 1071, 647, 656, 1080, 1045, 333, 648, 555, 1127, 576, 486, 602, 989, 399, 3, 1030, 1016, 782, 153, 962, 421, 620, 1036, 264, 944, 848, 736, 717, 26, 1121, 978, 338, 398, 449, 46, 778, 877, 773, 633, 287, 465, 957, 828, 462, 800, 419, 902, 709, 953, 549, 997, 724, 78, 779, 1019, 2, 1050, 546, 1048, 195, 1061, 995, 1032, 10, 25, 450, 357, 1135, 254, 559, 612, 912, 965, 179, 94, 739, 470, 616, 298, 900, 224, 598, 301, 436, 1099, 234, 1060, 1053, 839, 815, 158, 804, 945, 89, 31, 210, 365, 993, 413, 225, 115, 618, 42, 567, 320, 1125, 644, 532, 337, 513, 168, 609, 243, 494, 186, 734, 1159, 193, 439, 305, 641, 628, 300, 521, 636, 948, 198, 994, 556, 96, 1130, 568, 22, 557, 1021, 132, 938, 707, 463, 723, 1152, 915, 711, 156, 631, 931, 457, 1066, 433, 666, 464, 459, 597, 816, 471, 725, 614, 1063, 397, 716, 684, 176, 227, 577, 1004, 801, 637, 34, 761, 302, 404, 1090, 980, 1028, 262, 161, 426, 751, 617, 1000, 39, 345, 504, 880, 907, 409, 173, 515, 249, 1069, 285, 1156, 730, 350, 260, 150, 281, 923, 361, 1062, 918, 1052, 904, 282, 1095, 219, 503, 738, 88, 668, 563, 710, 946, 207, 146, 60, 340, 760, 691, 106, 827, 265, 571, 15, 1132, 1015, 985, 825, 970, 758, 223, 406, 921, 1044, 772, 120, 247, 4, 542, 101, 859, 467, 1150, 12, 871, 268, 1012, 630, 1009, 336, 322, 621, 491, 1026, 187, 629, 432, 385, 992, 809, 1024, 756, 366, 62, 188, 53, 899, 950, 939, 1175, 988, 13, 105, 692, 659, 160, 960, 134, 875, 492, 107, 505, 757, 932, 653, 533, 175, 342, 314, 1165, 936, 1114, 252, 639, 8, 695, 838, 539, 824, 458, 408, 508, 248, 148, 352, 1162, 663, 911, 578, 318, 1116, 108, 1106, 235, 844, 54, 889, 645, 586, 1068, 239, 326, 763, 689, 104, 81, 323, 48, 1031, 908, 1041, 851, 1124, 452, 961, 122, 127, 1108, 388, 215, 155, 73, 887, 475, 77, 348, 485, 84, 1148, 650, 509, 75, 299, 331, 144, 241, 288, 727, 619, 775, 625, 206, 977, 455, 934, 697, 777, 865, 245, 835, 585, 133, 70, 655, 1161, 1092, 892, 190, 7, 32, 807, 110, 693, 1134, 1064, 295, 19, 872, 729, 1154, 289, 1176, 657, 142, 890, 445, 952, 275, 866, 587, 927, 603, 943, 211, 1118, 728, 593, 701, 477, 222, 196, 440, 205, 531, 981, 354, 478, 165, 242, 958, 1115, 1075, 1074, 1007, 24, 967, 913, 49, 744, 11, 554, 201, 321, 437, 517, 999, 814, 529, 681, 236, 180, 750, 780, 135, 868, 516, 879, 820, 315, 1128, 560, 218, 845, 297, 949, 841, 1059, 309, 1167, 1155, 665, 425, 1172, 461, 812, 79, 143, 622, 983, 937, 438, 353, 832, 6, 901, 795, 990, 759, 869, 1120, 276, 686, 810, 199, 371, 174, 677, 627, 116, 1133, 435, 519, 113, 496, 1029, 706, 255, 764, 130, 1005, 525, 500, 453, 367, 720, 171, 278, 483, 1122, 754, 698, 1051, 599, 1126, 74, 867, 726, 885, 250, 191, 1027, 229, 996, 310, 209, 884, 111, 456, 1082, 76, 358, 343, 66, 356, 213, 874, 1164, 362, 813, 1138, 5, 1055, 324, 888, 480, 849, 1033, 228, 267, 410, 786, 1035, 139, 873, 972, 740, 303, 951, 304, 1140, 543, 682, 382, 393, 896, 488, 718, 447, 1054, 417, 643, 43, 964, 14, 237, 189, 662, 29, 306, 1144, 119, 654, 86, 1107, 783, 511, 876, 1136, 696, 1146, 1086, 507, 341, 649, 771, 169, 274, 37, 376, 317, 279, 395, 840, 966, 251, 687, 878, 787, 159, 600, 1087, 905, 319, 1160, 378, 1094, 263, 52, 987, 344, 308, 363, 416, 396, 924, 769, 688, 33, 615, 460, 979, 906, 121, 895, 1171, 102, 579, 526, 882, 1168, 177, 523, 380, 1076, 864, 1085, 125, 674, 83, 781, 64, 124, 1047, 51, 20, 661, 767, 258, 151, 833, 638, 1109, 590, 635, 1169, 930, 372, 448, 982, 991, 846, 538, 374, 792, 446, 708, 528, 283, 819, 1089, 858, 1038, 806, 466, 493, 1174, 347, 114, 136, 128, 863, 935, 1093, 415, 269, 510, 1129, 834, 1, 389, 68, 749, 963, 552, 487, 430, 212, 971, 847, 745, 194, 162, 713, 296, 1011, 592, 582, 917, 594, 429, 853, 1073, 784, 520, 499, 469, 451, 1079, 572, 862, 753, 737, 27, 1117, 547, 947, 123, 894, 1112, 855, 384, 1034, 422, 490, 712, 843, 192, 1173, 203, 634, 514, 928, 694, 423, 141, 420, 329, 1017, 1119, 377, 1046, 527, 998, 349, 272, 454, 1057, 501, 178, 392, 405, 774, 472, 238, 1042, 534, 699, 482, 1018, 569, 842, 164, 660, 1113, 536, 817, 95, 240, 387, 854, 126, 561, 743, 431, 672, 802, 208, 766, 690, 390, 1101, 591, 604, 109, 909, 803, 28, 286, 765, 163, 601, 1163, 722, 90, 1177, 1037, 152, 1098, 805, 334, 294, 829, 700, 974, 21]\n",
      "Number of Graphs (dataset_train): 1060\n",
      "Number of Graphs (dataset_test): 118\n",
      "+-----------------------------------------------------------------------------+\n",
      "Learning Rate: 0.001\n",
      "Epochs: 200\n",
      "Batch Size: 128\n",
      "Hidden Dimension 64\n",
      "Number of Layer in Encoder: 1\n",
      "Opimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device: cuda\n",
      "Model:\n",
      "SimCLR(\n",
      "  (encoder): Encoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=89, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      ))\n",
      "    )\n",
      "    (bns): ModuleList(\n",
      "      (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Projection_MLP(\n",
      "    (layer1): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (unilayer): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Start Training...\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss = 106.65320573912726\n",
      "Epoch 2 | Loss = 73.62726261880663\n",
      "Epoch 3 | Loss = 59.20301336712308\n",
      "Epoch 4 | Loss = 50.27505683898926\n",
      "Epoch 5 | Loss = 44.99513472451104\n",
      "Epoch 6 | Loss = 40.680235650804306\n",
      "Epoch 7 | Loss = 37.22411563661363\n",
      "Epoch 8 | Loss = 33.90539582570394\n",
      "Epoch 9 | Loss = 32.24701441658868\n",
      "Epoch 10 | Loss = 30.022226969401043\n",
      "Epoch 11 | Loss = 28.050679153866238\n",
      "Epoch 12 | Loss = 26.41856066385905\n",
      "Epoch 13 | Loss = 25.438933001624214\n",
      "Epoch 14 | Loss = 24.3162546687656\n",
      "Epoch 15 | Loss = 22.692049397362602\n",
      "Epoch 16 | Loss = 21.932762569851345\n",
      "Epoch 17 | Loss = 21.01245535744561\n",
      "Epoch 18 | Loss = 20.305296474032932\n",
      "Epoch 19 | Loss = 18.983005311754013\n",
      "Epoch 20 | Loss = 18.38005214267307\n",
      "Epoch 21 | Loss = 18.32732327779134\n",
      "Epoch 22 | Loss = 17.638273504045273\n",
      "Epoch 23 | Loss = 16.816340075598823\n",
      "Epoch 24 | Loss = 16.131436771816677\n",
      "Epoch 25 | Loss = 16.205933782789444\n",
      "Epoch 26 | Loss = 15.631032996707493\n",
      "Epoch 27 | Loss = 14.964894506666395\n",
      "Epoch 28 | Loss = 14.959367911020914\n",
      "Epoch 29 | Loss = 14.289630148145887\n",
      "Epoch 30 | Loss = 14.029854986402723\n",
      "Epoch 31 | Loss = 14.318834039900038\n",
      "Epoch 32 | Loss = 13.13559103012085\n",
      "Epoch 33 | Loss = 13.172276920742458\n",
      "Epoch 34 | Loss = 12.489835633171928\n",
      "Epoch 35 | Loss = 12.678903420766195\n",
      "Epoch 36 | Loss = 12.603240066104465\n",
      "Epoch 37 | Loss = 12.040099514855278\n",
      "Epoch 38 | Loss = 11.575408140818277\n",
      "Epoch 39 | Loss = 11.954046779208714\n",
      "Epoch 40 | Loss = 10.893695248497856\n",
      "Epoch 41 | Loss = 11.403172228071424\n",
      "Epoch 42 | Loss = 10.928942680358887\n",
      "Epoch 43 | Loss = 10.871669504377577\n",
      "Epoch 44 | Loss = 10.503938780890572\n",
      "Epoch 45 | Loss = 10.55650356080797\n",
      "Epoch 46 | Loss = 10.367767228020561\n",
      "Epoch 47 | Loss = 10.015500121646458\n",
      "Epoch 48 | Loss = 10.185108767615425\n",
      "Epoch 49 | Loss = 9.836780177222359\n",
      "Epoch 50 | Loss = 9.967043134901258\n",
      "Epoch 51 | Loss = 9.613008393181694\n",
      "Epoch 52 | Loss = 9.18686972724067\n",
      "Epoch 53 | Loss = 9.569128460354275\n",
      "Epoch 54 | Loss = 9.044723828633627\n",
      "Epoch 55 | Loss = 9.041554027133518\n",
      "Epoch 56 | Loss = 9.238834646013048\n",
      "Epoch 57 | Loss = 8.467239644792345\n",
      "Epoch 58 | Loss = 8.907021416558159\n",
      "Epoch 59 | Loss = 8.653641435835096\n",
      "Epoch 60 | Loss = 8.97532648510403\n",
      "Epoch 61 | Loss = 8.789064248402914\n",
      "Epoch 62 | Loss = 8.168316205342611\n",
      "Epoch 63 | Loss = 8.368004269070095\n",
      "Epoch 64 | Loss = 8.188575903574625\n",
      "Epoch 65 | Loss = 8.172201209598118\n",
      "Epoch 66 | Loss = 8.040857844882542\n",
      "Epoch 67 | Loss = 7.903550624847412\n",
      "Epoch 68 | Loss = 8.368071556091309\n",
      "Epoch 69 | Loss = 7.656549506717258\n",
      "Epoch 70 | Loss = 7.817077689700657\n",
      "Epoch 71 | Loss = 7.661552323235406\n",
      "Epoch 72 | Loss = 7.441441165076362\n",
      "Epoch 73 | Loss = 7.498000621795654\n",
      "Epoch 74 | Loss = 7.135806295606825\n",
      "Epoch 75 | Loss = 7.413388358222114\n",
      "Epoch 76 | Loss = 7.251316812303331\n",
      "Epoch 77 | Loss = 7.4538558853997126\n",
      "Epoch 78 | Loss = 7.3716930813259545\n",
      "Epoch 79 | Loss = 6.943341361151801\n",
      "Epoch 80 | Loss = 6.7662392722235785\n",
      "Epoch 81 | Loss = 6.619502173529731\n",
      "Epoch 82 | Loss = 6.970665613810222\n",
      "Epoch 83 | Loss = 7.077361636691624\n",
      "Epoch 84 | Loss = 6.825445440080431\n",
      "Epoch 85 | Loss = 6.730199654897054\n",
      "Epoch 86 | Loss = 6.878423796759711\n",
      "Epoch 87 | Loss = 6.848883681827122\n",
      "Epoch 88 | Loss = 6.543008751339382\n",
      "Epoch 89 | Loss = 6.493762969970703\n",
      "Epoch 90 | Loss = 6.8921230634053545\n",
      "Epoch 91 | Loss = 6.317072179582384\n",
      "Epoch 92 | Loss = 6.27665917078654\n",
      "Epoch 93 | Loss = 5.981834464603001\n",
      "Epoch 94 | Loss = 5.83260620964898\n",
      "Epoch 95 | Loss = 6.125753614637587\n",
      "Epoch 96 | Loss = 6.393633630540636\n",
      "Epoch 97 | Loss = 5.8795755704243975\n",
      "Epoch 98 | Loss = 6.0505068567064075\n",
      "Epoch 99 | Loss = 5.9026528464423285\n",
      "Epoch 100 | Loss = 5.5061864323086205\n",
      "Epoch 101 | Loss = 5.880766179826525\n",
      "Epoch 102 | Loss = 5.7905078993903265\n",
      "Epoch 103 | Loss = 5.542940669589573\n",
      "Epoch 104 | Loss = 5.246187633938259\n",
      "Epoch 105 | Loss = 5.093572669559055\n",
      "Epoch 106 | Loss = 5.782573752933079\n",
      "Epoch 107 | Loss = 5.470217810736762\n",
      "Epoch 108 | Loss = 5.329645580715603\n",
      "Epoch 109 | Loss = 5.6548608673943415\n",
      "Epoch 110 | Loss = 5.43877649307251\n",
      "Epoch 111 | Loss = 5.281032668219672\n",
      "Epoch 112 | Loss = 4.999422921074761\n",
      "Epoch 113 | Loss = 5.1775221824646\n",
      "Epoch 114 | Loss = 5.031640582614475\n",
      "Epoch 115 | Loss = 4.9717698097229\n",
      "Epoch 116 | Loss = 5.085377322302924\n",
      "Epoch 117 | Loss = 5.349936114417182\n",
      "Epoch 118 | Loss = 5.023130310906304\n",
      "Epoch 119 | Loss = 4.8680436346266\n",
      "Epoch 120 | Loss = 5.076990127563477\n",
      "Epoch 121 | Loss = 5.0184071858723955\n",
      "Epoch 122 | Loss = 4.999279657999675\n",
      "Epoch 123 | Loss = 4.605409410264757\n",
      "Epoch 124 | Loss = 4.802795622083876\n",
      "Epoch 125 | Loss = 4.6973215738932295\n",
      "Epoch 126 | Loss = 4.6644630961947975\n",
      "Epoch 127 | Loss = 4.761821640862359\n",
      "Epoch 128 | Loss = 4.595441235436334\n",
      "Epoch 129 | Loss = 4.534167448679606\n",
      "Epoch 130 | Loss = 4.576973809136285\n",
      "Epoch 131 | Loss = 4.692378520965576\n",
      "Epoch 132 | Loss = 4.539657963646783\n",
      "Epoch 133 | Loss = 4.534537739223904\n",
      "Epoch 134 | Loss = 4.365951961941189\n",
      "Epoch 135 | Loss = 3.964307361178928\n",
      "Epoch 136 | Loss = 4.426679346296522\n",
      "Epoch 137 | Loss = 4.077943113115099\n",
      "Epoch 138 | Loss = 4.28354729546441\n",
      "Epoch 139 | Loss = 4.141925017038981\n",
      "Epoch 140 | Loss = 4.356841193305121\n",
      "Epoch 141 | Loss = 4.430116335550944\n",
      "Epoch 142 | Loss = 4.183640268113878\n",
      "Epoch 143 | Loss = 4.220003763834636\n",
      "Epoch 144 | Loss = 3.853968302408854\n",
      "Epoch 145 | Loss = 3.7925961282518177\n",
      "Epoch 146 | Loss = 4.150857342614068\n",
      "Epoch 147 | Loss = 4.030977884928386\n",
      "Epoch 148 | Loss = 4.231484678056505\n",
      "Epoch 149 | Loss = 3.8179868592156305\n",
      "Epoch 150 | Loss = 3.742084503173828\n",
      "Epoch 151 | Loss = 3.7665672302246094\n",
      "Epoch 152 | Loss = 3.5150080256991916\n",
      "Epoch 153 | Loss = 4.0118823581271705\n",
      "Epoch 154 | Loss = 3.9570548269483776\n",
      "Epoch 155 | Loss = 3.630847613016764\n",
      "Epoch 156 | Loss = 3.484437041812473\n",
      "Epoch 157 | Loss = 3.7212591701083713\n",
      "Epoch 158 | Loss = 3.5440953572591147\n",
      "Epoch 159 | Loss = 3.9529717763264975\n",
      "Epoch 160 | Loss = 3.605413807762994\n",
      "Epoch 161 | Loss = 3.7866241137186685\n",
      "Epoch 162 | Loss = 3.624429225921631\n",
      "Epoch 163 | Loss = 3.6688515345255532\n",
      "Epoch 164 | Loss = 3.5037050247192383\n",
      "Epoch 165 | Loss = 3.4941278563605414\n",
      "Epoch 166 | Loss = 3.624899387359619\n",
      "Epoch 167 | Loss = 3.0838113360934787\n",
      "Epoch 168 | Loss = 3.441465907626682\n",
      "Epoch 169 | Loss = 3.3946031994289823\n",
      "Epoch 170 | Loss = 3.189288510216607\n",
      "Epoch 171 | Loss = 3.067818535698785\n",
      "Epoch 172 | Loss = 3.3724614249335394\n",
      "Epoch 173 | Loss = 3.072050783369276\n",
      "Epoch 174 | Loss = 3.1830202208624945\n",
      "Epoch 175 | Loss = 3.378976662953695\n",
      "Epoch 176 | Loss = 3.306754324171278\n",
      "Epoch 177 | Loss = 2.9710381825764975\n",
      "Epoch 178 | Loss = 2.8651286231146917\n",
      "Epoch 179 | Loss = 2.9017899301317005\n",
      "Epoch 180 | Loss = 2.891408920288086\n",
      "Epoch 181 | Loss = 2.7595206366644964\n",
      "Epoch 182 | Loss = 2.9454908900790744\n",
      "Epoch 183 | Loss = 2.9641910658942328\n",
      "Epoch 184 | Loss = 2.7007978757222495\n",
      "Epoch 185 | Loss = 2.90297269821167\n",
      "Epoch 186 | Loss = 2.665918403201633\n",
      "Epoch 187 | Loss = 2.7151136928134494\n",
      "Epoch 188 | Loss = 2.8255775769551597\n",
      "Epoch 189 | Loss = 2.558596558041043\n",
      "Epoch 190 | Loss = 2.733354091644287\n",
      "Epoch 191 | Loss = 2.6056688096788196\n",
      "Epoch 192 | Loss = 2.5814016660054526\n",
      "Epoch 193 | Loss = 2.9168355729844837\n",
      "Epoch 194 | Loss = 3.12631713019477\n",
      "Epoch 195 | Loss = 2.8126544422573514\n",
      "Epoch 196 | Loss = 2.5866439077589245\n",
      "Epoch 197 | Loss = 2.799298816257053\n",
      "Epoch 198 | Loss = 2.5334372520446777\n",
      "Epoch 199 | Loss = 2.683714813656277\n",
      "Epoch 200 | Loss = 2.7847919993930392\n",
      "+-----------------------------------------------------------------------------+\n",
      "Training Time: 25622.18918943405 seconds.\n",
      "Get Embedding...\n",
      "(118, 64) (118,)\n",
      "SVC Accuracy: [0.7689393939393939, 0.7537878787878788]\n",
      "+-----------------------------------------------------------------------------+\n",
      "Embedding Time: 13.065430641174316 seconds.\n",
      "Experient 18\n",
      "20211223-012703 :Log the record!\n",
      "+-----------------------------------------------------------------------------+\n",
      "Experient 19\n",
      "!!!\n",
      "89\n",
      "!!!\n",
      "[553, 217, 346, 850, 1139, 72, 280, 861, 916, 589, 50, 182, 307, 1067, 172, 583, 831, 791, 1014, 246, 18, 1013, 59, 1023, 984, 69, 755, 65, 71, 742, 1153, 256, 856, 266, 427, 956, 679, 826, 541, 147, 910, 0, 922, 683, 131, 103, 91, 1001, 166, 558, 796, 1078, 811, 852, 351, 1002, 383, 145, 197, 1072, 789, 17, 606, 9, 608, 1111, 664, 414, 154, 290, 954, 667, 959, 1097, 652, 339, 184, 368, 327, 506, 933, 370, 36, 768, 67, 1158, 93, 1123, 47, 624, 311, 883, 484, 702, 379, 316, 794, 822, 818, 55, 375, 1008, 605, 584, 920, 575, 986, 675, 82, 468, 1104, 202, 428, 566, 85, 550, 914, 1142, 545, 1145, 870, 1149, 332, 732, 185, 401, 359, 540, 714, 277, 785, 56, 45, 330, 1137, 564, 941, 703, 157, 1056, 1025, 325, 394, 360, 1077, 611, 1058, 1131, 830, 1081, 857, 1105, 364, 57, 233, 291, 1065, 926, 181, 231, 230, 808, 731, 678, 588, 929, 226, 537, 680, 38, 746, 595, 733, 244, 138, 1039, 441, 216, 762, 220, 284, 570, 1166, 1103, 117, 476, 261, 312, 232, 424, 149, 1110, 183, 651, 776, 522, 898, 381, 1100, 1088, 221, 1043, 596, 1084, 719, 669, 253, 715, 498, 35, 112, 495, 524, 548, 369, 735, 574, 580, 391, 167, 293, 942, 793, 273, 204, 1049, 474, 292, 973, 129, 1006, 402, 444, 975, 137, 1071, 647, 656, 1080, 1045, 333, 648, 555, 1127, 576, 486, 602, 989, 399, 3, 1030, 1016, 782, 153, 962, 421, 620, 1036, 264, 944, 848, 736, 717, 26, 1121, 978, 338, 398, 449, 46, 778, 877, 773, 633, 287, 465, 957, 828, 462, 800, 419, 902, 709, 953, 549, 997, 724, 78, 779, 1019, 2, 1050, 546, 1048, 195, 1061, 995, 1032, 10, 25, 450, 357, 1135, 254, 559, 612, 912, 965, 179, 94, 739, 470, 616, 298, 900, 224, 598, 301, 436, 1099, 234, 1060, 1053, 839, 815, 158, 804, 945, 89, 31, 210, 365, 993, 413, 225, 115, 618, 42, 567, 320, 1125, 644, 532, 337, 513, 168, 609, 243, 494, 186, 734, 1159, 193, 439, 305, 641, 628, 300, 521, 636, 948, 198, 994, 556, 96, 1130, 568, 22, 557, 1021, 132, 938, 707, 463, 723, 1152, 915, 711, 156, 631, 931, 457, 1066, 433, 666, 464, 459, 597, 816, 471, 725, 614, 1063, 397, 716, 684, 176, 227, 577, 1004, 801, 637, 34, 761, 302, 404, 1090, 980, 1028, 262, 161, 426, 751, 617, 1000, 39, 345, 504, 880, 907, 409, 173, 515, 249, 1069, 285, 1156, 730, 350, 260, 150, 281, 923, 361, 1062, 918, 1052, 904, 282, 1095, 219, 503, 738, 88, 668, 563, 710, 946, 207, 146, 60, 340, 760, 691, 106, 827, 265, 571, 15, 1132, 1015, 985, 825, 970, 758, 223, 406, 921, 1044, 772, 120, 247, 4, 542, 101, 859, 467, 1150, 12, 871, 268, 1012, 630, 1009, 336, 322, 621, 491, 1026, 187, 629, 432, 385, 992, 809, 1024, 756, 366, 62, 188, 53, 899, 950, 939, 1175, 988, 13, 105, 692, 659, 160, 960, 134, 875, 492, 107, 505, 757, 932, 653, 533, 175, 342, 314, 1165, 936, 1114, 252, 639, 8, 695, 838, 539, 824, 458, 408, 508, 248, 148, 352, 1162, 663, 911, 578, 318, 1116, 108, 1106, 235, 844, 54, 889, 645, 586, 1068, 239, 326, 763, 689, 104, 81, 323, 48, 1031, 908, 1041, 851, 1124, 452, 961, 122, 127, 1108, 388, 215, 155, 73, 887, 475, 77, 348, 485, 84, 1148, 650, 509, 75, 299, 331, 144, 241, 288, 727, 619, 775, 625, 206, 977, 455, 934, 697, 777, 865, 245, 835, 585, 133, 70, 655, 1161, 1092, 892, 190, 7, 32, 807, 110, 693, 1134, 1064, 295, 19, 872, 729, 1154, 289, 1176, 657, 142, 890, 445, 952, 275, 866, 587, 927, 603, 943, 211, 1118, 728, 593, 701, 477, 222, 196, 440, 205, 531, 981, 354, 478, 165, 242, 958, 1115, 1075, 1074, 1007, 24, 967, 913, 49, 744, 11, 554, 201, 321, 437, 517, 999, 814, 529, 681, 236, 180, 750, 780, 135, 868, 516, 879, 820, 315, 1128, 560, 218, 845, 297, 949, 841, 1059, 309, 1167, 1155, 665, 425, 1172, 461, 812, 79, 143, 622, 983, 937, 438, 353, 832, 6, 901, 795, 990, 759, 869, 1120, 276, 686, 810, 199, 371, 174, 677, 627, 116, 1133, 435, 519, 113, 496, 1029, 706, 255, 764, 130, 1005, 525, 500, 453, 367, 720, 171, 278, 483, 1122, 754, 698, 1051, 599, 1126, 74, 867, 726, 885, 250, 191, 1027, 229, 996, 310, 209, 884, 111, 456, 1082, 76, 358, 343, 66, 356, 213, 874, 1164, 362, 813, 1138, 5, 1055, 324, 888, 480, 849, 1033, 228, 267, 410, 786, 1035, 139, 873, 972, 740, 303, 951, 304, 1140, 543, 682, 382, 393, 896, 488, 718, 447, 1054, 417, 643, 43, 964, 14, 237, 189, 662, 29, 306, 1144, 119, 654, 86, 1107, 783, 511, 876, 1136, 696, 1146, 1086, 507, 341, 649, 771, 169, 274, 37, 376, 317, 279, 395, 840, 966, 251, 687, 878, 787, 159, 600, 1087, 905, 319, 1160, 378, 1094, 263, 52, 987, 344, 308, 363, 416, 396, 924, 769, 688, 33, 615, 460, 979, 906, 121, 895, 1171, 102, 579, 526, 882, 1168, 177, 523, 380, 1076, 864, 1085, 125, 674, 83, 781, 64, 124, 1047, 51, 20, 661, 767, 258, 151, 833, 638, 1109, 590, 635, 1169, 930, 372, 448, 982, 991, 846, 538, 374, 792, 446, 708, 528, 283, 819, 1089, 858, 1038, 806, 466, 493, 1174, 347, 114, 136, 128, 863, 935, 1093, 415, 269, 510, 1129, 834, 1, 389, 68, 749, 963, 552, 487, 430, 212, 971, 847, 745, 194, 162, 713, 296, 1011, 592, 582, 917, 594, 429, 853, 1073, 784, 520, 499, 469, 451, 1079, 572, 862, 753, 737, 27, 1117, 547, 947, 123, 894, 1112, 855, 384, 1034, 422, 490, 712, 843, 192, 1173, 203, 634, 514, 928, 694, 423, 141, 420, 329, 1017, 1119, 377, 1046, 527, 998, 349, 272, 454, 1057, 501, 178, 392, 405, 774, 472, 238, 1042, 534, 699, 482, 1018, 569, 842, 164, 660, 1113, 536, 817, 95, 240, 387, 854, 126, 561, 743, 431, 672, 802, 208, 766, 690, 390, 1101, 591, 604, 109, 909, 803, 28, 286, 765, 163, 601, 1163, 722, 90, 1177, 1037, 152, 1098, 805, 334, 294, 829, 700, 974, 21]\n",
      "Number of Graphs (dataset_train): 1060\n",
      "Number of Graphs (dataset_test): 118\n",
      "+-----------------------------------------------------------------------------+\n",
      "Learning Rate: 0.001\n",
      "Epochs: 200\n",
      "Batch Size: 128\n",
      "Hidden Dimension 512\n",
      "Number of Layer in Encoder: 1\n",
      "Opimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device: cuda\n",
      "Model:\n",
      "SimCLR(\n",
      "  (encoder): Encoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=89, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      ))\n",
      "    )\n",
      "    (bns): ModuleList(\n",
      "      (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Projection_MLP(\n",
      "    (layer1): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (unilayer): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Start Training...\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss = 39.43921105066935\n",
      "Epoch 2 | Loss = 17.98914072248671\n",
      "Epoch 3 | Loss = 9.773813936445448\n",
      "Epoch 4 | Loss = 5.046932379404704\n",
      "Epoch 5 | Loss = 1.780050277709961\n",
      "Epoch 6 | Loss = -0.7587081061469184\n",
      "Epoch 7 | Loss = -2.497444576687283\n",
      "Epoch 8 | Loss = -3.6040550337897406\n",
      "Epoch 9 | Loss = -4.595511959658729\n",
      "Epoch 10 | Loss = -5.152791546450721\n",
      "Epoch 11 | Loss = -5.855264958408144\n",
      "Epoch 12 | Loss = -6.630805783801609\n",
      "Epoch 13 | Loss = -6.706911742687225\n",
      "Epoch 14 | Loss = -7.203454302416907\n",
      "Epoch 15 | Loss = -7.650062759717305\n",
      "Epoch 16 | Loss = -7.64212582177586\n",
      "Epoch 17 | Loss = -8.414598597420586\n",
      "Epoch 18 | Loss = -8.737910932964748\n",
      "Epoch 19 | Loss = -9.242699278725517\n",
      "Epoch 20 | Loss = -9.517542415195042\n",
      "Epoch 21 | Loss = -9.92405194706387\n",
      "Epoch 22 | Loss = -10.058494091033936\n",
      "Epoch 23 | Loss = -10.151903788248697\n",
      "Epoch 24 | Loss = -10.463161574469673\n",
      "Epoch 25 | Loss = -10.575338761011759\n",
      "Epoch 26 | Loss = -11.091432836320665\n",
      "Epoch 27 | Loss = -11.504544893900553\n",
      "Epoch 28 | Loss = -11.92478577295939\n",
      "Epoch 29 | Loss = -11.639139758216011\n",
      "Epoch 30 | Loss = -11.924286365509033\n",
      "Epoch 31 | Loss = -12.202807161543104\n",
      "Epoch 32 | Loss = -12.579256216684977\n",
      "Epoch 33 | Loss = -12.399649090237087\n",
      "Epoch 34 | Loss = -12.590481387244331\n",
      "Epoch 35 | Loss = -13.004566139645046\n",
      "Epoch 36 | Loss = -13.11559788386027\n",
      "Epoch 37 | Loss = -13.269953197903103\n",
      "Epoch 38 | Loss = -13.507512198554146\n",
      "Epoch 39 | Loss = -13.62478690677219\n",
      "Epoch 40 | Loss = -13.879671997494167\n",
      "Epoch 41 | Loss = -13.94815969467163\n",
      "Epoch 42 | Loss = -13.99602582719591\n",
      "Epoch 43 | Loss = -14.109440644582113\n",
      "Epoch 44 | Loss = -14.08018249935574\n",
      "Epoch 45 | Loss = -14.22942754957411\n",
      "Epoch 46 | Loss = -14.245948844485813\n",
      "Epoch 47 | Loss = -14.103711075252956\n",
      "Epoch 48 | Loss = -14.507481098175049\n",
      "Epoch 49 | Loss = -14.66314829720391\n",
      "Epoch 50 | Loss = -15.071251657274034\n",
      "Epoch 51 | Loss = -15.35480546951294\n",
      "Epoch 52 | Loss = -15.21556838353475\n",
      "Epoch 53 | Loss = -15.58141009012858\n",
      "Epoch 54 | Loss = -15.310929828219944\n",
      "Epoch 55 | Loss = -15.543053521050346\n",
      "Epoch 56 | Loss = -15.739203717973497\n",
      "Epoch 57 | Loss = -15.743513266245523\n",
      "Epoch 58 | Loss = -15.747092723846436\n",
      "Epoch 59 | Loss = -15.892058319515652\n",
      "Epoch 60 | Loss = -15.646503554450142\n",
      "Epoch 61 | Loss = -15.873644457923042\n",
      "Epoch 62 | Loss = -15.889419873555502\n",
      "Epoch 63 | Loss = -16.000991662343342\n",
      "Epoch 64 | Loss = -16.164300017886692\n",
      "Epoch 65 | Loss = -16.237159623040093\n",
      "Epoch 66 | Loss = -16.520660559336346\n",
      "Epoch 67 | Loss = -16.398190445370144\n",
      "Epoch 68 | Loss = -16.616547743479412\n",
      "Epoch 69 | Loss = -16.673672252231174\n",
      "Epoch 70 | Loss = -16.759057150946724\n",
      "Epoch 71 | Loss = -16.89195950826009\n",
      "Epoch 72 | Loss = -16.842243724399143\n",
      "Epoch 73 | Loss = -17.004668288760715\n",
      "Epoch 74 | Loss = -16.967523521847195\n",
      "Epoch 75 | Loss = -17.099462774064804\n",
      "Epoch 76 | Loss = -17.179766919877792\n",
      "Epoch 77 | Loss = -17.258416387769913\n",
      "Epoch 78 | Loss = -17.210707929399277\n",
      "Epoch 79 | Loss = -17.372530460357666\n",
      "Epoch 80 | Loss = -17.488730748494465\n",
      "Epoch 81 | Loss = -17.593360477023655\n",
      "Epoch 82 | Loss = -17.70484511057536\n",
      "Epoch 83 | Loss = -17.753894329071045\n",
      "Epoch 84 | Loss = -17.54961856206258\n",
      "Epoch 85 | Loss = -17.67979023191664\n",
      "Epoch 86 | Loss = -17.644517792595757\n",
      "Epoch 87 | Loss = -17.801387363009983\n",
      "Epoch 88 | Loss = -17.87877458996243\n",
      "Epoch 89 | Loss = -17.849411222669815\n",
      "Epoch 90 | Loss = -18.09444899029202\n",
      "Epoch 91 | Loss = -17.94800302717421\n",
      "Epoch 92 | Loss = -18.257794221242268\n",
      "Epoch 93 | Loss = -18.273023976220024\n",
      "Epoch 94 | Loss = -18.33035495546129\n",
      "Epoch 95 | Loss = -18.316607634226482\n",
      "Epoch 96 | Loss = -18.16215451558431\n",
      "Epoch 97 | Loss = -18.426533699035645\n",
      "Epoch 98 | Loss = -18.44775210486518\n",
      "Epoch 99 | Loss = -18.582075066036648\n",
      "Epoch 100 | Loss = -18.350320127275253\n",
      "Epoch 101 | Loss = -18.42040517595079\n",
      "Epoch 102 | Loss = -18.62243101331923\n",
      "Epoch 103 | Loss = -18.67649088965522\n",
      "Epoch 104 | Loss = -18.534355057610405\n",
      "Epoch 105 | Loss = -18.81595468521118\n",
      "Epoch 106 | Loss = -18.872133043077255\n",
      "Epoch 107 | Loss = -18.690508524576824\n",
      "Epoch 108 | Loss = -18.759186638726128\n",
      "Epoch 109 | Loss = -18.764054934183758\n",
      "Epoch 110 | Loss = -18.931398232777912\n",
      "Epoch 111 | Loss = -19.025816652509903\n",
      "Epoch 112 | Loss = -19.11011340883043\n",
      "Epoch 113 | Loss = -19.053520732455784\n",
      "Epoch 114 | Loss = -19.12982702255249\n",
      "Epoch 115 | Loss = -19.13150125079685\n",
      "Epoch 116 | Loss = -19.17832401063707\n",
      "Epoch 117 | Loss = -19.067916870117188\n",
      "Epoch 118 | Loss = -19.16763385136922\n",
      "Epoch 119 | Loss = -19.16708623038398\n",
      "Epoch 120 | Loss = -19.37043020460341\n",
      "Epoch 121 | Loss = -19.23929124408298\n",
      "Epoch 122 | Loss = -19.363509231143528\n",
      "Epoch 123 | Loss = -19.69434075885349\n",
      "Epoch 124 | Loss = -19.519474241468643\n",
      "Epoch 125 | Loss = -19.576832824283176\n",
      "Epoch 126 | Loss = -19.500943077935112\n",
      "Epoch 127 | Loss = -19.377040174272324\n",
      "Epoch 128 | Loss = -19.637948777940537\n",
      "Epoch 129 | Loss = -19.707999017503525\n",
      "Epoch 130 | Loss = -19.814681424034966\n",
      "Epoch 131 | Loss = -19.761540836758083\n",
      "Epoch 132 | Loss = -19.628398206498886\n",
      "Epoch 133 | Loss = -19.901389174991184\n",
      "Epoch 134 | Loss = -19.686501450008816\n",
      "Epoch 135 | Loss = -20.021504137251114\n",
      "Epoch 136 | Loss = -19.87635596593221\n",
      "Epoch 137 | Loss = -19.931413173675537\n",
      "Epoch 138 | Loss = -19.76139275232951\n",
      "Epoch 139 | Loss = -20.04383177227444\n",
      "Epoch 141 | Loss = -20.00308969285753\n",
      "Epoch 142 | Loss = -19.91686545477973\n",
      "Epoch 143 | Loss = -19.978116936153835\n",
      "Epoch 144 | Loss = -20.165882216559517\n",
      "Epoch 145 | Loss = -20.290464454227024\n",
      "Epoch 146 | Loss = -20.288832929399277\n",
      "Epoch 147 | Loss = -19.95105134116279\n",
      "Epoch 148 | Loss = -20.20586485332913\n",
      "Epoch 149 | Loss = -20.117615222930908\n",
      "Epoch 150 | Loss = -20.17173565758599\n",
      "Epoch 151 | Loss = -20.323280175526936\n",
      "Epoch 152 | Loss = -20.351123332977295\n",
      "Epoch 153 | Loss = -20.407855987548828\n",
      "Epoch 154 | Loss = -20.57602416144477\n",
      "Epoch 155 | Loss = -20.567002243465847\n",
      "Epoch 156 | Loss = -20.647685474819607\n",
      "Epoch 157 | Loss = -20.523061964246963\n",
      "Epoch 158 | Loss = -20.515928798251682\n",
      "Epoch 159 | Loss = -20.610105143653023\n",
      "Epoch 160 | Loss = -20.592146343655056\n",
      "Epoch 161 | Loss = -20.690207640329998\n",
      "Epoch 162 | Loss = -20.620107226901585\n",
      "Epoch 163 | Loss = -20.843502150641548\n",
      "Epoch 164 | Loss = -20.683112091488308\n",
      "Epoch 165 | Loss = -20.789668877919514\n",
      "Epoch 166 | Loss = -20.7287401093377\n",
      "Epoch 167 | Loss = -20.77126720216539\n",
      "Epoch 168 | Loss = -20.697343243492973\n",
      "Epoch 169 | Loss = -20.914295302497017\n",
      "Epoch 170 | Loss = -20.806965668996174\n",
      "Epoch 171 | Loss = -20.8989126947191\n",
      "Epoch 172 | Loss = -20.95178249147203\n",
      "Epoch 173 | Loss = -20.908394442664253\n",
      "Epoch 174 | Loss = -20.90459394454956\n",
      "Epoch 175 | Loss = -20.983054055107964\n",
      "Epoch 176 | Loss = -21.033270570966934\n",
      "Epoch 177 | Loss = -21.09489870071411\n",
      "Epoch 178 | Loss = -21.12795093324449\n",
      "Epoch 179 | Loss = -21.212846014234756\n",
      "Epoch 180 | Loss = -21.0187889734904\n",
      "Epoch 181 | Loss = -21.135859860314262\n",
      "Epoch 182 | Loss = -21.107174025641548\n",
      "Epoch 183 | Loss = -20.94585609436035\n",
      "Epoch 184 | Loss = -21.121242894066704\n",
      "Epoch 185 | Loss = -21.266187349955242\n",
      "Epoch 186 | Loss = -21.182026810116238\n",
      "Epoch 187 | Loss = -21.397208372751873\n",
      "Epoch 188 | Loss = -21.406785647074383\n",
      "Epoch 189 | Loss = -21.286684883965385\n",
      "Epoch 190 | Loss = -21.31476418177287\n",
      "Epoch 191 | Loss = -21.32872459623549\n",
      "Epoch 192 | Loss = -21.257717079586453\n",
      "Epoch 193 | Loss = -21.284904585944282\n",
      "Epoch 194 | Loss = -21.418226136101616\n",
      "Epoch 195 | Loss = -21.35655339558919\n",
      "Epoch 196 | Loss = -21.449470202128094\n",
      "Epoch 197 | Loss = -21.393399821387398\n",
      "Epoch 198 | Loss = -21.524399333530003\n",
      "Epoch 199 | Loss = -21.412080658806694\n",
      "Epoch 200 | Loss = -21.471220016479492\n",
      "+-----------------------------------------------------------------------------+\n",
      "Training Time: 25614.911533117294 seconds.\n",
      "Get Embedding...\n",
      "(118, 512) (118,)\n",
      "SVC Accuracy: [0.7712121212121212, 0.7462121212121212]\n",
      "+-----------------------------------------------------------------------------+\n",
      "Embedding Time: 13.63161039352417 seconds.\n",
      "Experient 19\n",
      "20211223-083412 :Log the record!\n",
      "+-----------------------------------------------------------------------------+\n",
      "Experient 20\n",
      "!!!\n",
      "89\n",
      "!!!\n",
      "[553, 217, 346, 850, 1139, 72, 280, 861, 916, 589, 50, 182, 307, 1067, 172, 583, 831, 791, 1014, 246, 18, 1013, 59, 1023, 984, 69, 755, 65, 71, 742, 1153, 256, 856, 266, 427, 956, 679, 826, 541, 147, 910, 0, 922, 683, 131, 103, 91, 1001, 166, 558, 796, 1078, 811, 852, 351, 1002, 383, 145, 197, 1072, 789, 17, 606, 9, 608, 1111, 664, 414, 154, 290, 954, 667, 959, 1097, 652, 339, 184, 368, 327, 506, 933, 370, 36, 768, 67, 1158, 93, 1123, 47, 624, 311, 883, 484, 702, 379, 316, 794, 822, 818, 55, 375, 1008, 605, 584, 920, 575, 986, 675, 82, 468, 1104, 202, 428, 566, 85, 550, 914, 1142, 545, 1145, 870, 1149, 332, 732, 185, 401, 359, 540, 714, 277, 785, 56, 45, 330, 1137, 564, 941, 703, 157, 1056, 1025, 325, 394, 360, 1077, 611, 1058, 1131, 830, 1081, 857, 1105, 364, 57, 233, 291, 1065, 926, 181, 231, 230, 808, 731, 678, 588, 929, 226, 537, 680, 38, 746, 595, 733, 244, 138, 1039, 441, 216, 762, 220, 284, 570, 1166, 1103, 117, 476, 261, 312, 232, 424, 149, 1110, 183, 651, 776, 522, 898, 381, 1100, 1088, 221, 1043, 596, 1084, 719, 669, 253, 715, 498, 35, 112, 495, 524, 548, 369, 735, 574, 580, 391, 167, 293, 942, 793, 273, 204, 1049, 474, 292, 973, 129, 1006, 402, 444, 975, 137, 1071, 647, 656, 1080, 1045, 333, 648, 555, 1127, 576, 486, 602, 989, 399, 3, 1030, 1016, 782, 153, 962, 421, 620, 1036, 264, 944, 848, 736, 717, 26, 1121, 978, 338, 398, 449, 46, 778, 877, 773, 633, 287, 465, 957, 828, 462, 800, 419, 902, 709, 953, 549, 997, 724, 78, 779, 1019, 2, 1050, 546, 1048, 195, 1061, 995, 1032, 10, 25, 450, 357, 1135, 254, 559, 612, 912, 965, 179, 94, 739, 470, 616, 298, 900, 224, 598, 301, 436, 1099, 234, 1060, 1053, 839, 815, 158, 804, 945, 89, 31, 210, 365, 993, 413, 225, 115, 618, 42, 567, 320, 1125, 644, 532, 337, 513, 168, 609, 243, 494, 186, 734, 1159, 193, 439, 305, 641, 628, 300, 521, 636, 948, 198, 994, 556, 96, 1130, 568, 22, 557, 1021, 132, 938, 707, 463, 723, 1152, 915, 711, 156, 631, 931, 457, 1066, 433, 666, 464, 459, 597, 816, 471, 725, 614, 1063, 397, 716, 684, 176, 227, 577, 1004, 801, 637, 34, 761, 302, 404, 1090, 980, 1028, 262, 161, 426, 751, 617, 1000, 39, 345, 504, 880, 907, 409, 173, 515, 249, 1069, 285, 1156, 730, 350, 260, 150, 281, 923, 361, 1062, 918, 1052, 904, 282, 1095, 219, 503, 738, 88, 668, 563, 710, 946, 207, 146, 60, 340, 760, 691, 106, 827, 265, 571, 15, 1132, 1015, 985, 825, 970, 758, 223, 406, 921, 1044, 772, 120, 247, 4, 542, 101, 859, 467, 1150, 12, 871, 268, 1012, 630, 1009, 336, 322, 621, 491, 1026, 187, 629, 432, 385, 992, 809, 1024, 756, 366, 62, 188, 53, 899, 950, 939, 1175, 988, 13, 105, 692, 659, 160, 960, 134, 875, 492, 107, 505, 757, 932, 653, 533, 175, 342, 314, 1165, 936, 1114, 252, 639, 8, 695, 838, 539, 824, 458, 408, 508, 248, 148, 352, 1162, 663, 911, 578, 318, 1116, 108, 1106, 235, 844, 54, 889, 645, 586, 1068, 239, 326, 763, 689, 104, 81, 323, 48, 1031, 908, 1041, 851, 1124, 452, 961, 122, 127, 1108, 388, 215, 155, 73, 887, 475, 77, 348, 485, 84, 1148, 650, 509, 75, 299, 331, 144, 241, 288, 727, 619, 775, 625, 206, 977, 455, 934, 697, 777, 865, 245, 835, 585, 133, 70, 655, 1161, 1092, 892, 190, 7, 32, 807, 110, 693, 1134, 1064, 295, 19, 872, 729, 1154, 289, 1176, 657, 142, 890, 445, 952, 275, 866, 587, 927, 603, 943, 211, 1118, 728, 593, 701, 477, 222, 196, 440, 205, 531, 981, 354, 478, 165, 242, 958, 1115, 1075, 1074, 1007, 24, 967, 913, 49, 744, 11, 554, 201, 321, 437, 517, 999, 814, 529, 681, 236, 180, 750, 780, 135, 868, 516, 879, 820, 315, 1128, 560, 218, 845, 297, 949, 841, 1059, 309, 1167, 1155, 665, 425, 1172, 461, 812, 79, 143, 622, 983, 937, 438, 353, 832, 6, 901, 795, 990, 759, 869, 1120, 276, 686, 810, 199, 371, 174, 677, 627, 116, 1133, 435, 519, 113, 496, 1029, 706, 255, 764, 130, 1005, 525, 500, 453, 367, 720, 171, 278, 483, 1122, 754, 698, 1051, 599, 1126, 74, 867, 726, 885, 250, 191, 1027, 229, 996, 310, 209, 884, 111, 456, 1082, 76, 358, 343, 66, 356, 213, 874, 1164, 362, 813, 1138, 5, 1055, 324, 888, 480, 849, 1033, 228, 267, 410, 786, 1035, 139, 873, 972, 740, 303, 951, 304, 1140, 543, 682, 382, 393, 896, 488, 718, 447, 1054, 417, 643, 43, 964, 14, 237, 189, 662, 29, 306, 1144, 119, 654, 86, 1107, 783, 511, 876, 1136, 696, 1146, 1086, 507, 341, 649, 771, 169, 274, 37, 376, 317, 279, 395, 840, 966, 251, 687, 878, 787, 159, 600, 1087, 905, 319, 1160, 378, 1094, 263, 52, 987, 344, 308, 363, 416, 396, 924, 769, 688, 33, 615, 460, 979, 906, 121, 895, 1171, 102, 579, 526, 882, 1168, 177, 523, 380, 1076, 864, 1085, 125, 674, 83, 781, 64, 124, 1047, 51, 20, 661, 767, 258, 151, 833, 638, 1109, 590, 635, 1169, 930, 372, 448, 982, 991, 846, 538, 374, 792, 446, 708, 528, 283, 819, 1089, 858, 1038, 806, 466, 493, 1174, 347, 114, 136, 128, 863, 935, 1093, 415, 269, 510, 1129, 834, 1, 389, 68, 749, 963, 552, 487, 430, 212, 971, 847, 745, 194, 162, 713, 296, 1011, 592, 582, 917, 594, 429, 853, 1073, 784, 520, 499, 469, 451, 1079, 572, 862, 753, 737, 27, 1117, 547, 947, 123, 894, 1112, 855, 384, 1034, 422, 490, 712, 843, 192, 1173, 203, 634, 514, 928, 694, 423, 141, 420, 329, 1017, 1119, 377, 1046, 527, 998, 349, 272, 454, 1057, 501, 178, 392, 405, 774, 472, 238, 1042, 534, 699, 482, 1018, 569, 842, 164, 660, 1113, 536, 817, 95, 240, 387, 854, 126, 561, 743, 431, 672, 802, 208, 766, 690, 390, 1101, 591, 604, 109, 909, 803, 28, 286, 765, 163, 601, 1163, 722, 90, 1177, 1037, 152, 1098, 805, 334, 294, 829, 700, 974, 21]\n",
      "Number of Graphs (dataset_train): 1060\n",
      "Number of Graphs (dataset_test): 118\n",
      "+-----------------------------------------------------------------------------+\n",
      "Learning Rate: 0.001\n",
      "Epochs: 200\n",
      "Batch Size: 128\n",
      "Hidden Dimension 64\n",
      "Number of Layer in Encoder: 2\n",
      "Opimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device: cuda\n",
      "Model:\n",
      "SimCLR(\n",
      "  (encoder): Encoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=89, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      ))\n",
      "      (1): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      ))\n",
      "    )\n",
      "    (bns): ModuleList(\n",
      "      (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Projection_MLP(\n",
      "    (layer1): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (unilayer): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Start Training...\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss = 89.92927511533101\n",
      "Epoch 2 | Loss = 57.87151151233249\n",
      "Epoch 3 | Loss = 44.64805788464017\n",
      "Epoch 4 | Loss = 37.133371671040855\n",
      "Epoch 5 | Loss = 31.239386134677464\n",
      "Epoch 6 | Loss = 27.652652528550888\n",
      "Epoch 7 | Loss = 24.755386034647625\n",
      "Epoch 8 | Loss = 22.243388228946262\n",
      "Epoch 9 | Loss = 20.148584365844727\n",
      "Epoch 10 | Loss = 18.30736271540324\n",
      "Epoch 11 | Loss = 16.729559739430744\n",
      "Epoch 12 | Loss = 15.755707740783691\n",
      "Epoch 13 | Loss = 14.867966916826036\n",
      "Epoch 14 | Loss = 13.352868503994411\n",
      "Epoch 15 | Loss = 12.835677888658312\n",
      "Epoch 16 | Loss = 11.565414746602377\n",
      "Epoch 17 | Loss = 10.902485953436958\n",
      "Epoch 18 | Loss = 10.393405967288547\n",
      "Epoch 19 | Loss = 9.536806424458822\n",
      "Epoch 20 | Loss = 8.602660338083902\n",
      "Epoch 21 | Loss = 7.872929943932427\n",
      "Epoch 22 | Loss = 7.735918786790636\n",
      "Epoch 23 | Loss = 7.174758381313747\n",
      "Epoch 24 | Loss = 6.8429029782613116\n",
      "Epoch 25 | Loss = 6.059680249955919\n",
      "Epoch 26 | Loss = 5.520777119530572\n",
      "Epoch 27 | Loss = 5.135431660546197\n",
      "Epoch 28 | Loss = 4.9009425905015735\n",
      "Epoch 29 | Loss = 5.0348472065395775\n",
      "Epoch 30 | Loss = 4.740563127729628\n",
      "Epoch 31 | Loss = 4.113987233903673\n",
      "Epoch 32 | Loss = 3.629971504211426\n",
      "Epoch 33 | Loss = 3.4178597132364907\n",
      "Epoch 34 | Loss = 3.1220250129699707\n",
      "Epoch 35 | Loss = 2.2380896144443088\n",
      "Epoch 36 | Loss = 2.46004491382175\n",
      "Epoch 37 | Loss = 2.216988934410943\n",
      "Epoch 38 | Loss = 1.8543544345431857\n",
      "Epoch 39 | Loss = 1.6482653088039823\n",
      "Epoch 40 | Loss = 1.6144911977979872\n",
      "Epoch 41 | Loss = 1.34909454981486\n",
      "Epoch 42 | Loss = 0.8527043660481771\n",
      "Epoch 43 | Loss = 1.1327685250176325\n",
      "Epoch 44 | Loss = 0.785910341474745\n",
      "Epoch 45 | Loss = 0.5158159467909071\n",
      "Epoch 46 | Loss = 0.6774747636583116\n",
      "Epoch 47 | Loss = 0.04782957500881619\n",
      "Epoch 48 | Loss = -0.1648297839694553\n",
      "Epoch 49 | Loss = -0.06633355882432726\n",
      "Epoch 50 | Loss = -0.9087760183546278\n",
      "Epoch 51 | Loss = -1.1227675543891058\n",
      "Epoch 52 | Loss = -0.6828561888800727\n",
      "Epoch 53 | Loss = -1.1436015764872234\n",
      "Epoch 54 | Loss = -1.2628173033396404\n",
      "Epoch 55 | Loss = -0.8682602246602377\n",
      "Epoch 56 | Loss = -1.5132674641079373\n",
      "Epoch 57 | Loss = -1.2830821143256292\n",
      "Epoch 58 | Loss = -1.6263535287645128\n",
      "Epoch 59 | Loss = -1.6683554649353027\n",
      "Epoch 60 | Loss = -2.2641641563839383\n",
      "Epoch 61 | Loss = -2.014973772896661\n",
      "Epoch 62 | Loss = -2.3210283385382757\n",
      "Epoch 63 | Loss = -2.31303694513109\n",
      "Epoch 64 | Loss = -2.6147159735361734\n",
      "Epoch 65 | Loss = -2.647790167066786\n",
      "Epoch 66 | Loss = -2.7827294402652316\n",
      "Epoch 67 | Loss = -2.810675435596042\n",
      "Epoch 68 | Loss = -2.9016567866007485\n",
      "Epoch 69 | Loss = -3.0210087299346924\n",
      "Epoch 70 | Loss = -3.4038365946875677\n",
      "Epoch 71 | Loss = -3.2765611277686224\n",
      "Epoch 72 | Loss = -3.555133408970303\n",
      "Epoch 73 | Loss = -3.358941634496053\n",
      "Epoch 74 | Loss = -3.7631601757473416\n",
      "Epoch 75 | Loss = -3.4254601531558566\n",
      "Epoch 76 | Loss = -3.6907409032185874\n",
      "Epoch 77 | Loss = -3.7938619322246976\n",
      "Epoch 78 | Loss = -4.053506718741523\n",
      "Epoch 79 | Loss = -4.2352710697386\n",
      "Epoch 80 | Loss = -4.027908305327098\n",
      "Epoch 81 | Loss = -4.445540103647444\n",
      "Epoch 82 | Loss = -4.543131205770704\n",
      "Epoch 83 | Loss = -4.865470263693068\n",
      "Epoch 84 | Loss = -4.455758935875362\n",
      "Epoch 85 | Loss = -4.643012364705403\n",
      "Epoch 86 | Loss = -4.443063894907634\n",
      "Epoch 87 | Loss = -4.6236500011550055\n",
      "Epoch 88 | Loss = -4.443089657359653\n",
      "Epoch 89 | Loss = -5.039158917135662\n",
      "Epoch 90 | Loss = -5.141746898492177\n",
      "Epoch 91 | Loss = -4.962390131420559\n",
      "Epoch 92 | Loss = -5.483112403915988\n",
      "Epoch 93 | Loss = -5.245161682367325\n",
      "Epoch 94 | Loss = -5.387099876999855\n",
      "Epoch 95 | Loss = -5.4416203399499254\n",
      "Epoch 96 | Loss = -5.578495376639896\n",
      "Epoch 97 | Loss = -5.785373752315839\n",
      "Epoch 98 | Loss = -5.626933899190691\n",
      "Epoch 99 | Loss = -5.545565903186798\n",
      "Epoch 100 | Loss = -5.5110184252262115\n",
      "Epoch 101 | Loss = -5.6574623617861\n",
      "Epoch 102 | Loss = -5.9376101228925915\n",
      "Epoch 103 | Loss = -6.1801372319459915\n",
      "Epoch 104 | Loss = -5.6578549014197455\n",
      "Epoch 105 | Loss = -6.23431572649214\n",
      "Epoch 106 | Loss = -6.1265651351875725\n",
      "Epoch 107 | Loss = -6.387338674730724\n",
      "Epoch 108 | Loss = -6.023180888758765\n",
      "Epoch 109 | Loss = -6.401655159062809\n",
      "Epoch 110 | Loss = -6.396900319390827\n",
      "Epoch 111 | Loss = -6.415369225872888\n",
      "Epoch 112 | Loss = -6.5078102846940356\n",
      "Epoch 113 | Loss = -6.087569220198525\n",
      "Epoch 114 | Loss = -6.203266491492589\n",
      "Epoch 115 | Loss = -6.466566865642865\n",
      "Epoch 116 | Loss = -6.5376936006877155\n",
      "Epoch 117 | Loss = -6.7105322811338635\n",
      "Epoch 118 | Loss = -7.071013112862905\n",
      "Epoch 119 | Loss = -6.71125023232566\n",
      "Epoch 120 | Loss = -6.863078123993343\n",
      "Epoch 121 | Loss = -6.868109491136339\n",
      "Epoch 122 | Loss = -7.040036969714695\n",
      "Epoch 123 | Loss = -7.172223293119007\n",
      "Epoch 124 | Loss = -6.8823387953970165\n",
      "Epoch 125 | Loss = -6.9523548814985485\n",
      "Epoch 126 | Loss = -7.129205273257361\n",
      "Epoch 127 | Loss = -7.008583287398021\n",
      "Epoch 128 | Loss = -7.027229090531667\n",
      "Epoch 129 | Loss = -7.073188801606496\n",
      "Epoch 130 | Loss = -7.350743368268013\n",
      "Epoch 131 | Loss = -7.495024316840702\n",
      "Epoch 132 | Loss = -7.655867232216729\n",
      "Epoch 133 | Loss = -7.2855737408002215\n",
      "Epoch 134 | Loss = -7.517929779158698\n",
      "Epoch 135 | Loss = -7.667643679512872\n",
      "Epoch 136 | Loss = -7.416977134015825\n",
      "Epoch 137 | Loss = -7.696106407377455\n",
      "Epoch 138 | Loss = -7.705913437737359\n",
      "Epoch 139 | Loss = -7.980680730607775\n",
      "Epoch 140 | Loss = -7.725869582759009\n",
      "Epoch 141 | Loss = -7.725571254889171\n",
      "Epoch 142 | Loss = -7.928252882427639\n",
      "Epoch 143 | Loss = -7.9518360959159\n",
      "Epoch 144 | Loss = -8.165570815404257\n",
      "Epoch 145 | Loss = -7.7188080416785345\n",
      "Epoch 146 | Loss = -7.71410948700375\n",
      "Epoch 147 | Loss = -7.99917537636227\n",
      "Epoch 148 | Loss = -8.050493525134193\n",
      "Epoch 149 | Loss = -8.122820734977722\n",
      "Epoch 150 | Loss = -7.974912345409393\n",
      "Epoch 151 | Loss = -7.829013639026218\n",
      "Epoch 152 | Loss = -8.192254914177788\n",
      "Epoch 153 | Loss = -8.137462655703226\n",
      "Epoch 154 | Loss = -8.25483669175042\n",
      "Epoch 155 | Loss = -8.244857549667358\n",
      "Epoch 156 | Loss = -8.356951640711891\n",
      "Epoch 157 | Loss = -8.176992230945164\n",
      "Epoch 158 | Loss = -8.61302219496833\n",
      "Epoch 159 | Loss = -8.655436568790012\n",
      "Epoch 160 | Loss = -8.59319159719679\n",
      "Epoch 161 | Loss = -8.421940883000692\n",
      "Epoch 162 | Loss = -8.74227982097202\n",
      "Epoch 163 | Loss = -8.660148832533094\n",
      "Epoch 164 | Loss = -8.927926487392849\n",
      "Epoch 165 | Loss = -8.945605516433716\n",
      "Epoch 166 | Loss = -8.632391320334541\n",
      "Epoch 167 | Loss = -8.766546275880602\n",
      "Epoch 168 | Loss = -8.603349076377022\n",
      "Epoch 169 | Loss = -8.831201553344727\n",
      "Epoch 170 | Loss = -8.63416404194302\n",
      "Epoch 171 | Loss = -9.011819998423258\n",
      "Epoch 172 | Loss = -8.717785172992283\n",
      "Epoch 173 | Loss = -8.876741727193197\n",
      "Epoch 174 | Loss = -9.168138477537367\n",
      "Epoch 175 | Loss = -8.803320540322197\n",
      "Epoch 176 | Loss = -9.301145129733616\n",
      "Epoch 177 | Loss = -9.16254833009508\n",
      "Epoch 178 | Loss = -9.130559285481771\n",
      "Epoch 179 | Loss = -9.237489541371664\n",
      "Epoch 180 | Loss = -8.92052592171563\n",
      "Epoch 181 | Loss = -9.141876829995049\n",
      "Epoch 182 | Loss = -9.383459938897026\n",
      "Epoch 183 | Loss = -9.128020895851982\n",
      "Epoch 184 | Loss = -9.021585252549913\n",
      "Epoch 185 | Loss = -9.249825451109144\n",
      "Epoch 186 | Loss = -9.194463835822212\n",
      "Epoch 187 | Loss = -9.144597238964504\n",
      "Epoch 188 | Loss = -9.15378713607788\n",
      "Epoch 189 | Loss = -9.310374683803982\n",
      "Epoch 190 | Loss = -9.415890216827393\n",
      "Epoch 191 | Loss = -9.4075804816352\n",
      "Epoch 192 | Loss = -9.345160431332058\n",
      "Epoch 193 | Loss = -9.394821776284111\n",
      "Epoch 194 | Loss = -9.55649275249905\n",
      "Epoch 195 | Loss = -9.727335585488213\n",
      "Epoch 196 | Loss = -9.513556294971043\n",
      "Epoch 197 | Loss = -9.777912537256876\n",
      "Epoch 198 | Loss = -9.65634306271871\n",
      "Epoch 199 | Loss = -9.940505743026733\n",
      "(118, 128) (118,)\n",
      "SVC Accuracy: [0.728030303030303, 0.7530303030303029]\n",
      "+-----------------------------------------------------------------------------+\n",
      "Embedding Time: 13.32204794883728 seconds.\n",
      "Experient 20\n",
      "20211223-154114 :Log the record!\n",
      "+-----------------------------------------------------------------------------+\n",
      "Experient 21\n",
      "!!!\n",
      "89\n",
      "!!!\n",
      "[553, 217, 346, 850, 1139, 72, 280, 861, 916, 589, 50, 182, 307, 1067, 172, 583, 831, 791, 1014, 246, 18, 1013, 59, 1023, 984, 69, 755, 65, 71, 742, 1153, 256, 856, 266, 427, 956, 679, 826, 541, 147, 910, 0, 922, 683, 131, 103, 91, 1001, 166, 558, 796, 1078, 811, 852, 351, 1002, 383, 145, 197, 1072, 789, 17, 606, 9, 608, 1111, 664, 414, 154, 290, 954, 667, 959, 1097, 652, 339, 184, 368, 327, 506, 933, 370, 36, 768, 67, 1158, 93, 1123, 47, 624, 311, 883, 484, 702, 379, 316, 794, 822, 818, 55, 375, 1008, 605, 584, 920, 575, 986, 675, 82, 468, 1104, 202, 428, 566, 85, 550, 914, 1142, 545, 1145, 870, 1149, 332, 732, 185, 401, 359, 540, 714, 277, 785, 56, 45, 330, 1137, 564, 941, 703, 157, 1056, 1025, 325, 394, 360, 1077, 611, 1058, 1131, 830, 1081, 857, 1105, 364, 57, 233, 291, 1065, 926, 181, 231, 230, 808, 731, 678, 588, 929, 226, 537, 680, 38, 746, 595, 733, 244, 138, 1039, 441, 216, 762, 220, 284, 570, 1166, 1103, 117, 476, 261, 312, 232, 424, 149, 1110, 183, 651, 776, 522, 898, 381, 1100, 1088, 221, 1043, 596, 1084, 719, 669, 253, 715, 498, 35, 112, 495, 524, 548, 369, 735, 574, 580, 391, 167, 293, 942, 793, 273, 204, 1049, 474, 292, 973, 129, 1006, 402, 444, 975, 137, 1071, 647, 656, 1080, 1045, 333, 648, 555, 1127, 576, 486, 602, 989, 399, 3, 1030, 1016, 782, 153, 962, 421, 620, 1036, 264, 944, 848, 736, 717, 26, 1121, 978, 338, 398, 449, 46, 778, 877, 773, 633, 287, 465, 957, 828, 462, 800, 419, 902, 709, 953, 549, 997, 724, 78, 779, 1019, 2, 1050, 546, 1048, 195, 1061, 995, 1032, 10, 25, 450, 357, 1135, 254, 559, 612, 912, 965, 179, 94, 739, 470, 616, 298, 900, 224, 598, 301, 436, 1099, 234, 1060, 1053, 839, 815, 158, 804, 945, 89, 31, 210, 365, 993, 413, 225, 115, 618, 42, 567, 320, 1125, 644, 532, 337, 513, 168, 609, 243, 494, 186, 734, 1159, 193, 439, 305, 641, 628, 300, 521, 636, 948, 198, 994, 556, 96, 1130, 568, 22, 557, 1021, 132, 938, 707, 463, 723, 1152, 915, 711, 156, 631, 931, 457, 1066, 433, 666, 464, 459, 597, 816, 471, 725, 614, 1063, 397, 716, 684, 176, 227, 577, 1004, 801, 637, 34, 761, 302, 404, 1090, 980, 1028, 262, 161, 426, 751, 617, 1000, 39, 345, 504, 880, 907, 409, 173, 515, 249, 1069, 285, 1156, 730, 350, 260, 150, 281, 923, 361, 1062, 918, 1052, 904, 282, 1095, 219, 503, 738, 88, 668, 563, 710, 946, 207, 146, 60, 340, 760, 691, 106, 827, 265, 571, 15, 1132, 1015, 985, 825, 970, 758, 223, 406, 921, 1044, 772, 120, 247, 4, 542, 101, 859, 467, 1150, 12, 871, 268, 1012, 630, 1009, 336, 322, 621, 491, 1026, 187, 629, 432, 385, 992, 809, 1024, 756, 366, 62, 188, 53, 899, 950, 939, 1175, 988, 13, 105, 692, 659, 160, 960, 134, 875, 492, 107, 505, 757, 932, 653, 533, 175, 342, 314, 1165, 936, 1114, 252, 639, 8, 695, 838, 539, 824, 458, 408, 508, 248, 148, 352, 1162, 663, 911, 578, 318, 1116, 108, 1106, 235, 844, 54, 889, 645, 586, 1068, 239, 326, 763, 689, 104, 81, 323, 48, 1031, 908, 1041, 851, 1124, 452, 961, 122, 127, 1108, 388, 215, 155, 73, 887, 475, 77, 348, 485, 84, 1148, 650, 509, 75, 299, 331, 144, 241, 288, 727, 619, 775, 625, 206, 977, 455, 934, 697, 777, 865, 245, 835, 585, 133, 70, 655, 1161, 1092, 892, 190, 7, 32, 807, 110, 693, 1134, 1064, 295, 19, 872, 729, 1154, 289, 1176, 657, 142, 890, 445, 952, 275, 866, 587, 927, 603, 943, 211, 1118, 728, 593, 701, 477, 222, 196, 440, 205, 531, 981, 354, 478, 165, 242, 958, 1115, 1075, 1074, 1007, 24, 967, 913, 49, 744, 11, 554, 201, 321, 437, 517, 999, 814, 529, 681, 236, 180, 750, 780, 135, 868, 516, 879, 820, 315, 1128, 560, 218, 845, 297, 949, 841, 1059, 309, 1167, 1155, 665, 425, 1172, 461, 812, 79, 143, 622, 983, 937, 438, 353, 832, 6, 901, 795, 990, 759, 869, 1120, 276, 686, 810, 199, 371, 174, 677, 627, 116, 1133, 435, 519, 113, 496, 1029, 706, 255, 764, 130, 1005, 525, 500, 453, 367, 720, 171, 278, 483, 1122, 754, 698, 1051, 599, 1126, 74, 867, 726, 885, 250, 191, 1027, 229, 996, 310, 209, 884, 111, 456, 1082, 76, 358, 343, 66, 356, 213, 874, 1164, 362, 813, 1138, 5, 1055, 324, 888, 480, 849, 1033, 228, 267, 410, 786, 1035, 139, 873, 972, 740, 303, 951, 304, 1140, 543, 682, 382, 393, 896, 488, 718, 447, 1054, 417, 643, 43, 964, 14, 237, 189, 662, 29, 306, 1144, 119, 654, 86, 1107, 783, 511, 876, 1136, 696, 1146, 1086, 507, 341, 649, 771, 169, 274, 37, 376, 317, 279, 395, 840, 966, 251, 687, 878, 787, 159, 600, 1087, 905, 319, 1160, 378, 1094, 263, 52, 987, 344, 308, 363, 416, 396, 924, 769, 688, 33, 615, 460, 979, 906, 121, 895, 1171, 102, 579, 526, 882, 1168, 177, 523, 380, 1076, 864, 1085, 125, 674, 83, 781, 64, 124, 1047, 51, 20, 661, 767, 258, 151, 833, 638, 1109, 590, 635, 1169, 930, 372, 448, 982, 991, 846, 538, 374, 792, 446, 708, 528, 283, 819, 1089, 858, 1038, 806, 466, 493, 1174, 347, 114, 136, 128, 863, 935, 1093, 415, 269, 510, 1129, 834, 1, 389, 68, 749, 963, 552, 487, 430, 212, 971, 847, 745, 194, 162, 713, 296, 1011, 592, 582, 917, 594, 429, 853, 1073, 784, 520, 499, 469, 451, 1079, 572, 862, 753, 737, 27, 1117, 547, 947, 123, 894, 1112, 855, 384, 1034, 422, 490, 712, 843, 192, 1173, 203, 634, 514, 928, 694, 423, 141, 420, 329, 1017, 1119, 377, 1046, 527, 998, 349, 272, 454, 1057, 501, 178, 392, 405, 774, 472, 238, 1042, 534, 699, 482, 1018, 569, 842, 164, 660, 1113, 536, 817, 95, 240, 387, 854, 126, 561, 743, 431, 672, 802, 208, 766, 690, 390, 1101, 591, 604, 109, 909, 803, 28, 286, 765, 163, 601, 1163, 722, 90, 1177, 1037, 152, 1098, 805, 334, 294, 829, 700, 974, 21]\n",
      "Number of Graphs (dataset_train): 1060\n",
      "Number of Graphs (dataset_test): 118\n",
      "+-----------------------------------------------------------------------------+\n",
      "Learning Rate: 0.001\n",
      "Epochs: 200\n",
      "Batch Size: 128\n",
      "Hidden Dimension 512\n",
      "Number of Layer in Encoder: 2\n",
      "Opimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device: cuda\n",
      "Model:\n",
      "SimCLR(\n",
      "  (encoder): Encoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=89, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      ))\n",
      "      (1): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      ))\n",
      "    )\n",
      "    (bns): ModuleList(\n",
      "      (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Projection_MLP(\n",
      "    (layer1): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (unilayer): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Start Training...\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss = 49.775176737043594\n",
      "Epoch 2 | Loss = 25.825519614749485\n",
      "Epoch 3 | Loss = 15.84581094317966\n",
      "Epoch 4 | Loss = 8.712230099572075\n",
      "Epoch 5 | Loss = 4.536907937791613\n",
      "Epoch 6 | Loss = 0.7365436024136014\n",
      "Epoch 7 | Loss = -1.8500653902689617\n",
      "Epoch 8 | Loss = -3.920775214831034\n",
      "Epoch 9 | Loss = -5.227264967229631\n",
      "Epoch 10 | Loss = -6.7068007323477\n",
      "Epoch 11 | Loss = -7.396102054251565\n",
      "Epoch 12 | Loss = -8.564476701948378\n",
      "Epoch 13 | Loss = -9.255345874362522\n",
      "Epoch 14 | Loss = -9.881707906723022\n",
      "Epoch 15 | Loss = -10.619361718495687\n",
      "Epoch 16 | Loss = -11.13208924399482\n",
      "Epoch 17 | Loss = -11.470318953196207\n",
      "Epoch 18 | Loss = -12.241349591149223\n",
      "Epoch 19 | Loss = -12.47073703342014\n",
      "Epoch 20 | Loss = -13.194957415262857\n",
      "Epoch 21 | Loss = -13.343110137515598\n",
      "Epoch 22 | Loss = -13.642466810014513\n",
      "Epoch 23 | Loss = -14.225166744656033\n",
      "Epoch 24 | Loss = -14.246906174553764\n",
      "Epoch 25 | Loss = -14.924766487545437\n",
      "Epoch 26 | Loss = -15.03720998764038\n",
      "Epoch 27 | Loss = -15.50137816535102\n",
      "Epoch 28 | Loss = -15.872592131296793\n",
      "Epoch 29 | Loss = -16.0166310734219\n",
      "Epoch 30 | Loss = -16.328498734368218\n",
      "Epoch 31 | Loss = -16.506398095024956\n",
      "Epoch 32 | Loss = -16.75124502182007\n",
      "Epoch 33 | Loss = -16.96640184190538\n",
      "Epoch 34 | Loss = -17.196822590298122\n",
      "Epoch 35 | Loss = -17.28414551417033\n",
      "Epoch 36 | Loss = -17.581699265374077\n",
      "Epoch 37 | Loss = -17.6837100982666\n",
      "Epoch 38 | Loss = -18.172397295633953\n",
      "Epoch 39 | Loss = -18.261359479692246\n",
      "Epoch 40 | Loss = -18.290568616655136\n",
      "Epoch 41 | Loss = -18.710750314924454\n",
      "Epoch 42 | Loss = -18.94251955880059\n",
      "Epoch 43 | Loss = -19.077396710713703\n",
      "Epoch 44 | Loss = -19.171767287784153\n",
      "Epoch 45 | Loss = -19.245626078711616\n",
      "Epoch 46 | Loss = -19.53935284084744\n",
      "Epoch 47 | Loss = -19.626285447014702\n",
      "Epoch 48 | Loss = -19.63992108239068\n",
      "Epoch 49 | Loss = -19.87238433625963\n",
      "Epoch 50 | Loss = -19.98720794253879\n",
      "Epoch 51 | Loss = -20.205332703060574\n",
      "Epoch 52 | Loss = -20.225633674197727\n",
      "Epoch 53 | Loss = -20.30930365456475\n",
      "Epoch 54 | Loss = -20.356610139211018\n",
      "Epoch 55 | Loss = -20.60154808892144\n",
      "Epoch 56 | Loss = -20.58501958847046\n",
      "Epoch 57 | Loss = -20.791400220659042\n",
      "Epoch 58 | Loss = -20.779328770107693\n",
      "Epoch 59 | Loss = -20.960412555270725\n",
      "Epoch 60 | Loss = -20.925236384073894\n",
      "Epoch 61 | Loss = -21.020432260301376\n",
      "Epoch 62 | Loss = -21.185431957244873\n",
      "Epoch 63 | Loss = -21.322496043311226\n",
      "Epoch 64 | Loss = -21.47623422410753\n",
      "Epoch 65 | Loss = -21.576246049669052\n",
      "Epoch 66 | Loss = -21.72339375813802\n",
      "Epoch 67 | Loss = -21.79891512129042\n",
      "Epoch 68 | Loss = -21.851551638709175\n",
      "Epoch 69 | Loss = -22.0216253068712\n",
      "Epoch 70 | Loss = -21.93025811513265\n",
      "Epoch 71 | Loss = -22.084157943725586\n",
      "Epoch 72 | Loss = -22.173320081498886\n",
      "Epoch 73 | Loss = -22.13071452246772\n",
      "Epoch 74 | Loss = -22.32130061255561\n",
      "Epoch 75 | Loss = -22.348492675357395\n",
      "Epoch 76 | Loss = -22.407236576080322\n",
      "Epoch 77 | Loss = -22.469344086117214\n",
      "Epoch 78 | Loss = -22.5055005285475\n",
      "Epoch 79 | Loss = -22.445498095618355\n",
      "Epoch 80 | Loss = -22.587312433454727\n",
      "Epoch 81 | Loss = -22.507699966430664\n",
      "Epoch 82 | Loss = -22.710558308495415\n",
      "Epoch 83 | Loss = -22.780476305219864\n",
      "Epoch 84 | Loss = -22.835749202304417\n",
      "Epoch 85 | Loss = -22.863366709815132\n",
      "Epoch 86 | Loss = -22.921933280097115\n",
      "Epoch 87 | Loss = -23.011667516496445\n",
      "Epoch 88 | Loss = -23.060206360287136\n",
      "Epoch 89 | Loss = -23.094528092278374\n",
      "Epoch 90 | Loss = -23.062581009334988\n",
      "Epoch 91 | Loss = -23.115619235568577\n",
      "Epoch 92 | Loss = -23.115208095974392\n",
      "Epoch 93 | Loss = -23.267353269788956\n",
      "Epoch 94 | Loss = -23.30483939912584\n",
      "Epoch 95 | Loss = -23.40877119700114\n",
      "Epoch 96 | Loss = -23.333769268459744\n",
      "Epoch 97 | Loss = -23.52328962749905\n",
      "Epoch 98 | Loss = -23.579547087351482\n",
      "Epoch 99 | Loss = -23.583746698167587\n",
      "Epoch 100 | Loss = -23.628481811947292\n",
      "Epoch 101 | Loss = -23.71116214328342\n",
      "Epoch 102 | Loss = -23.625746250152588\n",
      "Epoch 103 | Loss = -23.5987302992079\n",
      "Epoch 104 | Loss = -23.58513042661879\n",
      "Epoch 105 | Loss = -23.47973812950982\n",
      "Epoch 106 | Loss = -23.403713014390732\n",
      "Epoch 107 | Loss = -23.551900068918865\n",
      "Epoch 108 | Loss = -23.586008972591824\n",
      "Epoch 109 | Loss = -23.73662503560384\n",
      "Epoch 110 | Loss = -23.83985620074802\n",
      "Epoch 111 | Loss = -23.98083511988322\n",
      "Epoch 112 | Loss = -24.033285723792183\n",
      "Epoch 113 | Loss = -23.98664485083686\n",
      "Epoch 114 | Loss = -24.08119996388753\n",
      "Epoch 115 | Loss = -24.087552229563396\n",
      "Epoch 116 | Loss = -24.17225233713786\n",
      "Epoch 117 | Loss = -24.123612297905815\n",
      "Epoch 118 | Loss = -24.182173993852402\n",
      "Epoch 119 | Loss = -24.167053116692436\n",
      "Epoch 120 | Loss = -24.291967550913494\n",
      "Epoch 121 | Loss = -24.27393086751302\n",
      "Epoch 122 | Loss = -24.352556493547226\n",
      "Epoch 123 | Loss = -24.476234489017063\n",
      "Epoch 124 | Loss = -24.558838367462158\n",
      "Epoch 125 | Loss = -24.59519248538547\n",
      "Epoch 126 | Loss = -24.551239066653782\n",
      "Epoch 127 | Loss = -24.571535640292698\n",
      "Epoch 128 | Loss = -24.57380512025621\n",
      "Epoch 129 | Loss = -24.67938036388821\n",
      "Epoch 130 | Loss = -24.59903595182631\n",
      "Epoch 131 | Loss = -24.562968995836044\n",
      "Epoch 132 | Loss = -24.618105305565727\n",
      "Epoch 133 | Loss = -24.667458004421658\n",
      "Epoch 134 | Loss = -24.692040337456596\n",
      "Epoch 135 | Loss = -24.711781183878582\n",
      "Epoch 136 | Loss = -24.777522987789578\n",
      "Epoch 137 | Loss = -24.84451346927219\n",
      "Epoch 138 | Loss = -24.90099064509074\n",
      "Epoch 139 | Loss = -24.929749117957222\n",
      "Epoch 140 | Loss = -24.98047622044881\n",
      "Epoch 141 | Loss = -24.93095827102661\n",
      "Epoch 142 | Loss = -24.99819172753228\n",
      "Epoch 143 | Loss = -24.983634365929497\n",
      "Epoch 144 | Loss = -25.021583557128906\n",
      "Epoch 145 | Loss = -25.015422132280136\n",
      "Epoch 146 | Loss = -24.989700423346626\n",
      "Epoch 147 | Loss = -25.12556250890096\n",
      "Epoch 148 | Loss = -25.06001043319702\n",
      "Epoch 149 | Loss = -25.09666045506795\n",
      "Epoch 150 | Loss = -25.118799262576633\n",
      "Epoch 151 | Loss = -25.10327174928453\n",
      "Epoch 152 | Loss = -25.152274131774902\n",
      "Epoch 153 | Loss = -25.198757489522297\n",
      "Epoch 154 | Loss = -25.235365602705215\n",
      "Epoch 155 | Loss = -25.318710168202717\n",
      "Epoch 156 | Loss = -25.310293939378525\n",
      "Epoch 157 | Loss = -25.30696227815416\n",
      "Epoch 158 | Loss = -25.151442580752903\n",
      "Epoch 159 | Loss = -24.945502863989937\n",
      "Epoch 160 | Loss = -25.16664669248793\n",
      "Epoch 161 | Loss = -25.180883354610867\n",
      "Epoch 162 | Loss = -25.297092119852703\n",
      "Epoch 163 | Loss = -25.279761208428276\n",
      "Epoch 164 | Loss = -25.413088904486763\n",
      "Epoch 165 | Loss = -25.419844998253716\n",
      "Epoch 166 | Loss = -25.50308762656318\n",
      "Epoch 167 | Loss = -25.446758535173203\n",
      "Epoch 168 | Loss = -25.442434363894993\n",
      "Epoch 169 | Loss = -25.56260765923394\n",
      "Epoch 170 | Loss = -25.478517797258164\n",
      "Epoch 171 | Loss = -25.507223500145805\n",
      "Epoch 172 | Loss = -25.54739252726237\n",
      "Epoch 173 | Loss = -25.55726279152764\n",
      "Epoch 174 | Loss = -25.558821148342556\n",
      "Epoch 175 | Loss = -25.550778494940865\n",
      "Epoch 176 | Loss = -25.5710785124037\n",
      "Epoch 177 | Loss = -25.64672507180108\n",
      "Epoch 178 | Loss = -25.654953956604004\n",
      "Epoch 179 | Loss = -25.710952599843342\n",
      "Epoch 180 | Loss = -25.71417495939467\n",
      "Epoch 181 | Loss = -25.705640210045708\n",
      "Epoch 182 | Loss = -25.693523089090984\n",
      "Epoch 183 | Loss = -25.797110027737087\n",
      "Epoch 184 | Loss = -25.727120240529377\n",
      "Epoch 185 | Loss = -25.790811856587727\n",
      "Epoch 186 | Loss = -25.737530443403458\n",
      "Epoch 187 | Loss = -25.788561556074356\n",
      "Epoch 188 | Loss = -25.733886188930935\n",
      "Epoch 189 | Loss = -25.75933239195082\n",
      "Epoch 190 | Loss = -25.560156451331245\n",
      "Epoch 191 | Loss = -25.29063336054484\n",
      "Epoch 192 | Loss = -25.178460174136692\n",
      "Epoch 193 | Loss = -25.295127497779\n",
      "Epoch 194 | Loss = -25.357183032565647\n",
      "Epoch 195 | Loss = -25.39720842573378\n",
      "Epoch 196 | Loss = -25.511697186364067\n",
      "Epoch 197 | Loss = -25.612105634477402\n",
      "Epoch 198 | Loss = -25.617780420515274\n",
      "Epoch 199 | Loss = -25.790012465582954\n",
      "Epoch 200 | Loss = -25.760109212663437\n",
      "+-----------------------------------------------------------------------------+\n",
      "Training Time: 25583.26529455185 seconds.\n",
      "Get Embedding...\n",
      "(118, 1024) (118,)\n",
      "SVC Accuracy: [0.7643939393939394, 0.781060606060606]\n",
      "+-----------------------------------------------------------------------------+\n",
      "Embedding Time: 14.504867553710938 seconds.\n",
      "Experient 21\n",
      "20211223-224753 :Log the record!\n",
      "+-----------------------------------------------------------------------------+\n",
      "Experient 22\n",
      "!!!\n",
      "89\n",
      "!!!\n",
      "[553, 217, 346, 850, 1139, 72, 280, 861, 916, 589, 50, 182, 307, 1067, 172, 583, 831, 791, 1014, 246, 18, 1013, 59, 1023, 984, 69, 755, 65, 71, 742, 1153, 256, 856, 266, 427, 956, 679, 826, 541, 147, 910, 0, 922, 683, 131, 103, 91, 1001, 166, 558, 796, 1078, 811, 852, 351, 1002, 383, 145, 197, 1072, 789, 17, 606, 9, 608, 1111, 664, 414, 154, 290, 954, 667, 959, 1097, 652, 339, 184, 368, 327, 506, 933, 370, 36, 768, 67, 1158, 93, 1123, 47, 624, 311, 883, 484, 702, 379, 316, 794, 822, 818, 55, 375, 1008, 605, 584, 920, 575, 986, 675, 82, 468, 1104, 202, 428, 566, 85, 550, 914, 1142, 545, 1145, 870, 1149, 332, 732, 185, 401, 359, 540, 714, 277, 785, 56, 45, 330, 1137, 564, 941, 703, 157, 1056, 1025, 325, 394, 360, 1077, 611, 1058, 1131, 830, 1081, 857, 1105, 364, 57, 233, 291, 1065, 926, 181, 231, 230, 808, 731, 678, 588, 929, 226, 537, 680, 38, 746, 595, 733, 244, 138, 1039, 441, 216, 762, 220, 284, 570, 1166, 1103, 117, 476, 261, 312, 232, 424, 149, 1110, 183, 651, 776, 522, 898, 381, 1100, 1088, 221, 1043, 596, 1084, 719, 669, 253, 715, 498, 35, 112, 495, 524, 548, 369, 735, 574, 580, 391, 167, 293, 942, 793, 273, 204, 1049, 474, 292, 973, 129, 1006, 402, 444, 975, 137, 1071, 647, 656, 1080, 1045, 333, 648, 555, 1127, 576, 486, 602, 989, 399, 3, 1030, 1016, 782, 153, 962, 421, 620, 1036, 264, 944, 848, 736, 717, 26, 1121, 978, 338, 398, 449, 46, 778, 877, 773, 633, 287, 465, 957, 828, 462, 800, 419, 902, 709, 953, 549, 997, 724, 78, 779, 1019, 2, 1050, 546, 1048, 195, 1061, 995, 1032, 10, 25, 450, 357, 1135, 254, 559, 612, 912, 965, 179, 94, 739, 470, 616, 298, 900, 224, 598, 301, 436, 1099, 234, 1060, 1053, 839, 815, 158, 804, 945, 89, 31, 210, 365, 993, 413, 225, 115, 618, 42, 567, 320, 1125, 644, 532, 337, 513, 168, 609, 243, 494, 186, 734, 1159, 193, 439, 305, 641, 628, 300, 521, 636, 948, 198, 994, 556, 96, 1130, 568, 22, 557, 1021, 132, 938, 707, 463, 723, 1152, 915, 711, 156, 631, 931, 457, 1066, 433, 666, 464, 459, 597, 816, 471, 725, 614, 1063, 397, 716, 684, 176, 227, 577, 1004, 801, 637, 34, 761, 302, 404, 1090, 980, 1028, 262, 161, 426, 751, 617, 1000, 39, 345, 504, 880, 907, 409, 173, 515, 249, 1069, 285, 1156, 730, 350, 260, 150, 281, 923, 361, 1062, 918, 1052, 904, 282, 1095, 219, 503, 738, 88, 668, 563, 710, 946, 207, 146, 60, 340, 760, 691, 106, 827, 265, 571, 15, 1132, 1015, 985, 825, 970, 758, 223, 406, 921, 1044, 772, 120, 247, 4, 542, 101, 859, 467, 1150, 12, 871, 268, 1012, 630, 1009, 336, 322, 621, 491, 1026, 187, 629, 432, 385, 992, 809, 1024, 756, 366, 62, 188, 53, 899, 950, 939, 1175, 988, 13, 105, 692, 659, 160, 960, 134, 875, 492, 107, 505, 757, 932, 653, 533, 175, 342, 314, 1165, 936, 1114, 252, 639, 8, 695, 838, 539, 824, 458, 408, 508, 248, 148, 352, 1162, 663, 911, 578, 318, 1116, 108, 1106, 235, 844, 54, 889, 645, 586, 1068, 239, 326, 763, 689, 104, 81, 323, 48, 1031, 908, 1041, 851, 1124, 452, 961, 122, 127, 1108, 388, 215, 155, 73, 887, 475, 77, 348, 485, 84, 1148, 650, 509, 75, 299, 331, 144, 241, 288, 727, 619, 775, 625, 206, 977, 455, 934, 697, 777, 865, 245, 835, 585, 133, 70, 655, 1161, 1092, 892, 190, 7, 32, 807, 110, 693, 1134, 1064, 295, 19, 872, 729, 1154, 289, 1176, 657, 142, 890, 445, 952, 275, 866, 587, 927, 603, 943, 211, 1118, 728, 593, 701, 477, 222, 196, 440, 205, 531, 981, 354, 478, 165, 242, 958, 1115, 1075, 1074, 1007, 24, 967, 913, 49, 744, 11, 554, 201, 321, 437, 517, 999, 814, 529, 681, 236, 180, 750, 780, 135, 868, 516, 879, 820, 315, 1128, 560, 218, 845, 297, 949, 841, 1059, 309, 1167, 1155, 665, 425, 1172, 461, 812, 79, 143, 622, 983, 937, 438, 353, 832, 6, 901, 795, 990, 759, 869, 1120, 276, 686, 810, 199, 371, 174, 677, 627, 116, 1133, 435, 519, 113, 496, 1029, 706, 255, 764, 130, 1005, 525, 500, 453, 367, 720, 171, 278, 483, 1122, 754, 698, 1051, 599, 1126, 74, 867, 726, 885, 250, 191, 1027, 229, 996, 310, 209, 884, 111, 456, 1082, 76, 358, 343, 66, 356, 213, 874, 1164, 362, 813, 1138, 5, 1055, 324, 888, 480, 849, 1033, 228, 267, 410, 786, 1035, 139, 873, 972, 740, 303, 951, 304, 1140, 543, 682, 382, 393, 896, 488, 718, 447, 1054, 417, 643, 43, 964, 14, 237, 189, 662, 29, 306, 1144, 119, 654, 86, 1107, 783, 511, 876, 1136, 696, 1146, 1086, 507, 341, 649, 771, 169, 274, 37, 376, 317, 279, 395, 840, 966, 251, 687, 878, 787, 159, 600, 1087, 905, 319, 1160, 378, 1094, 263, 52, 987, 344, 308, 363, 416, 396, 924, 769, 688, 33, 615, 460, 979, 906, 121, 895, 1171, 102, 579, 526, 882, 1168, 177, 523, 380, 1076, 864, 1085, 125, 674, 83, 781, 64, 124, 1047, 51, 20, 661, 767, 258, 151, 833, 638, 1109, 590, 635, 1169, 930, 372, 448, 982, 991, 846, 538, 374, 792, 446, 708, 528, 283, 819, 1089, 858, 1038, 806, 466, 493, 1174, 347, 114, 136, 128, 863, 935, 1093, 415, 269, 510, 1129, 834, 1, 389, 68, 749, 963, 552, 487, 430, 212, 971, 847, 745, 194, 162, 713, 296, 1011, 592, 582, 917, 594, 429, 853, 1073, 784, 520, 499, 469, 451, 1079, 572, 862, 753, 737, 27, 1117, 547, 947, 123, 894, 1112, 855, 384, 1034, 422, 490, 712, 843, 192, 1173, 203, 634, 514, 928, 694, 423, 141, 420, 329, 1017, 1119, 377, 1046, 527, 998, 349, 272, 454, 1057, 501, 178, 392, 405, 774, 472, 238, 1042, 534, 699, 482, 1018, 569, 842, 164, 660, 1113, 536, 817, 95, 240, 387, 854, 126, 561, 743, 431, 672, 802, 208, 766, 690, 390, 1101, 591, 604, 109, 909, 803, 28, 286, 765, 163, 601, 1163, 722, 90, 1177, 1037, 152, 1098, 805, 334, 294, 829, 700, 974, 21]\n",
      "Number of Graphs (dataset_train): 1060\n",
      "Number of Graphs (dataset_test): 118\n",
      "+-----------------------------------------------------------------------------+\n",
      "Learning Rate: 0.001\n",
      "Epochs: 200\n",
      "Batch Size: 128\n",
      "Hidden Dimension 64\n",
      "Number of Layer in Encoder: 3\n",
      "Opimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device: cuda\n",
      "Model:\n",
      "SimCLR(\n",
      "  (encoder): Encoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=89, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      ))\n",
      "      (1): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      ))\n",
      "      (2): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      ))\n",
      "    )\n",
      "    (bns): ModuleList(\n",
      "      (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Projection_MLP(\n",
      "    (layer1): Sequential(\n",
      "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (1): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (1): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (1): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (unilayer): Sequential(\n",
      "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=192, out_features=192, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Start Training...\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss = 106.96864861912198\n",
      "Epoch 2 | Loss = 65.88851756519742\n",
      "Epoch 3 | Loss = 49.25577470991347\n",
      "Epoch 4 | Loss = 39.267890294392906\n",
      "Epoch 5 | Loss = 34.65324534310235\n",
      "Epoch 6 | Loss = 30.006040626102024\n",
      "Epoch 7 | Loss = 26.59289863374498\n",
      "Epoch 8 | Loss = 23.33609528011746\n",
      "Epoch 9 | Loss = 20.99751255247328\n",
      "Epoch 10 | Loss = 18.37019877963596\n",
      "Epoch 11 | Loss = 17.000194390614826\n",
      "Epoch 12 | Loss = 15.478703710767958\n",
      "Epoch 13 | Loss = 13.92277897728814\n",
      "Epoch 14 | Loss = 12.959550751580132\n",
      "Epoch 15 | Loss = 12.27254549662272\n",
      "Epoch 16 | Loss = 10.928048451741537\n",
      "Epoch 17 | Loss = 9.541405147976345\n",
      "Epoch 18 | Loss = 8.820722632937962\n",
      "Epoch 19 | Loss = 7.907309108310276\n",
      "Epoch 20 | Loss = 7.043888303968641\n",
      "Epoch 21 | Loss = 6.511705663469103\n",
      "Epoch 22 | Loss = 6.195018185509576\n",
      "Epoch 23 | Loss = 5.572229915195042\n",
      "Epoch 24 | Loss = 4.742797374725342\n",
      "Epoch 25 | Loss = 4.045413229200575\n",
      "Epoch 26 | Loss = 3.928058624267578\n",
      "Epoch 27 | Loss = 2.965005291832818\n",
      "Epoch 28 | Loss = 2.7224702305263944\n",
      "Epoch 29 | Loss = 2.5404336187574597\n",
      "Epoch 30 | Loss = 1.7669996685451932\n",
      "Epoch 31 | Loss = 1.5341933568318684\n",
      "Epoch 32 | Loss = 1.1497601403130426\n",
      "Epoch 33 | Loss = 0.33320702446831596\n",
      "Epoch 34 | Loss = 0.1358793576558431\n",
      "Epoch 35 | Loss = 0.015117221408420138\n",
      "Epoch 36 | Loss = -0.6370183626810709\n",
      "Epoch 37 | Loss = -0.47498771879408097\n",
      "Epoch 38 | Loss = -0.5968324873182509\n",
      "Epoch 39 | Loss = -0.7583712471856011\n",
      "Epoch 40 | Loss = -1.2155742115444608\n",
      "Epoch 41 | Loss = -1.5451440811157227\n",
      "Epoch 42 | Loss = -1.8752444320254855\n",
      "Epoch 43 | Loss = -1.849007977379693\n",
      "Epoch 44 | Loss = -2.0167805353800454\n",
      "Epoch 45 | Loss = -2.6585057046678333\n",
      "Epoch 46 | Loss = -2.8789461188846164\n",
      "Epoch 47 | Loss = -2.9221553007761636\n",
      "Epoch 48 | Loss = -3.38586695988973\n",
      "Epoch 49 | Loss = -3.403869522942437\n",
      "Epoch 50 | Loss = -3.4337126943800182\n",
      "Epoch 51 | Loss = -3.768198755052355\n",
      "Epoch 52 | Loss = -3.8228749566608005\n",
      "Epoch 53 | Loss = -4.268050326241387\n",
      "Epoch 54 | Loss = -4.770020908779568\n",
      "Epoch 55 | Loss = -4.531961970859104\n",
      "Epoch 56 | Loss = -4.677936103608873\n",
      "Epoch 57 | Loss = -4.789279960923725\n",
      "Epoch 58 | Loss = -5.1195513440503015\n",
      "Epoch 59 | Loss = -4.961604005760616\n",
      "Epoch 60 | Loss = -5.4121153354644775\n",
      "Epoch 61 | Loss = -5.421454522344801\n",
      "Epoch 63 | Loss = -5.9200044473012285\n",
      "Epoch 64 | Loss = -5.969038801060782\n",
      "Epoch 65 | Loss = -5.586752812067668\n",
      "Epoch 66 | Loss = -5.620454748471578\n",
      "Epoch 67 | Loss = -6.10620151211818\n",
      "Epoch 68 | Loss = -6.089889076020983\n",
      "Epoch 69 | Loss = -6.255939527518219\n",
      "Epoch 70 | Loss = -6.628816780116823\n",
      "Epoch 71 | Loss = -6.571334848801295\n",
      "Epoch 72 | Loss = -6.730220927132501\n",
      "Epoch 73 | Loss = -6.9572266472710504\n",
      "Epoch 74 | Loss = -7.158550745911068\n",
      "Epoch 75 | Loss = -6.805734167496364\n",
      "Epoch 76 | Loss = -7.198875771628486\n",
      "Epoch 77 | Loss = -7.365513198905521\n",
      "Epoch 78 | Loss = -7.093554705381393\n",
      "Epoch 79 | Loss = -7.31766183508767\n",
      "Epoch 80 | Loss = -7.666503065162235\n",
      "Epoch 81 | Loss = -7.865480369991726\n",
      "Epoch 82 | Loss = -7.610801282856199\n",
      "Epoch 83 | Loss = -7.701339628961351\n",
      "Epoch 84 | Loss = -7.614715364244249\n",
      "Epoch 85 | Loss = -8.039621273676554\n",
      "Epoch 86 | Loss = -8.100333915816414\n",
      "Epoch 87 | Loss = -8.404166671964857\n",
      "Epoch 88 | Loss = -8.426426808039347\n",
      "Epoch 89 | Loss = -8.563505702548557\n",
      "Epoch 90 | Loss = -8.1474304066764\n",
      "Epoch 91 | Loss = -8.708971208996243\n",
      "Epoch 92 | Loss = -8.636720180511475\n",
      "Epoch 93 | Loss = -8.72516745991177\n",
      "Epoch 94 | Loss = -8.620526340272692\n",
      "Epoch 95 | Loss = -8.933764113320244\n",
      "Epoch 96 | Loss = -8.915145529641045\n",
      "Epoch 97 | Loss = -8.89794789420234\n",
      "Epoch 98 | Loss = -8.895168251461453\n",
      "Epoch 99 | Loss = -9.142507155736288\n",
      "Epoch 100 | Loss = -9.11079994837443\n",
      "Epoch 101 | Loss = -9.439259582095676\n",
      "Epoch 102 | Loss = -9.17881965637207\n",
      "Epoch 103 | Loss = -9.530793110529581\n",
      "Epoch 104 | Loss = -9.521064387427437\n",
      "Epoch 105 | Loss = -9.32019329071045\n",
      "Epoch 106 | Loss = -9.702193154229057\n",
      "Epoch 107 | Loss = -9.585536479949951\n",
      "Epoch 108 | Loss = -9.540501382615831\n",
      "Epoch 109 | Loss = -9.50764897134569\n",
      "Epoch 110 | Loss = -9.783036708831787\n",
      "Epoch 111 | Loss = -9.774416579140556\n",
      "Epoch 112 | Loss = -10.297006871965197\n",
      "Epoch 113 | Loss = -10.00281678305732\n",
      "Epoch 114 | Loss = -10.108493089675903\n",
      "Epoch 115 | Loss = -10.39314768049452\n",
      "Epoch 116 | Loss = -10.14150791698032\n",
      "Epoch 117 | Loss = -10.220994790395102\n",
      "Epoch 118 | Loss = -10.356004238128662\n",
      "Epoch 119 | Loss = -10.488294124603271\n",
      "Epoch 120 | Loss = -10.388513141208225\n",
      "Epoch 121 | Loss = -10.752478387620714\n",
      "Epoch 122 | Loss = -10.858588324652779\n",
      "Epoch 123 | Loss = -10.804936832851833\n",
      "Epoch 124 | Loss = -10.461516910129124\n",
      "Epoch 125 | Loss = -10.80114354027642\n",
      "Epoch 126 | Loss = -10.511625607808432\n",
      "Epoch 127 | Loss = -10.89488278494941\n",
      "Epoch 128 | Loss = -10.74023691813151\n",
      "Epoch 129 | Loss = -11.052694267696804\n",
      "Epoch 130 | Loss = -11.147836473253038\n",
      "Epoch 131 | Loss = -10.942835066053602\n",
      "Epoch 132 | Loss = -10.739988009134928\n",
      "Epoch 133 | Loss = -10.818725374009874\n",
      "Epoch 134 | Loss = -11.10193003548516\n",
      "Epoch 135 | Loss = -11.165718873341879\n",
      "Epoch 136 | Loss = -11.218943277994791\n",
      "Epoch 137 | Loss = -11.14154307047526\n",
      "Epoch 138 | Loss = -11.368115001254612\n",
      "Epoch 139 | Loss = -11.233183330959744\n",
      "Epoch 140 | Loss = -11.167305575476753\n",
      "Epoch 141 | Loss = -11.085135989718967\n",
      "Epoch 142 | Loss = -11.268440617455376\n",
      "Epoch 143 | Loss = -11.211750401390923\n",
      "Epoch 144 | Loss = -11.452771663665771\n",
      "Epoch 145 | Loss = -11.591234472062853\n",
      "Epoch 146 | Loss = -11.573597219255236\n",
      "Epoch 147 | Loss = -11.81605413224962\n",
      "Epoch 148 | Loss = -11.813591268327501\n",
      "Epoch 149 | Loss = -11.819705592261421\n",
      "Epoch 150 | Loss = -11.799587355719673\n",
      "Epoch 151 | Loss = -11.886393441094292\n",
      "Epoch 152 | Loss = -11.968890402052137\n",
      "Epoch 153 | Loss = -12.001307911343044\n",
      "Epoch 154 | Loss = -12.119894451565212\n",
      "Epoch 155 | Loss = -12.183736748165554\n",
      "Epoch 156 | Loss = -12.22943671544393\n",
      "Epoch 157 | Loss = -11.987505912780762\n",
      "Epoch 158 | Loss = -12.106378290388319\n",
      "Epoch 159 | Loss = -12.157900545332167\n",
      "Epoch 160 | Loss = -12.220216539171007\n",
      "Epoch 161 | Loss = -12.44281726413303\n",
      "Epoch 162 | Loss = -12.379395220014784\n",
      "Epoch 163 | Loss = -12.299667093488905\n",
      "Epoch 164 | Loss = -12.260549598269993\n",
      "Epoch 165 | Loss = -12.69774145550198\n",
      "Epoch 166 | Loss = -12.46733464135064\n",
      "Epoch 167 | Loss = -12.48241795433892\n",
      "Epoch 168 | Loss = -12.429831610785591\n",
      "Epoch 169 | Loss = -12.47801431020101\n",
      "Epoch 170 | Loss = -12.636812157101101\n",
      "Epoch 171 | Loss = -12.46834659576416\n",
      "Epoch 172 | Loss = -12.521950880686441\n",
      "Epoch 173 | Loss = -12.675092485215929\n",
      "Epoch 174 | Loss = -12.83579773373074\n",
      "Epoch 175 | Loss = -12.573441452450222\n",
      "Epoch 176 | Loss = -12.598782433403862\n",
      "Epoch 177 | Loss = -12.70270930396186\n",
      "Epoch 178 | Loss = -12.710807694329155\n",
      "Epoch 179 | Loss = -12.837319956885445\n",
      "Epoch 180 | Loss = -12.962980959150526\n",
      "Epoch 181 | Loss = -13.108241081237793\n",
      "Epoch 182 | Loss = -13.249181111653646\n",
      "Epoch 183 | Loss = -13.18823332256741\n",
      "Epoch 184 | Loss = -13.270758575863308\n",
      "Epoch 185 | Loss = -12.933160728878445\n",
      "Epoch 186 | Loss = -13.065064059363472\n",
      "Epoch 187 | Loss = -13.396788067287869\n",
      "Epoch 188 | Loss = -13.15146991941664\n",
      "Epoch 189 | Loss = -13.38426817788018\n",
      "Epoch 190 | Loss = -13.498653835720486\n",
      "Epoch 191 | Loss = -13.54571262995402\n",
      "Epoch 192 | Loss = -13.324333349863688\n",
      "Epoch 193 | Loss = -13.446475505828857\n",
      "Epoch 194 | Loss = -13.437439971499973\n",
      "Epoch 195 | Loss = -13.273605982462565\n",
      "Epoch 196 | Loss = -13.328151120079887\n",
      "Epoch 197 | Loss = -13.434434996710884\n",
      "Epoch 198 | Loss = -13.410633616977268\n",
      "Epoch 199 | Loss = -13.41925377315945\n",
      "Epoch 200 | Loss = -13.490271144443089\n",
      "+-----------------------------------------------------------------------------+\n",
      "Training Time: 25062.315225601196 seconds.\n",
      "Get Embedding...\n",
      "(118, 192) (118,)\n",
      "SVC Accuracy: [0.7643939393939394, 0.7545454545454546]\n",
      "+-----------------------------------------------------------------------------+\n",
      "Embedding Time: 12.811692237854004 seconds.\n",
      "Experient 22\n",
      "20211224-054553 :Log the record!\n",
      "+-----------------------------------------------------------------------------+\n",
      "Experient 23\n",
      "!!!\n",
      "89\n",
      "!!!\n",
      "[553, 217, 346, 850, 1139, 72, 280, 861, 916, 589, 50, 182, 307, 1067, 172, 583, 831, 791, 1014, 246, 18, 1013, 59, 1023, 984, 69, 755, 65, 71, 742, 1153, 256, 856, 266, 427, 956, 679, 826, 541, 147, 910, 0, 922, 683, 131, 103, 91, 1001, 166, 558, 796, 1078, 811, 852, 351, 1002, 383, 145, 197, 1072, 789, 17, 606, 9, 608, 1111, 664, 414, 154, 290, 954, 667, 959, 1097, 652, 339, 184, 368, 327, 506, 933, 370, 36, 768, 67, 1158, 93, 1123, 47, 624, 311, 883, 484, 702, 379, 316, 794, 822, 818, 55, 375, 1008, 605, 584, 920, 575, 986, 675, 82, 468, 1104, 202, 428, 566, 85, 550, 914, 1142, 545, 1145, 870, 1149, 332, 732, 185, 401, 359, 540, 714, 277, 785, 56, 45, 330, 1137, 564, 941, 703, 157, 1056, 1025, 325, 394, 360, 1077, 611, 1058, 1131, 830, 1081, 857, 1105, 364, 57, 233, 291, 1065, 926, 181, 231, 230, 808, 731, 678, 588, 929, 226, 537, 680, 38, 746, 595, 733, 244, 138, 1039, 441, 216, 762, 220, 284, 570, 1166, 1103, 117, 476, 261, 312, 232, 424, 149, 1110, 183, 651, 776, 522, 898, 381, 1100, 1088, 221, 1043, 596, 1084, 719, 669, 253, 715, 498, 35, 112, 495, 524, 548, 369, 735, 574, 580, 391, 167, 293, 942, 793, 273, 204, 1049, 474, 292, 973, 129, 1006, 402, 444, 975, 137, 1071, 647, 656, 1080, 1045, 333, 648, 555, 1127, 576, 486, 602, 989, 399, 3, 1030, 1016, 782, 153, 962, 421, 620, 1036, 264, 944, 848, 736, 717, 26, 1121, 978, 338, 398, 449, 46, 778, 877, 773, 633, 287, 465, 957, 828, 462, 800, 419, 902, 709, 953, 549, 997, 724, 78, 779, 1019, 2, 1050, 546, 1048, 195, 1061, 995, 1032, 10, 25, 450, 357, 1135, 254, 559, 612, 912, 965, 179, 94, 739, 470, 616, 298, 900, 224, 598, 301, 436, 1099, 234, 1060, 1053, 839, 815, 158, 804, 945, 89, 31, 210, 365, 993, 413, 225, 115, 618, 42, 567, 320, 1125, 644, 532, 337, 513, 168, 609, 243, 494, 186, 734, 1159, 193, 439, 305, 641, 628, 300, 521, 636, 948, 198, 994, 556, 96, 1130, 568, 22, 557, 1021, 132, 938, 707, 463, 723, 1152, 915, 711, 156, 631, 931, 457, 1066, 433, 666, 464, 459, 597, 816, 471, 725, 614, 1063, 397, 716, 684, 176, 227, 577, 1004, 801, 637, 34, 761, 302, 404, 1090, 980, 1028, 262, 161, 426, 751, 617, 1000, 39, 345, 504, 880, 907, 409, 173, 515, 249, 1069, 285, 1156, 730, 350, 260, 150, 281, 923, 361, 1062, 918, 1052, 904, 282, 1095, 219, 503, 738, 88, 668, 563, 710, 946, 207, 146, 60, 340, 760, 691, 106, 827, 265, 571, 15, 1132, 1015, 985, 825, 970, 758, 223, 406, 921, 1044, 772, 120, 247, 4, 542, 101, 859, 467, 1150, 12, 871, 268, 1012, 630, 1009, 336, 322, 621, 491, 1026, 187, 629, 432, 385, 992, 809, 1024, 756, 366, 62, 188, 53, 899, 950, 939, 1175, 988, 13, 105, 692, 659, 160, 960, 134, 875, 492, 107, 505, 757, 932, 653, 533, 175, 342, 314, 1165, 936, 1114, 252, 639, 8, 695, 838, 539, 824, 458, 408, 508, 248, 148, 352, 1162, 663, 911, 578, 318, 1116, 108, 1106, 235, 844, 54, 889, 645, 586, 1068, 239, 326, 763, 689, 104, 81, 323, 48, 1031, 908, 1041, 851, 1124, 452, 961, 122, 127, 1108, 388, 215, 155, 73, 887, 475, 77, 348, 485, 84, 1148, 650, 509, 75, 299, 331, 144, 241, 288, 727, 619, 775, 625, 206, 977, 455, 934, 697, 777, 865, 245, 835, 585, 133, 70, 655, 1161, 1092, 892, 190, 7, 32, 807, 110, 693, 1134, 1064, 295, 19, 872, 729, 1154, 289, 1176, 657, 142, 890, 445, 952, 275, 866, 587, 927, 603, 943, 211, 1118, 728, 593, 701, 477, 222, 196, 440, 205, 531, 981, 354, 478, 165, 242, 958, 1115, 1075, 1074, 1007, 24, 967, 913, 49, 744, 11, 554, 201, 321, 437, 517, 999, 814, 529, 681, 236, 180, 750, 780, 135, 868, 516, 879, 820, 315, 1128, 560, 218, 845, 297, 949, 841, 1059, 309, 1167, 1155, 665, 425, 1172, 461, 812, 79, 143, 622, 983, 937, 438, 353, 832, 6, 901, 795, 990, 759, 869, 1120, 276, 686, 810, 199, 371, 174, 677, 627, 116, 1133, 435, 519, 113, 496, 1029, 706, 255, 764, 130, 1005, 525, 500, 453, 367, 720, 171, 278, 483, 1122, 754, 698, 1051, 599, 1126, 74, 867, 726, 885, 250, 191, 1027, 229, 996, 310, 209, 884, 111, 456, 1082, 76, 358, 343, 66, 356, 213, 874, 1164, 362, 813, 1138, 5, 1055, 324, 888, 480, 849, 1033, 228, 267, 410, 786, 1035, 139, 873, 972, 740, 303, 951, 304, 1140, 543, 682, 382, 393, 896, 488, 718, 447, 1054, 417, 643, 43, 964, 14, 237, 189, 662, 29, 306, 1144, 119, 654, 86, 1107, 783, 511, 876, 1136, 696, 1146, 1086, 507, 341, 649, 771, 169, 274, 37, 376, 317, 279, 395, 840, 966, 251, 687, 878, 787, 159, 600, 1087, 905, 319, 1160, 378, 1094, 263, 52, 987, 344, 308, 363, 416, 396, 924, 769, 688, 33, 615, 460, 979, 906, 121, 895, 1171, 102, 579, 526, 882, 1168, 177, 523, 380, 1076, 864, 1085, 125, 674, 83, 781, 64, 124, 1047, 51, 20, 661, 767, 258, 151, 833, 638, 1109, 590, 635, 1169, 930, 372, 448, 982, 991, 846, 538, 374, 792, 446, 708, 528, 283, 819, 1089, 858, 1038, 806, 466, 493, 1174, 347, 114, 136, 128, 863, 935, 1093, 415, 269, 510, 1129, 834, 1, 389, 68, 749, 963, 552, 487, 430, 212, 971, 847, 745, 194, 162, 713, 296, 1011, 592, 582, 917, 594, 429, 853, 1073, 784, 520, 499, 469, 451, 1079, 572, 862, 753, 737, 27, 1117, 547, 947, 123, 894, 1112, 855, 384, 1034, 422, 490, 712, 843, 192, 1173, 203, 634, 514, 928, 694, 423, 141, 420, 329, 1017, 1119, 377, 1046, 527, 998, 349, 272, 454, 1057, 501, 178, 392, 405, 774, 472, 238, 1042, 534, 699, 482, 1018, 569, 842, 164, 660, 1113, 536, 817, 95, 240, 387, 854, 126, 561, 743, 431, 672, 802, 208, 766, 690, 390, 1101, 591, 604, 109, 909, 803, 28, 286, 765, 163, 601, 1163, 722, 90, 1177, 1037, 152, 1098, 805, 334, 294, 829, 700, 974, 21]\n",
      "Number of Graphs (dataset_train): 1060\n",
      "Number of Graphs (dataset_test): 118\n",
      "+-----------------------------------------------------------------------------+\n",
      "Learning Rate: 0.001\n",
      "Epochs: 200\n",
      "Batch Size: 128\n",
      "Hidden Dimension 512\n",
      "Number of Layer in Encoder: 3\n",
      "Opimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device: cuda\n",
      "Model:\n",
      "SimCLR(\n",
      "  (encoder): Encoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=89, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      ))\n",
      "      (1): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      ))\n",
      "      (2): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      ))\n",
      "    )\n",
      "    (bns): ModuleList(\n",
      "      (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Projection_MLP(\n",
      "    (layer1): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (unilayer): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Start Training...\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss = 65.49983098771837\n",
      "Epoch 2 | Loss = 39.240423149532745\n",
      "Epoch 3 | Loss = 25.526357544793022\n",
      "Epoch 4 | Loss = 16.503506872389053\n",
      "Epoch 5 | Loss = 8.807073540157742\n",
      "Epoch 6 | Loss = 3.9880403412712946\n",
      "Epoch 7 | Loss = 0.262131479051378\n",
      "Epoch 8 | Loss = -2.51572502983941\n",
      "Epoch 9 | Loss = -4.227730280823177\n",
      "Epoch 10 | Loss = -6.4838324950801\n",
      "Epoch 11 | Loss = -7.243477463722229\n",
      "Epoch 12 | Loss = -8.613619989818996\n",
      "Epoch 13 | Loss = -9.554359118143717\n",
      "Epoch 14 | Loss = -10.186663150787354\n",
      "Epoch 15 | Loss = -10.829610453711616\n",
      "Epoch 16 | Loss = -11.727242204878065\n",
      "Epoch 17 | Loss = -11.86448785993788\n",
      "Epoch 18 | Loss = -12.632768472035727\n",
      "Epoch 19 | Loss = -13.12519031100803\n",
      "Epoch 20 | Loss = -13.849408785502115\n",
      "Epoch 21 | Loss = -14.478107664320204\n",
      "Epoch 22 | Loss = -15.179456869761148\n",
      "Epoch 23 | Loss = -15.557445314195421\n",
      "Epoch 24 | Loss = -15.881510734558105\n",
      "Epoch 25 | Loss = -16.203157160017227\n",
      "Epoch 26 | Loss = -16.544703589545357\n",
      "Epoch 27 | Loss = -16.742527961730957\n",
      "Epoch 28 | Loss = -16.906197388966877\n",
      "Epoch 29 | Loss = -17.291481971740723\n",
      "Epoch 30 | Loss = -17.58859618504842\n",
      "Epoch 31 | Loss = -17.80810350841946\n",
      "Epoch 32 | Loss = -18.19318093193902\n",
      "Epoch 33 | Loss = -18.28464725282457\n",
      "Epoch 34 | Loss = -18.58033100763957\n",
      "Epoch 35 | Loss = -18.916191048092312\n",
      "Epoch 36 | Loss = -19.175001939137776\n",
      "Epoch 37 | Loss = -19.540111700693767\n",
      "Epoch 38 | Loss = -19.851800865597195\n",
      "Epoch 39 | Loss = -19.96357838312785\n",
      "Epoch 40 | Loss = -20.040827751159668\n",
      "Epoch 41 | Loss = -20.081776989830864\n",
      "Epoch 42 | Loss = -20.358046743604874\n",
      "Epoch 43 | Loss = -20.528422143724228\n",
      "Epoch 44 | Loss = -20.58947292963664\n",
      "Epoch 45 | Loss = -20.863183604346382\n",
      "Epoch 46 | Loss = -20.894730514950222\n",
      "Epoch 47 | Loss = -21.05148198869493\n",
      "Epoch 48 | Loss = -21.301669862535263\n",
      "Epoch 49 | Loss = -21.45353110631307\n",
      "Epoch 50 | Loss = -21.572063128153484\n",
      "Epoch 51 | Loss = -21.793605751461453\n",
      "Epoch 52 | Loss = -21.901592201656765\n",
      "Epoch 53 | Loss = -22.015770806206596\n",
      "Epoch 54 | Loss = -22.164940039316814\n",
      "Epoch 55 | Loss = -22.261768182118733\n",
      "Epoch 56 | Loss = -22.29585705863105\n",
      "Epoch 57 | Loss = -22.37123531765408\n",
      "Epoch 58 | Loss = -22.479456371731228\n",
      "Epoch 59 | Loss = -22.547669410705566\n",
      "Epoch 60 | Loss = -22.65510235892402\n",
      "Epoch 61 | Loss = -22.720529821183945\n",
      "Epoch 62 | Loss = -22.927420245276558\n",
      "Epoch 63 | Loss = -23.086793263753254\n",
      "Epoch 64 | Loss = -23.061039023929172\n",
      "Epoch 65 | Loss = -23.109404404958088\n",
      "Epoch 66 | Loss = -23.1926433245341\n",
      "Epoch 67 | Loss = -23.3658447265625\n",
      "Epoch 68 | Loss = -23.35585615370009\n",
      "Epoch 69 | Loss = -23.43486155403985\n",
      "Epoch 70 | Loss = -23.498462200164795\n",
      "Epoch 71 | Loss = -23.561649958292644\n",
      "Epoch 72 | Loss = -23.577018154991997\n",
      "Epoch 73 | Loss = -23.612449645996094\n",
      "Epoch 74 | Loss = -23.629885567559135\n",
      "Epoch 75 | Loss = -23.75085989634196\n",
      "Epoch 76 | Loss = -23.915815777248806\n",
      "Epoch 77 | Loss = -23.92147397994995\n",
      "Epoch 78 | Loss = -23.97944074206882\n",
      "Epoch 79 | Loss = -24.07320017284817\n",
      "Epoch 80 | Loss = -24.138929737938774\n",
      "Epoch 81 | Loss = -24.123173183865017\n",
      "Epoch 82 | Loss = -24.145534886254204\n",
      "Epoch 83 | Loss = -24.26306496726142\n",
      "Epoch 84 | Loss = -24.42150772942437\n",
      "Epoch 85 | Loss = -24.377783563401962\n",
      "Epoch 86 | Loss = -24.447604762183296\n",
      "Epoch 87 | Loss = -24.463892724778916\n",
      "Epoch 88 | Loss = -24.238243367936875\n",
      "Epoch 89 | Loss = -24.26090908050537\n",
      "Epoch 90 | Loss = -24.44096194373237\n",
      "Epoch 91 | Loss = -24.627860016292995\n",
      "Epoch 92 | Loss = -24.589190642038982\n",
      "Epoch 93 | Loss = -24.69478925069173\n",
      "Epoch 94 | Loss = -24.649512343936497\n",
      "Epoch 95 | Loss = -24.68659978442722\n",
      "Epoch 96 | Loss = -24.772182570563423\n",
      "Epoch 97 | Loss = -24.807867950863308\n",
      "Epoch 98 | Loss = -24.905946837531197\n",
      "Epoch 99 | Loss = -25.05430089102851\n",
      "Epoch 100 | Loss = -25.07404687669542\n",
      "Epoch 101 | Loss = -24.95720402399699\n",
      "Epoch 102 | Loss = -24.854494518703884\n",
      "Epoch 103 | Loss = -25.039360523223877\n",
      "Epoch 104 | Loss = -25.07884947458903\n",
      "Epoch 105 | Loss = -25.122878392537434\n",
      "Epoch 106 | Loss = -25.210577964782715\n",
      "Epoch 107 | Loss = -25.29966444439358\n",
      "Epoch 108 | Loss = -25.275833818647598\n",
      "Epoch 109 | Loss = -25.299947367774116\n",
      "Epoch 110 | Loss = -25.3352788289388\n",
      "Epoch 111 | Loss = -25.393478552500408\n",
      "Epoch 112 | Loss = -25.404249138302273\n",
      "Epoch 113 | Loss = -25.54508564207289\n",
      "Epoch 114 | Loss = -25.52849547068278\n",
      "Epoch 115 | Loss = -25.58607080247667\n",
      "Epoch 116 | Loss = -25.624945375654434\n",
      "Epoch 117 | Loss = -25.695561091105144\n",
      "Epoch 118 | Loss = -25.538568761613632\n",
      "Epoch 119 | Loss = -25.62053108215332\n",
      "Epoch 120 | Loss = -25.395869149102104\n",
      "Epoch 121 | Loss = -25.321977880265976\n",
      "Epoch 122 | Loss = -25.559934351179336\n",
      "Epoch 123 | Loss = -25.61033058166504\n",
      "Epoch 124 | Loss = -25.566932519276936\n",
      "Epoch 125 | Loss = -25.44085513220893\n",
      "Epoch 126 | Loss = -25.699314064449734\n",
      "Epoch 127 | Loss = -25.741434468163384\n",
      "Epoch 128 | Loss = -25.763973077138264\n",
      "Epoch 129 | Loss = -25.739550749460857\n",
      "Epoch 130 | Loss = -25.8104952706231\n",
      "Epoch 131 | Loss = -25.82639455795288\n",
      "Epoch 132 | Loss = -25.885074138641357\n",
      "Epoch 133 | Loss = -25.91522545284695\n",
      "Epoch 134 | Loss = -26.03048912684123\n",
      "Epoch 135 | Loss = -26.039238929748535\n",
      "Epoch 136 | Loss = -26.023198710547554\n",
      "Epoch 137 | Loss = -25.944172435336643\n",
      "Epoch 138 | Loss = -25.93933868408203\n",
      "Epoch 139 | Loss = -25.95291068818834\n",
      "Epoch 140 | Loss = -25.758180565304226\n",
      "Epoch 141 | Loss = -25.171959082285564\n",
      "Epoch 142 | Loss = -25.33031638463338\n",
      "Epoch 143 | Loss = -25.557819843292236\n",
      "Epoch 144 | Loss = -25.7559249136183\n",
      "Epoch 145 | Loss = -25.884684562683105\n",
      "Epoch 146 | Loss = -26.013831032647026\n",
      "Epoch 147 | Loss = -26.049042966630722\n",
      "Epoch 148 | Loss = -26.150961293114555\n",
      "Epoch 149 | Loss = -26.129277229309082\n",
      "Epoch 150 | Loss = -26.229001892937553\n",
      "Epoch 151 | Loss = -26.178181065453423\n",
      "Epoch 152 | Loss = -26.19842873679267\n",
      "Epoch 153 | Loss = -26.25910520553589\n",
      "Epoch 154 | Loss = -26.29069487253825\n",
      "Epoch 155 | Loss = -26.349070178137886\n",
      "Epoch 156 | Loss = -26.355632146199543\n",
      "Epoch 157 | Loss = -26.355817529890274\n",
      "Epoch 158 | Loss = -26.386913723415798\n",
      "Epoch 159 | Loss = -26.419844839308\n",
      "Epoch 160 | Loss = -26.44111590915256\n",
      "Epoch 161 | Loss = -26.418922265370686\n",
      "Epoch 162 | Loss = -26.32502359814114\n",
      "Epoch 163 | Loss = -26.270274957021076\n",
      "Epoch 164 | Loss = -26.24407566918267\n",
      "Epoch 165 | Loss = -26.32056294547187\n",
      "Epoch 166 | Loss = -26.38295777638753\n",
      "Epoch 167 | Loss = -26.4007904264662\n",
      "Epoch 168 | Loss = -26.412132740020752\n",
      "Epoch 169 | Loss = -26.466917090945774\n",
      "Epoch 170 | Loss = -26.494873364766438\n",
      "Epoch 171 | Loss = -26.51507266362508\n",
      "Epoch 172 | Loss = -26.589571052127415\n",
      "Epoch 173 | Loss = -26.586866060892742\n",
      "Epoch 174 | Loss = -26.64905834197998\n",
      "Epoch 175 | Loss = -26.663131607903374\n",
      "Epoch 176 | Loss = -26.684300157758926\n",
      "Epoch 177 | Loss = -26.70940176645915\n",
      "Epoch 178 | Loss = -26.737417115105522\n",
      "Epoch 179 | Loss = -26.74257426791721\n",
      "Epoch 180 | Loss = -26.71755774815877\n",
      "Epoch 181 | Loss = -26.745038403405083\n",
      "Epoch 182 | Loss = -26.77301369773017\n",
      "Epoch 183 | Loss = -26.767194959852432\n",
      "Epoch 184 | Loss = -26.74771499633789\n",
      "Epoch 185 | Loss = -26.752122031317818\n",
      "Epoch 186 | Loss = -26.79927847120497\n",
      "Epoch 187 | Loss = -26.752103593614365\n",
      "Epoch 188 | Loss = -26.781464099884033\n",
      "Epoch 189 | Loss = -26.85112264421251\n",
      "Epoch 190 | Loss = -26.827159351772732\n",
      "Epoch 191 | Loss = -26.869952784644234\n",
      "Epoch 192 | Loss = -26.88142728805542\n",
      "Epoch 193 | Loss = -26.842868010203045\n",
      "Epoch 194 | Loss = -26.722105291154648\n",
      "Epoch 195 | Loss = -26.865957842932808\n",
      "Epoch 196 | Loss = -26.899616718292236\n",
      "Epoch 197 | Loss = -26.92055590947469\n",
      "Epoch 198 | Loss = -26.927337858412002\n",
      "Epoch 199 | Loss = -26.966337574852837\n",
      "Epoch 200 | Loss = -26.894479327731663\n",
      "+-----------------------------------------------------------------------------+\n",
      "Training Time: 25775.802569627762 seconds.\n",
      "Get Embedding...\n",
      "(118, 1536) (118,)\n",
      "SVC Accuracy: [0.7530303030303032, 0.7795454545454545]\n",
      "+-----------------------------------------------------------------------------+\n",
      "Embedding Time: 15.260961294174194 seconds.\n",
      "Experient 23\n",
      "20211224-125545 :Log the record!\n",
      "+-----------------------------------------------------------------------------+\n",
      "Experient 24\n",
      "!!!\n",
      "89\n",
      "!!!\n",
      "[553, 217, 346, 850, 1139, 72, 280, 861, 916, 589, 50, 182, 307, 1067, 172, 583, 831, 791, 1014, 246, 18, 1013, 59, 1023, 984, 69, 755, 65, 71, 742, 1153, 256, 856, 266, 427, 956, 679, 826, 541, 147, 910, 0, 922, 683, 131, 103, 91, 1001, 166, 558, 796, 1078, 811, 852, 351, 1002, 383, 145, 197, 1072, 789, 17, 606, 9, 608, 1111, 664, 414, 154, 290, 954, 667, 959, 1097, 652, 339, 184, 368, 327, 506, 933, 370, 36, 768, 67, 1158, 93, 1123, 47, 624, 311, 883, 484, 702, 379, 316, 794, 822, 818, 55, 375, 1008, 605, 584, 920, 575, 986, 675, 82, 468, 1104, 202, 428, 566, 85, 550, 914, 1142, 545, 1145, 870, 1149, 332, 732, 185, 401, 359, 540, 714, 277, 785, 56, 45, 330, 1137, 564, 941, 703, 157, 1056, 1025, 325, 394, 360, 1077, 611, 1058, 1131, 830, 1081, 857, 1105, 364, 57, 233, 291, 1065, 926, 181, 231, 230, 808, 731, 678, 588, 929, 226, 537, 680, 38, 746, 595, 733, 244, 138, 1039, 441, 216, 762, 220, 284, 570, 1166, 1103, 117, 476, 261, 312, 232, 424, 149, 1110, 183, 651, 776, 522, 898, 381, 1100, 1088, 221, 1043, 596, 1084, 719, 669, 253, 715, 498, 35, 112, 495, 524, 548, 369, 735, 574, 580, 391, 167, 293, 942, 793, 273, 204, 1049, 474, 292, 973, 129, 1006, 402, 444, 975, 137, 1071, 647, 656, 1080, 1045, 333, 648, 555, 1127, 576, 486, 602, 989, 399, 3, 1030, 1016, 782, 153, 962, 421, 620, 1036, 264, 944, 848, 736, 717, 26, 1121, 978, 338, 398, 449, 46, 778, 877, 773, 633, 287, 465, 957, 828, 462, 800, 419, 902, 709, 953, 549, 997, 724, 78, 779, 1019, 2, 1050, 546, 1048, 195, 1061, 995, 1032, 10, 25, 450, 357, 1135, 254, 559, 612, 912, 965, 179, 94, 739, 470, 616, 298, 900, 224, 598, 301, 436, 1099, 234, 1060, 1053, 839, 815, 158, 804, 945, 89, 31, 210, 365, 993, 413, 225, 115, 618, 42, 567, 320, 1125, 644, 532, 337, 513, 168, 609, 243, 494, 186, 734, 1159, 193, 439, 305, 641, 628, 300, 521, 636, 948, 198, 994, 556, 96, 1130, 568, 22, 557, 1021, 132, 938, 707, 463, 723, 1152, 915, 711, 156, 631, 931, 457, 1066, 433, 666, 464, 459, 597, 816, 471, 725, 614, 1063, 397, 716, 684, 176, 227, 577, 1004, 801, 637, 34, 761, 302, 404, 1090, 980, 1028, 262, 161, 426, 751, 617, 1000, 39, 345, 504, 880, 907, 409, 173, 515, 249, 1069, 285, 1156, 730, 350, 260, 150, 281, 923, 361, 1062, 918, 1052, 904, 282, 1095, 219, 503, 738, 88, 668, 563, 710, 946, 207, 146, 60, 340, 760, 691, 106, 827, 265, 571, 15, 1132, 1015, 985, 825, 970, 758, 223, 406, 921, 1044, 772, 120, 247, 4, 542, 101, 859, 467, 1150, 12, 871, 268, 1012, 630, 1009, 336, 322, 621, 491, 1026, 187, 629, 432, 385, 992, 809, 1024, 756, 366, 62, 188, 53, 899, 950, 939, 1175, 988, 13, 105, 692, 659, 160, 960, 134, 875, 492, 107, 505, 757, 932, 653, 533, 175, 342, 314, 1165, 936, 1114, 252, 639, 8, 695, 838, 539, 824, 458, 408, 508, 248, 148, 352, 1162, 663, 911, 578, 318, 1116, 108, 1106, 235, 844, 54, 889, 645, 586, 1068, 239, 326, 763, 689, 104, 81, 323, 48, 1031, 908, 1041, 851, 1124, 452, 961, 122, 127, 1108, 388, 215, 155, 73, 887, 475, 77, 348, 485, 84, 1148, 650, 509, 75, 299, 331, 144, 241, 288, 727, 619, 775, 625, 206, 977, 455, 934, 697, 777, 865, 245, 835, 585, 133, 70, 655, 1161, 1092, 892, 190, 7, 32, 807, 110, 693, 1134, 1064, 295, 19, 872, 729, 1154, 289, 1176, 657, 142, 890, 445, 952, 275, 866, 587, 927, 603, 943, 211, 1118, 728, 593, 701, 477, 222, 196, 440, 205, 531, 981, 354, 478, 165, 242, 958, 1115, 1075, 1074, 1007, 24, 967, 913, 49, 744, 11, 554, 201, 321, 437, 517, 999, 814, 529, 681, 236, 180, 750, 780, 135, 868, 516, 879, 820, 315, 1128, 560, 218, 845, 297, 949, 841, 1059, 309, 1167, 1155, 665, 425, 1172, 461, 812, 79, 143, 622, 983, 937, 438, 353, 832, 6, 901, 795, 990, 759, 869, 1120, 276, 686, 810, 199, 371, 174, 677, 627, 116, 1133, 435, 519, 113, 496, 1029, 706, 255, 764, 130, 1005, 525, 500, 453, 367, 720, 171, 278, 483, 1122, 754, 698, 1051, 599, 1126, 74, 867, 726, 885, 250, 191, 1027, 229, 996, 310, 209, 884, 111, 456, 1082, 76, 358, 343, 66, 356, 213, 874, 1164, 362, 813, 1138, 5, 1055, 324, 888, 480, 849, 1033, 228, 267, 410, 786, 1035, 139, 873, 972, 740, 303, 951, 304, 1140, 543, 682, 382, 393, 896, 488, 718, 447, 1054, 417, 643, 43, 964, 14, 237, 189, 662, 29, 306, 1144, 119, 654, 86, 1107, 783, 511, 876, 1136, 696, 1146, 1086, 507, 341, 649, 771, 169, 274, 37, 376, 317, 279, 395, 840, 966, 251, 687, 878, 787, 159, 600, 1087, 905, 319, 1160, 378, 1094, 263, 52, 987, 344, 308, 363, 416, 396, 924, 769, 688, 33, 615, 460, 979, 906, 121, 895, 1171, 102, 579, 526, 882, 1168, 177, 523, 380, 1076, 864, 1085, 125, 674, 83, 781, 64, 124, 1047, 51, 20, 661, 767, 258, 151, 833, 638, 1109, 590, 635, 1169, 930, 372, 448, 982, 991, 846, 538, 374, 792, 446, 708, 528, 283, 819, 1089, 858, 1038, 806, 466, 493, 1174, 347, 114, 136, 128, 863, 935, 1093, 415, 269, 510, 1129, 834, 1, 389, 68, 749, 963, 552, 487, 430, 212, 971, 847, 745, 194, 162, 713, 296, 1011, 592, 582, 917, 594, 429, 853, 1073, 784, 520, 499, 469, 451, 1079, 572, 862, 753, 737, 27, 1117, 547, 947, 123, 894, 1112, 855, 384, 1034, 422, 490, 712, 843, 192, 1173, 203, 634, 514, 928, 694, 423, 141, 420, 329, 1017, 1119, 377, 1046, 527, 998, 349, 272, 454, 1057, 501, 178, 392, 405, 774, 472, 238, 1042, 534, 699, 482, 1018, 569, 842, 164, 660, 1113, 536, 817, 95, 240, 387, 854, 126, 561, 743, 431, 672, 802, 208, 766, 690, 390, 1101, 591, 604, 109, 909, 803, 28, 286, 765, 163, 601, 1163, 722, 90, 1177, 1037, 152, 1098, 805, 334, 294, 829, 700, 974, 21]\n",
      "Number of Graphs (dataset_train): 1060\n",
      "Number of Graphs (dataset_test): 118\n",
      "+-----------------------------------------------------------------------------+\n",
      "Learning Rate: 0.001\n",
      "Epochs: 200\n",
      "Batch Size: 64\n",
      "Hidden Dimension 64\n",
      "Number of Layer in Encoder: 1\n",
      "Opimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device: cuda\n",
      "Model:\n",
      "SimCLR(\n",
      "  (encoder): Encoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=89, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      ))\n",
      "    )\n",
      "    (bns): ModuleList(\n",
      "      (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Projection_MLP(\n",
      "    (layer1): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (unilayer): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Start Training...\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss = 260.2087755203247\n",
      "Epoch 2 | Loss = 255.87413030512192\n",
      "Epoch 3 | Loss = 254.04436285355513\n",
      "Epoch 4 | Loss = 248.0178949131685\n",
      "Epoch 5 | Loss = 247.61159285377053\n",
      "Epoch 6 | Loss = 242.3843999189489\n",
      "Epoch 7 | Loss = 246.4107369815602\n",
      "Epoch 8 | Loss = 243.978840266957\n",
      "Epoch 9 | Loss = 239.44190204844756\n",
      "Epoch 10 | Loss = 241.36156009225283\n",
      "Epoch 11 | Loss = 241.31802878660315\n",
      "Epoch 12 | Loss = 236.46476358525894\n",
      "Epoch 13 | Loss = 239.0829262452967\n",
      "Epoch 14 | Loss = 237.50628252590403\n",
      "Epoch 15 | Loss = 239.2671002219705\n",
      "Epoch 16 | Loss = 240.784029006958\n",
      "Epoch 17 | Loss = 236.7903575336232\n",
      "Epoch 18 | Loss = 237.8893378762638\n",
      "Epoch 19 | Loss = 239.61212528453154\n",
      "Epoch 20 | Loss = 238.5651351143332\n",
      "Epoch 21 | Loss = 234.4255314434276\n",
      "Epoch 22 | Loss = 238.71274695676917\n",
      "Epoch 23 | Loss = 234.75405328414018\n",
      "Epoch 24 | Loss = 240.14152347340303\n",
      "Epoch 25 | Loss = 238.86373604045195\n",
      "Epoch 26 | Loss = 236.9571092829985\n",
      "Epoch 27 | Loss = 237.63271774965173\n",
      "Epoch 28 | Loss = 235.44234315086814\n",
      "Epoch 29 | Loss = 234.94662110945757\n",
      "Epoch 30 | Loss = 236.59763414719527\n",
      "Epoch 31 | Loss = 234.76316036897546\n",
      "Epoch 32 | Loss = 232.24385413001565\n",
      "Epoch 33 | Loss = 231.63324030707864\n",
      "Epoch 34 | Loss = 232.25874625935273\n",
      "Epoch 35 | Loss = 235.2169340358061\n",
      "Epoch 36 | Loss = 231.1677365583532\n",
      "Epoch 37 | Loss = 229.70625176149255\n",
      "Epoch 38 | Loss = 231.67350011713364\n",
      "Epoch 39 | Loss = 231.9259938071756\n",
      "Epoch 40 | Loss = 233.333075355081\n",
      "Epoch 41 | Loss = 232.72869413039263\n",
      "Epoch 42 | Loss = 234.53209630180808\n",
      "Epoch 43 | Loss = 234.1350051094504\n",
      "Epoch 44 | Loss = 232.77916717529297\n",
      "Epoch 45 | Loss = 232.85405175826128\n",
      "Epoch 46 | Loss = 233.29359705307905\n",
      "Epoch 47 | Loss = 226.34321285696592\n",
      "Epoch 48 | Loss = 228.72128206140854\n",
      "Epoch 49 | Loss = 230.77159365485696\n",
      "Epoch 50 | Loss = 230.95621905607337\n",
      "Epoch 51 | Loss = 233.8447203355677\n",
      "Epoch 52 | Loss = 229.664226139293\n",
      "Epoch 53 | Loss = 231.78994891222786\n",
      "Epoch 54 | Loss = 228.30065934798296\n",
      "Epoch 55 | Loss = 229.92447735281553\n",
      "Epoch 56 | Loss = 229.94908282336067\n",
      "Epoch 57 | Loss = 230.9197888654821\n",
      "Epoch 58 | Loss = 230.04370302312515\n",
      "Epoch 59 | Loss = 227.64205955056582\n",
      "Epoch 60 | Loss = 229.88866991155288\n",
      "Epoch 61 | Loss = 228.86147740307976\n",
      "Epoch 62 | Loss = 232.4895340975593\n",
      "Epoch 63 | Loss = 230.26709539750044\n",
      "Epoch 64 | Loss = 231.36538864584531\n",
      "Epoch 65 | Loss = 225.35339333029356\n",
      "Epoch 66 | Loss = 232.99923941668342\n",
      "Epoch 67 | Loss = 233.51395371380974\n",
      "Epoch 68 | Loss = 226.43816039141487\n",
      "Epoch 69 | Loss = 229.6451510261087\n",
      "Epoch 70 | Loss = 226.07129540162927\n",
      "Epoch 71 | Loss = 230.50761149911318\n",
      "Epoch 72 | Loss = 228.07656490101533\n",
      "Epoch 73 | Loss = 229.42719958810244\n",
      "Epoch 74 | Loss = 232.77922759336585\n",
      "Epoch 75 | Loss = 229.90836182762595\n",
      "Epoch 76 | Loss = 226.6252426259658\n",
      "Epoch 77 | Loss = 225.9211972180535\n",
      "Epoch 78 | Loss = 225.21652849982766\n",
      "Epoch 79 | Loss = 229.24217942181755\n",
      "Epoch 80 | Loss = 228.36996779722327\n",
      "Epoch 81 | Loss = 231.81605888815488\n",
      "Epoch 82 | Loss = 229.25377318438362\n",
      "Epoch 83 | Loss = 227.99359753552605\n",
      "Epoch 84 | Loss = 226.34574127197266\n",
      "Epoch 85 | Loss = 231.52894721311682\n",
      "Epoch 86 | Loss = 228.1174508263083\n",
      "Epoch 87 | Loss = 228.55888484506045\n",
      "Epoch 88 | Loss = 226.88310163161333\n",
      "Epoch 89 | Loss = 229.37377593096565\n",
      "Epoch 90 | Loss = 228.18074860292322\n",
      "Epoch 91 | Loss = 225.4164395051844\n",
      "Epoch 92 | Loss = 226.98458492054658\n",
      "Epoch 93 | Loss = 225.48247320511763\n",
      "Epoch 94 | Loss = 227.9506872962503\n",
      "Epoch 95 | Loss = 225.35060949886545\n",
      "Epoch 96 | Loss = 225.7699991674984\n",
      "Epoch 97 | Loss = 227.87672587002024\n",
      "Epoch 98 | Loss = 229.8232681611005\n",
      "Epoch 99 | Loss = 224.55614645340864\n",
      "Epoch 100 | Loss = 230.96185622495764\n",
      "Epoch 101 | Loss = 226.93400225919837\n",
      "Epoch 102 | Loss = 227.81899037080652\n",
      "Epoch 103 | Loss = 227.2866996316349\n",
      "Epoch 104 | Loss = 226.15752455767463\n",
      "Epoch 105 | Loss = 223.51188687717215\n",
      "Epoch 106 | Loss = 230.25169899884392\n",
      "Epoch 107 | Loss = 225.8331676931942\n",
      "Epoch 108 | Loss = 224.657392838422\n",
      "Epoch 109 | Loss = 224.9278366425458\n",
      "Epoch 110 | Loss = 227.8170633877025\n",
      "Epoch 111 | Loss = 225.01750441158518\n",
      "Epoch 112 | Loss = 228.2642983829274\n",
      "Epoch 113 | Loss = 229.52652998531565\n",
      "Epoch 114 | Loss = 225.6102793076459\n",
      "Epoch 115 | Loss = 227.83357536091523\n",
      "Epoch 116 | Loss = 223.75364062365364\n",
      "Epoch 117 | Loss = 227.57322956533994\n",
      "Epoch 118 | Loss = 229.24070840723374\n",
      "Epoch 119 | Loss = 227.5103354734533\n",
      "Epoch 120 | Loss = 229.6891253415276\n",
      "Epoch 121 | Loss = 228.41319000019746\n",
      "Epoch 122 | Loss = 225.91870414509492\n",
      "Epoch 123 | Loss = 224.17205350539263\n",
      "Epoch 124 | Loss = 222.53122834598318\n",
      "Epoch 125 | Loss = 227.37428822236902\n",
      "Epoch 126 | Loss = 225.30504866207346\n",
      "Epoch 127 | Loss = 224.3285290774177\n",
      "Epoch 128 | Loss = 226.0043509988224\n",
      "Epoch 129 | Loss = 225.63361942066865\n",
      "Epoch 130 | Loss = 225.27204305985396\n",
      "Epoch 131 | Loss = 223.83079882229075\n",
      "Epoch 132 | Loss = 225.23818902408377\n",
      "Epoch 133 | Loss = 226.4685095618753\n",
      "Epoch 134 | Loss = 225.37417524001177\n",
      "Epoch 135 | Loss = 224.8670517977546\n",
      "Epoch 136 | Loss = 226.82192824868594\n",
      "Epoch 137 | Loss = 224.22639038983513\n",
      "Epoch 138 | Loss = 225.91921155592973\n",
      "Epoch 139 | Loss = 223.83055423287783\n",
      "Epoch 140 | Loss = 226.98531790340647\n",
      "Epoch 141 | Loss = 223.22001614290124\n",
      "Epoch 142 | Loss = 229.8693836997537\n",
      "Epoch 143 | Loss = 227.16190556918875\n",
      "Epoch 144 | Loss = 225.8216366487391\n",
      "Epoch 145 | Loss = 226.6677121555104\n",
      "Epoch 146 | Loss = 224.8583319607903\n",
      "Epoch 147 | Loss = 225.89488203385298\n",
      "Epoch 148 | Loss = 221.3369888978846\n",
      "Epoch 149 | Loss = 228.19309122422163\n",
      "Epoch 150 | Loss = 227.8792527703678\n",
      "Epoch 151 | Loss = 222.43340744691736\n",
      "Epoch 152 | Loss = 225.62778360703413\n",
      "Epoch 153 | Loss = 223.1563699946684\n",
      "Epoch 154 | Loss = 222.46536288541907\n",
      "Epoch 155 | Loss = 224.419888047611\n",
      "Epoch 156 | Loss = 225.25131932426902\n",
      "Epoch 157 | Loss = 225.1948232089772\n",
      "Epoch 158 | Loss = 218.6670108682969\n",
      "Epoch 159 | Loss = 228.66482179305132\n",
      "Epoch 160 | Loss = 224.96073403077966\n",
      "Epoch 161 | Loss = 225.0821257198558\n",
      "Epoch 162 | Loss = 227.33928388707778\n",
      "Epoch 163 | Loss = 220.73350429534912\n",
      "Epoch 164 | Loss = 225.41722471573775\n",
      "Epoch 165 | Loss = 221.030707247117\n",
      "Epoch 166 | Loss = 222.7073076472563\n",
      "Epoch 167 | Loss = 228.95722675323486\n",
      "Epoch 168 | Loss = 224.27358386095833\n",
      "Epoch 169 | Loss = 221.06127632365508\n",
      "Epoch 170 | Loss = 222.14451217651367\n",
      "Epoch 171 | Loss = 224.05839089786306\n",
      "Epoch 172 | Loss = 219.21194295322195\n",
      "Epoch 173 | Loss = 223.8504046271829\n",
      "Epoch 174 | Loss = 221.7534174638636\n",
      "Epoch 175 | Loss = 221.5348711013794\n",
      "Epoch 176 | Loss = 224.07117131177117\n",
      "Epoch 177 | Loss = 222.53123513390037\n",
      "Epoch 178 | Loss = 224.99151616937974\n",
      "Epoch 179 | Loss = 223.80871509103213\n",
      "Epoch 180 | Loss = 223.29681127211626\n",
      "Epoch 181 | Loss = 226.19658627229578\n",
      "Epoch 182 | Loss = 222.19019048354204\n",
      "Epoch 183 | Loss = 226.53938214919145\n",
      "Epoch 184 | Loss = 224.2300307329963\n",
      "Epoch 185 | Loss = 224.79631070529715\n",
      "Epoch 186 | Loss = 224.9039769453161\n",
      "Epoch 187 | Loss = 224.18351106082693\n",
      "Epoch 188 | Loss = 224.47081161947813\n",
      "Epoch 189 | Loss = 221.7456423254574\n",
      "Epoch 190 | Loss = 224.56560241474824\n",
      "Epoch 191 | Loss = 222.75629946764778\n",
      "Epoch 192 | Loss = 223.40442348929014\n",
      "Epoch 193 | Loss = 223.82602214813232\n",
      "Epoch 194 | Loss = 223.09582693436568\n",
      "Epoch 195 | Loss = 218.71015088698442\n",
      "Epoch 196 | Loss = 224.84646522297578\n",
      "Epoch 197 | Loss = 221.46479096132165\n",
      "Epoch 198 | Loss = 225.2708003661212\n",
      "Epoch 199 | Loss = 224.68054692885454\n",
      "Epoch 200 | Loss = 223.1880486432244\n",
      "+-----------------------------------------------------------------------------+\n",
      "Training Time: 23117.466629505157 seconds.\n",
      "Get Embedding...\n",
      "(118, 64) (118,)\n",
      "SVC Accuracy: [0.7015151515151515, 0.7113636363636363]\n",
      "+-----------------------------------------------------------------------------+\n",
      "Embedding Time: 13.566655397415161 seconds.\n",
      "Experient 24\n",
      "20211224-192117 :Log the record!\n",
      "+-----------------------------------------------------------------------------+\n",
      "Experient 25\n",
      "!!!\n",
      "89\n",
      "!!!\n",
      "[553, 217, 346, 850, 1139, 72, 280, 861, 916, 589, 50, 182, 307, 1067, 172, 583, 831, 791, 1014, 246, 18, 1013, 59, 1023, 984, 69, 755, 65, 71, 742, 1153, 256, 856, 266, 427, 956, 679, 826, 541, 147, 910, 0, 922, 683, 131, 103, 91, 1001, 166, 558, 796, 1078, 811, 852, 351, 1002, 383, 145, 197, 1072, 789, 17, 606, 9, 608, 1111, 664, 414, 154, 290, 954, 667, 959, 1097, 652, 339, 184, 368, 327, 506, 933, 370, 36, 768, 67, 1158, 93, 1123, 47, 624, 311, 883, 484, 702, 379, 316, 794, 822, 818, 55, 375, 1008, 605, 584, 920, 575, 986, 675, 82, 468, 1104, 202, 428, 566, 85, 550, 914, 1142, 545, 1145, 870, 1149, 332, 732, 185, 401, 359, 540, 714, 277, 785, 56, 45, 330, 1137, 564, 941, 703, 157, 1056, 1025, 325, 394, 360, 1077, 611, 1058, 1131, 830, 1081, 857, 1105, 364, 57, 233, 291, 1065, 926, 181, 231, 230, 808, 731, 678, 588, 929, 226, 537, 680, 38, 746, 595, 733, 244, 138, 1039, 441, 216, 762, 220, 284, 570, 1166, 1103, 117, 476, 261, 312, 232, 424, 149, 1110, 183, 651, 776, 522, 898, 381, 1100, 1088, 221, 1043, 596, 1084, 719, 669, 253, 715, 498, 35, 112, 495, 524, 548, 369, 735, 574, 580, 391, 167, 293, 942, 793, 273, 204, 1049, 474, 292, 973, 129, 1006, 402, 444, 975, 137, 1071, 647, 656, 1080, 1045, 333, 648, 555, 1127, 576, 486, 602, 989, 399, 3, 1030, 1016, 782, 153, 962, 421, 620, 1036, 264, 944, 848, 736, 717, 26, 1121, 978, 338, 398, 449, 46, 778, 877, 773, 633, 287, 465, 957, 828, 462, 800, 419, 902, 709, 953, 549, 997, 724, 78, 779, 1019, 2, 1050, 546, 1048, 195, 1061, 995, 1032, 10, 25, 450, 357, 1135, 254, 559, 612, 912, 965, 179, 94, 739, 470, 616, 298, 900, 224, 598, 301, 436, 1099, 234, 1060, 1053, 839, 815, 158, 804, 945, 89, 31, 210, 365, 993, 413, 225, 115, 618, 42, 567, 320, 1125, 644, 532, 337, 513, 168, 609, 243, 494, 186, 734, 1159, 193, 439, 305, 641, 628, 300, 521, 636, 948, 198, 994, 556, 96, 1130, 568, 22, 557, 1021, 132, 938, 707, 463, 723, 1152, 915, 711, 156, 631, 931, 457, 1066, 433, 666, 464, 459, 597, 816, 471, 725, 614, 1063, 397, 716, 684, 176, 227, 577, 1004, 801, 637, 34, 761, 302, 404, 1090, 980, 1028, 262, 161, 426, 751, 617, 1000, 39, 345, 504, 880, 907, 409, 173, 515, 249, 1069, 285, 1156, 730, 350, 260, 150, 281, 923, 361, 1062, 918, 1052, 904, 282, 1095, 219, 503, 738, 88, 668, 563, 710, 946, 207, 146, 60, 340, 760, 691, 106, 827, 265, 571, 15, 1132, 1015, 985, 825, 970, 758, 223, 406, 921, 1044, 772, 120, 247, 4, 542, 101, 859, 467, 1150, 12, 871, 268, 1012, 630, 1009, 336, 322, 621, 491, 1026, 187, 629, 432, 385, 992, 809, 1024, 756, 366, 62, 188, 53, 899, 950, 939, 1175, 988, 13, 105, 692, 659, 160, 960, 134, 875, 492, 107, 505, 757, 932, 653, 533, 175, 342, 314, 1165, 936, 1114, 252, 639, 8, 695, 838, 539, 824, 458, 408, 508, 248, 148, 352, 1162, 663, 911, 578, 318, 1116, 108, 1106, 235, 844, 54, 889, 645, 586, 1068, 239, 326, 763, 689, 104, 81, 323, 48, 1031, 908, 1041, 851, 1124, 452, 961, 122, 127, 1108, 388, 215, 155, 73, 887, 475, 77, 348, 485, 84, 1148, 650, 509, 75, 299, 331, 144, 241, 288, 727, 619, 775, 625, 206, 977, 455, 934, 697, 777, 865, 245, 835, 585, 133, 70, 655, 1161, 1092, 892, 190, 7, 32, 807, 110, 693, 1134, 1064, 295, 19, 872, 729, 1154, 289, 1176, 657, 142, 890, 445, 952, 275, 866, 587, 927, 603, 943, 211, 1118, 728, 593, 701, 477, 222, 196, 440, 205, 531, 981, 354, 478, 165, 242, 958, 1115, 1075, 1074, 1007, 24, 967, 913, 49, 744, 11, 554, 201, 321, 437, 517, 999, 814, 529, 681, 236, 180, 750, 780, 135, 868, 516, 879, 820, 315, 1128, 560, 218, 845, 297, 949, 841, 1059, 309, 1167, 1155, 665, 425, 1172, 461, 812, 79, 143, 622, 983, 937, 438, 353, 832, 6, 901, 795, 990, 759, 869, 1120, 276, 686, 810, 199, 371, 174, 677, 627, 116, 1133, 435, 519, 113, 496, 1029, 706, 255, 764, 130, 1005, 525, 500, 453, 367, 720, 171, 278, 483, 1122, 754, 698, 1051, 599, 1126, 74, 867, 726, 885, 250, 191, 1027, 229, 996, 310, 209, 884, 111, 456, 1082, 76, 358, 343, 66, 356, 213, 874, 1164, 362, 813, 1138, 5, 1055, 324, 888, 480, 849, 1033, 228, 267, 410, 786, 1035, 139, 873, 972, 740, 303, 951, 304, 1140, 543, 682, 382, 393, 896, 488, 718, 447, 1054, 417, 643, 43, 964, 14, 237, 189, 662, 29, 306, 1144, 119, 654, 86, 1107, 783, 511, 876, 1136, 696, 1146, 1086, 507, 341, 649, 771, 169, 274, 37, 376, 317, 279, 395, 840, 966, 251, 687, 878, 787, 159, 600, 1087, 905, 319, 1160, 378, 1094, 263, 52, 987, 344, 308, 363, 416, 396, 924, 769, 688, 33, 615, 460, 979, 906, 121, 895, 1171, 102, 579, 526, 882, 1168, 177, 523, 380, 1076, 864, 1085, 125, 674, 83, 781, 64, 124, 1047, 51, 20, 661, 767, 258, 151, 833, 638, 1109, 590, 635, 1169, 930, 372, 448, 982, 991, 846, 538, 374, 792, 446, 708, 528, 283, 819, 1089, 858, 1038, 806, 466, 493, 1174, 347, 114, 136, 128, 863, 935, 1093, 415, 269, 510, 1129, 834, 1, 389, 68, 749, 963, 552, 487, 430, 212, 971, 847, 745, 194, 162, 713, 296, 1011, 592, 582, 917, 594, 429, 853, 1073, 784, 520, 499, 469, 451, 1079, 572, 862, 753, 737, 27, 1117, 547, 947, 123, 894, 1112, 855, 384, 1034, 422, 490, 712, 843, 192, 1173, 203, 634, 514, 928, 694, 423, 141, 420, 329, 1017, 1119, 377, 1046, 527, 998, 349, 272, 454, 1057, 501, 178, 392, 405, 774, 472, 238, 1042, 534, 699, 482, 1018, 569, 842, 164, 660, 1113, 536, 817, 95, 240, 387, 854, 126, 561, 743, 431, 672, 802, 208, 766, 690, 390, 1101, 591, 604, 109, 909, 803, 28, 286, 765, 163, 601, 1163, 722, 90, 1177, 1037, 152, 1098, 805, 334, 294, 829, 700, 974, 21]\n",
      "Number of Graphs (dataset_train): 1060\n",
      "Number of Graphs (dataset_test): 118\n",
      "+-----------------------------------------------------------------------------+\n",
      "Learning Rate: 0.001\n",
      "Epochs: 200\n",
      "Batch Size: 64\n",
      "Hidden Dimension 512\n",
      "Number of Layer in Encoder: 1\n",
      "Opimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device: cuda\n",
      "Model:\n",
      "SimCLR(\n",
      "  (encoder): Encoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=89, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      ))\n",
      "    )\n",
      "    (bns): ModuleList(\n",
      "      (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Projection_MLP(\n",
      "    (layer1): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (unilayer): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Start Training...\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss = 242.60054851980772\n",
      "Epoch 2 | Loss = 238.64852905273438\n",
      "Epoch 3 | Loss = 240.1801237779505\n",
      "Epoch 4 | Loss = 240.32374774708467\n",
      "Epoch 5 | Loss = 240.24518343981575\n",
      "Epoch 6 | Loss = 243.0190118340885\n",
      "Epoch 7 | Loss = 236.12028671713438\n",
      "Epoch 8 | Loss = 235.63850885279038\n",
      "Epoch 9 | Loss = 238.12690459980683\n",
      "Epoch 10 | Loss = 240.25746199663948\n",
      "Epoch 11 | Loss = 234.66943499621223\n",
      "Epoch 12 | Loss = 234.80149156907027\n",
      "Epoch 13 | Loss = 233.76656128378477\n",
      "Epoch 14 | Loss = 235.06114819470574\n",
      "Epoch 15 | Loss = 233.54153560189638\n",
      "Epoch 16 | Loss = 236.46502203099868\n",
      "Epoch 17 | Loss = 232.10468499800737\n",
      "Epoch 18 | Loss = 233.36725105958826\n",
      "Epoch 19 | Loss = 228.9950507668888\n",
      "Epoch 20 | Loss = 233.51632482865278\n",
      "Epoch 21 | Loss = 233.1136906567742\n",
      "Epoch 22 | Loss = 234.4780813104966\n",
      "Epoch 23 | Loss = 232.6842886980842\n",
      "Epoch 24 | Loss = 234.62384549309226\n",
      "Epoch 25 | Loss = 234.2520913516774\n",
      "Epoch 26 | Loss = 227.97102013756248\n",
      "Epoch 27 | Loss = 229.00787706936106\n",
      "Epoch 28 | Loss = 230.01275724523208\n",
      "Epoch 29 | Loss = 235.08229710074033\n",
      "Epoch 30 | Loss = 227.83683709537283\n",
      "Epoch 31 | Loss = 226.48875898473403\n",
      "Epoch 32 | Loss = 230.09988745521096\n",
      "Epoch 33 | Loss = 227.7008934581981\n",
      "Epoch 34 | Loss = 227.3362860399134\n",
      "Epoch 35 | Loss = 223.98099416844985\n",
      "Epoch 36 | Loss = 230.51129918939927\n",
      "Epoch 37 | Loss = 227.4646775301765\n",
      "Epoch 38 | Loss = 225.60183160445268\n",
      "Epoch 39 | Loss = 230.48190363715676\n",
      "Epoch 40 | Loss = 225.17104401307947\n",
      "Epoch 41 | Loss = 226.3713981404024\n",
      "Epoch 42 | Loss = 226.05743520400102\n",
      "Epoch 43 | Loss = 227.9816651063807\n",
      "Epoch 44 | Loss = 223.16440357881433\n",
      "Epoch 45 | Loss = 227.08747067170984\n",
      "Epoch 46 | Loss = 226.91246857362634\n",
      "Epoch 47 | Loss = 229.01575335334329\n",
      "Epoch 48 | Loss = 227.79185210957246\n",
      "Epoch 49 | Loss = 226.11364089741426\n",
      "Epoch 50 | Loss = 226.25888072743135\n",
      "Epoch 51 | Loss = 228.84837077645693\n",
      "Epoch 52 | Loss = 225.59455798653994\n",
      "Epoch 53 | Loss = 226.05372154011445\n",
      "Epoch 54 | Loss = 222.10981139014748\n",
      "Epoch 55 | Loss = 226.22414908689612\n",
      "Epoch 56 | Loss = 224.73693286671357\n",
      "Epoch 57 | Loss = 227.9128549239215\n",
      "Epoch 58 | Loss = 225.45977171729592\n",
      "Epoch 59 | Loss = 224.2217440885656\n",
      "Epoch 60 | Loss = 227.95530638975256\n",
      "Epoch 61 | Loss = 229.13312698813047\n",
      "Epoch 62 | Loss = 223.20166918810676\n",
      "Epoch 63 | Loss = 224.37982267491958\n",
      "Epoch 64 | Loss = 225.73247791739072\n",
      "Epoch 65 | Loss = 224.4545924242805\n",
      "Epoch 66 | Loss = 223.52614296183867\n",
      "Epoch 67 | Loss = 230.0550231372609\n",
      "Epoch 68 | Loss = 225.14070785746856\n",
      "Epoch 69 | Loss = 225.4720828673419\n",
      "Epoch 70 | Loss = 227.07851196737852\n",
      "Epoch 71 | Loss = 226.25487725874956\n",
      "Epoch 72 | Loss = 219.91432756536148\n",
      "Epoch 73 | Loss = 225.13652218089385\n",
      "Epoch 74 | Loss = 225.34250674528235\n",
      "Epoch 75 | Loss = 224.1359458250158\n",
      "Epoch 76 | Loss = 228.3064573512358\n",
      "Epoch 77 | Loss = 221.9843744390151\n",
      "Epoch 78 | Loss = 225.08724285574522\n",
      "Epoch 79 | Loss = 224.54913330078125\n",
      "Epoch 80 | Loss = 221.8657413370469\n",
      "Epoch 81 | Loss = 222.33054273268755\n",
      "Epoch 82 | Loss = 222.63825293148264\n",
      "Epoch 83 | Loss = 222.66362908307244\n",
      "Epoch 84 | Loss = 221.05523232852713\n",
      "Epoch 85 | Loss = 227.70069010117476\n",
      "Epoch 86 | Loss = 223.54624389199648\n",
      "Epoch 87 | Loss = 222.87683733771829\n",
      "Epoch 88 | Loss = 220.886435620925\n",
      "Epoch 89 | Loss = 223.7277530782363\n",
      "Epoch 90 | Loss = 223.25564945445342\n",
      "Epoch 91 | Loss = 221.80092480603386\n",
      "Epoch 92 | Loss = 224.65480580049402\n",
      "Epoch 93 | Loss = 224.5133846507353\n",
      "Epoch 94 | Loss = 221.61147846895105\n",
      "Epoch 95 | Loss = 225.74763830970315\n",
      "Epoch 96 | Loss = 222.16756293352913\n",
      "Epoch 97 | Loss = 223.6126537322998\n",
      "Epoch 98 | Loss = 220.64670506645652\n",
      "Epoch 99 | Loss = 218.2116562899421\n",
      "Epoch 100 | Loss = 221.9125208574183\n",
      "Epoch 101 | Loss = 219.9960293489344\n",
      "Epoch 102 | Loss = 220.9681080649881\n",
      "Epoch 103 | Loss = 219.61221818362966\n",
      "Epoch 104 | Loss = 219.3977491715375\n",
      "Epoch 105 | Loss = 222.67298799402573\n",
      "Epoch 106 | Loss = 224.35624683604522\n",
      "Epoch 107 | Loss = 225.06453127019546\n",
      "Epoch 108 | Loss = 225.16972165949204\n",
      "Epoch 109 | Loss = 222.833033842199\n",
      "Epoch 110 | Loss = 224.2776258132037\n",
      "Epoch 111 | Loss = 224.95854630189783\n",
      "Epoch 112 | Loss = 221.1297057656681\n",
      "Epoch 113 | Loss = 223.31472357581643\n",
      "Epoch 114 | Loss = 219.1896567625158\n",
      "Epoch 115 | Loss = 225.016505465788\n",
      "Epoch 116 | Loss = 223.18963729634004\n",
      "Epoch 117 | Loss = 220.89546231662527\n",
      "Epoch 118 | Loss = 222.3339932385613\n",
      "Epoch 119 | Loss = 219.02682753170237\n",
      "Epoch 120 | Loss = 221.51595317616182\n",
      "Epoch 121 | Loss = 221.99791953142952\n",
      "Epoch 122 | Loss = 217.4129369399127\n",
      "Epoch 123 | Loss = 220.62548435435576\n",
      "Epoch 124 | Loss = 222.0928067039041\n",
      "Epoch 125 | Loss = 223.15033491920022\n",
      "Epoch 126 | Loss = 220.94747778948616\n",
      "Epoch 127 | Loss = 221.77428957995247\n",
      "Epoch 128 | Loss = 226.15682108262007\n",
      "Epoch 129 | Loss = 218.00424093358657\n",
      "Epoch 130 | Loss = 221.5820364671595\n",
      "Epoch 131 | Loss = 220.13852562623865\n",
      "Epoch 132 | Loss = 221.37792149712058\n",
      "Epoch 133 | Loss = 220.22530095717485\n",
      "Epoch 134 | Loss = 223.84462951211367\n",
      "Epoch 135 | Loss = 222.31711146410774\n",
      "Epoch 136 | Loss = 219.02829523647532\n",
      "Epoch 137 | Loss = 217.3671505310956\n",
      "Epoch 138 | Loss = 219.5340434803682\n",
      "Epoch 139 | Loss = 221.13199020834531\n",
      "Epoch 140 | Loss = 219.59271116817698\n",
      "Epoch 141 | Loss = 219.84509619544534\n",
      "Epoch 142 | Loss = 217.42470011991614\n",
      "Epoch 143 | Loss = 219.96441493314856\n",
      "Epoch 144 | Loss = 221.76731597675996\n",
      "Epoch 145 | Loss = 221.277346386629\n",
      "Epoch 146 | Loss = 223.3382495431339\n",
      "Epoch 147 | Loss = 218.22907481474036\n",
      "Epoch 148 | Loss = 221.63545008266672\n",
      "Epoch 149 | Loss = 224.88310785854563\n",
      "Epoch 150 | Loss = 219.83916759490967\n",
      "Epoch 151 | Loss = 217.38524543537812\n",
      "Epoch 152 | Loss = 217.96196365356445\n",
      "Epoch 153 | Loss = 222.30234651004568\n",
      "Epoch 154 | Loss = 216.91080076554243\n",
      "Epoch 155 | Loss = 217.51680957569795\n",
      "Epoch 156 | Loss = 222.4619978175444\n",
      "Epoch 157 | Loss = 216.45736857021555\n",
      "Epoch 158 | Loss = 220.40960957022276\n",
      "Epoch 159 | Loss = 217.84530824773452\n",
      "Epoch 160 | Loss = 222.8292984120986\n",
      "Epoch 161 | Loss = 221.94576762704287\n",
      "Epoch 162 | Loss = 222.56764316558838\n",
      "Epoch 163 | Loss = 217.7132921218872\n",
      "Epoch 164 | Loss = 217.6523328668931\n",
      "Epoch 165 | Loss = 217.89964905907127\n",
      "Epoch 166 | Loss = 218.92897684433882\n",
      "Epoch 167 | Loss = 221.3820390701294\n",
      "Epoch 168 | Loss = 213.1820088554831\n",
      "Epoch 169 | Loss = 218.21590485292322\n",
      "Epoch 170 | Loss = 218.46982350068933\n",
      "Epoch 171 | Loss = 223.75318751615637\n",
      "Epoch 172 | Loss = 220.3009938071756\n",
      "Epoch 173 | Loss = 219.19583387935862\n",
      "Epoch 174 | Loss = 219.0241835538079\n",
      "Epoch 175 | Loss = 216.5560739741606\n",
      "Epoch 176 | Loss = 219.4361930173986\n",
      "Epoch 177 | Loss = 217.47395790324492\n",
      "Epoch 178 | Loss = 219.90749476937685\n",
      "Epoch 179 | Loss = 219.04940958584055\n",
      "Epoch 180 | Loss = 219.91799792121438\n",
      "Epoch 181 | Loss = 216.3222017288208\n",
      "Epoch 182 | Loss = 217.79392315359678\n",
      "Epoch 183 | Loss = 219.0481747458963\n",
      "Epoch 184 | Loss = 216.9988114413093\n",
      "Epoch 185 | Loss = 216.38719917746153\n",
      "Epoch 186 | Loss = 219.29998038796816\n",
      "Epoch 187 | Loss = 218.86309982748594\n",
      "Epoch 188 | Loss = 216.11989952536192\n",
      "Epoch 189 | Loss = 217.36886260088752\n",
      "Epoch 190 | Loss = 219.1359723035027\n",
      "Epoch 191 | Loss = 223.05098769244026\n",
      "Epoch 192 | Loss = 219.8804661245907\n",
      "Epoch 193 | Loss = 218.3066791085636\n",
      "Epoch 194 | Loss = 219.40166164846983\n",
      "Epoch 195 | Loss = 218.3703369813807\n",
      "Epoch 196 | Loss = 219.0476422590368\n",
      "Epoch 197 | Loss = 220.58872015335982\n",
      "Epoch 198 | Loss = 218.22818178289077\n",
      "Epoch 199 | Loss = 217.9543836818022\n",
      "Epoch 200 | Loss = 219.81919484979966\n",
      "+-----------------------------------------------------------------------------+\n",
      "Training Time: 23160.198348283768 seconds.\n",
      "Get Embedding...\n",
      "(118, 512) (118,)\n",
      "SVC Accuracy: [0.7643939393939394, 0.7053030303030303]\n",
      "+-----------------------------------------------------------------------------+\n",
      "Embedding Time: 14.0596444606781 seconds.\n",
      "Experient 25\n",
      "20211225-014732 :Log the record!\n",
      "+-----------------------------------------------------------------------------+\n",
      "Experient 26\n",
      "!!!\n",
      "89\n",
      "!!!\n",
      "[553, 217, 346, 850, 1139, 72, 280, 861, 916, 589, 50, 182, 307, 1067, 172, 583, 831, 791, 1014, 246, 18, 1013, 59, 1023, 984, 69, 755, 65, 71, 742, 1153, 256, 856, 266, 427, 956, 679, 826, 541, 147, 910, 0, 922, 683, 131, 103, 91, 1001, 166, 558, 796, 1078, 811, 852, 351, 1002, 383, 145, 197, 1072, 789, 17, 606, 9, 608, 1111, 664, 414, 154, 290, 954, 667, 959, 1097, 652, 339, 184, 368, 327, 506, 933, 370, 36, 768, 67, 1158, 93, 1123, 47, 624, 311, 883, 484, 702, 379, 316, 794, 822, 818, 55, 375, 1008, 605, 584, 920, 575, 986, 675, 82, 468, 1104, 202, 428, 566, 85, 550, 914, 1142, 545, 1145, 870, 1149, 332, 732, 185, 401, 359, 540, 714, 277, 785, 56, 45, 330, 1137, 564, 941, 703, 157, 1056, 1025, 325, 394, 360, 1077, 611, 1058, 1131, 830, 1081, 857, 1105, 364, 57, 233, 291, 1065, 926, 181, 231, 230, 808, 731, 678, 588, 929, 226, 537, 680, 38, 746, 595, 733, 244, 138, 1039, 441, 216, 762, 220, 284, 570, 1166, 1103, 117, 476, 261, 312, 232, 424, 149, 1110, 183, 651, 776, 522, 898, 381, 1100, 1088, 221, 1043, 596, 1084, 719, 669, 253, 715, 498, 35, 112, 495, 524, 548, 369, 735, 574, 580, 391, 167, 293, 942, 793, 273, 204, 1049, 474, 292, 973, 129, 1006, 402, 444, 975, 137, 1071, 647, 656, 1080, 1045, 333, 648, 555, 1127, 576, 486, 602, 989, 399, 3, 1030, 1016, 782, 153, 962, 421, 620, 1036, 264, 944, 848, 736, 717, 26, 1121, 978, 338, 398, 449, 46, 778, 877, 773, 633, 287, 465, 957, 828, 462, 800, 419, 902, 709, 953, 549, 997, 724, 78, 779, 1019, 2, 1050, 546, 1048, 195, 1061, 995, 1032, 10, 25, 450, 357, 1135, 254, 559, 612, 912, 965, 179, 94, 739, 470, 616, 298, 900, 224, 598, 301, 436, 1099, 234, 1060, 1053, 839, 815, 158, 804, 945, 89, 31, 210, 365, 993, 413, 225, 115, 618, 42, 567, 320, 1125, 644, 532, 337, 513, 168, 609, 243, 494, 186, 734, 1159, 193, 439, 305, 641, 628, 300, 521, 636, 948, 198, 994, 556, 96, 1130, 568, 22, 557, 1021, 132, 938, 707, 463, 723, 1152, 915, 711, 156, 631, 931, 457, 1066, 433, 666, 464, 459, 597, 816, 471, 725, 614, 1063, 397, 716, 684, 176, 227, 577, 1004, 801, 637, 34, 761, 302, 404, 1090, 980, 1028, 262, 161, 426, 751, 617, 1000, 39, 345, 504, 880, 907, 409, 173, 515, 249, 1069, 285, 1156, 730, 350, 260, 150, 281, 923, 361, 1062, 918, 1052, 904, 282, 1095, 219, 503, 738, 88, 668, 563, 710, 946, 207, 146, 60, 340, 760, 691, 106, 827, 265, 571, 15, 1132, 1015, 985, 825, 970, 758, 223, 406, 921, 1044, 772, 120, 247, 4, 542, 101, 859, 467, 1150, 12, 871, 268, 1012, 630, 1009, 336, 322, 621, 491, 1026, 187, 629, 432, 385, 992, 809, 1024, 756, 366, 62, 188, 53, 899, 950, 939, 1175, 988, 13, 105, 692, 659, 160, 960, 134, 875, 492, 107, 505, 757, 932, 653, 533, 175, 342, 314, 1165, 936, 1114, 252, 639, 8, 695, 838, 539, 824, 458, 408, 508, 248, 148, 352, 1162, 663, 911, 578, 318, 1116, 108, 1106, 235, 844, 54, 889, 645, 586, 1068, 239, 326, 763, 689, 104, 81, 323, 48, 1031, 908, 1041, 851, 1124, 452, 961, 122, 127, 1108, 388, 215, 155, 73, 887, 475, 77, 348, 485, 84, 1148, 650, 509, 75, 299, 331, 144, 241, 288, 727, 619, 775, 625, 206, 977, 455, 934, 697, 777, 865, 245, 835, 585, 133, 70, 655, 1161, 1092, 892, 190, 7, 32, 807, 110, 693, 1134, 1064, 295, 19, 872, 729, 1154, 289, 1176, 657, 142, 890, 445, 952, 275, 866, 587, 927, 603, 943, 211, 1118, 728, 593, 701, 477, 222, 196, 440, 205, 531, 981, 354, 478, 165, 242, 958, 1115, 1075, 1074, 1007, 24, 967, 913, 49, 744, 11, 554, 201, 321, 437, 517, 999, 814, 529, 681, 236, 180, 750, 780, 135, 868, 516, 879, 820, 315, 1128, 560, 218, 845, 297, 949, 841, 1059, 309, 1167, 1155, 665, 425, 1172, 461, 812, 79, 143, 622, 983, 937, 438, 353, 832, 6, 901, 795, 990, 759, 869, 1120, 276, 686, 810, 199, 371, 174, 677, 627, 116, 1133, 435, 519, 113, 496, 1029, 706, 255, 764, 130, 1005, 525, 500, 453, 367, 720, 171, 278, 483, 1122, 754, 698, 1051, 599, 1126, 74, 867, 726, 885, 250, 191, 1027, 229, 996, 310, 209, 884, 111, 456, 1082, 76, 358, 343, 66, 356, 213, 874, 1164, 362, 813, 1138, 5, 1055, 324, 888, 480, 849, 1033, 228, 267, 410, 786, 1035, 139, 873, 972, 740, 303, 951, 304, 1140, 543, 682, 382, 393, 896, 488, 718, 447, 1054, 417, 643, 43, 964, 14, 237, 189, 662, 29, 306, 1144, 119, 654, 86, 1107, 783, 511, 876, 1136, 696, 1146, 1086, 507, 341, 649, 771, 169, 274, 37, 376, 317, 279, 395, 840, 966, 251, 687, 878, 787, 159, 600, 1087, 905, 319, 1160, 378, 1094, 263, 52, 987, 344, 308, 363, 416, 396, 924, 769, 688, 33, 615, 460, 979, 906, 121, 895, 1171, 102, 579, 526, 882, 1168, 177, 523, 380, 1076, 864, 1085, 125, 674, 83, 781, 64, 124, 1047, 51, 20, 661, 767, 258, 151, 833, 638, 1109, 590, 635, 1169, 930, 372, 448, 982, 991, 846, 538, 374, 792, 446, 708, 528, 283, 819, 1089, 858, 1038, 806, 466, 493, 1174, 347, 114, 136, 128, 863, 935, 1093, 415, 269, 510, 1129, 834, 1, 389, 68, 749, 963, 552, 487, 430, 212, 971, 847, 745, 194, 162, 713, 296, 1011, 592, 582, 917, 594, 429, 853, 1073, 784, 520, 499, 469, 451, 1079, 572, 862, 753, 737, 27, 1117, 547, 947, 123, 894, 1112, 855, 384, 1034, 422, 490, 712, 843, 192, 1173, 203, 634, 514, 928, 694, 423, 141, 420, 329, 1017, 1119, 377, 1046, 527, 998, 349, 272, 454, 1057, 501, 178, 392, 405, 774, 472, 238, 1042, 534, 699, 482, 1018, 569, 842, 164, 660, 1113, 536, 817, 95, 240, 387, 854, 126, 561, 743, 431, 672, 802, 208, 766, 690, 390, 1101, 591, 604, 109, 909, 803, 28, 286, 765, 163, 601, 1163, 722, 90, 1177, 1037, 152, 1098, 805, 334, 294, 829, 700, 974, 21]\n",
      "Number of Graphs (dataset_train): 1060\n",
      "Number of Graphs (dataset_test): 118\n",
      "+-----------------------------------------------------------------------------+\n",
      "Learning Rate: 0.001\n",
      "Epochs: 200\n",
      "Batch Size: 64\n",
      "Hidden Dimension 64\n",
      "Number of Layer in Encoder: 2\n",
      "Opimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device: cuda\n",
      "Model:\n",
      "SimCLR(\n",
      "  (encoder): Encoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=89, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      ))\n",
      "      (1): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      ))\n",
      "    )\n",
      "    (bns): ModuleList(\n",
      "      (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Projection_MLP(\n",
      "    (layer1): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (unilayer): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Start Training...\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss = 248.4691229427562\n",
      "Epoch 2 | Loss = 245.7206938687493\n",
      "Epoch 3 | Loss = 246.20800943935618\n",
      "Epoch 4 | Loss = 241.2985937455121\n",
      "Epoch 5 | Loss = 242.52409261815689\n",
      "Epoch 6 | Loss = 242.39834409601548\n",
      "Epoch 7 | Loss = 239.98021125793457\n",
      "Epoch 8 | Loss = 238.41316857057458\n",
      "Epoch 9 | Loss = 237.91071269091438\n",
      "Epoch 10 | Loss = 237.14316996406106\n",
      "Epoch 11 | Loss = 238.1577901279225\n",
      "Epoch 12 | Loss = 235.58356896568748\n",
      "Epoch 13 | Loss = 239.98770781124338\n",
      "Epoch 14 | Loss = 240.08968818888945\n",
      "Epoch 15 | Loss = 236.2028818691478\n",
      "Epoch 16 | Loss = 234.2961867837345\n",
      "Epoch 17 | Loss = 240.01269766863655\n",
      "Epoch 18 | Loss = 237.7216412600349\n",
      "Epoch 19 | Loss = 238.6031415041755\n",
      "Epoch 20 | Loss = 233.53861864875344\n",
      "Epoch 21 | Loss = 232.08965755911436\n",
      "Epoch 22 | Loss = 239.10639510435217\n",
      "Epoch 23 | Loss = 232.89983042548684\n",
      "Epoch 24 | Loss = 235.44992570316091\n",
      "Epoch 25 | Loss = 231.07151951509363\n",
      "Epoch 26 | Loss = 233.30881208531997\n",
      "Epoch 27 | Loss = 231.1298012452967\n",
      "Epoch 28 | Loss = 235.02804868361528\n",
      "Epoch 29 | Loss = 233.22541882010069\n",
      "Epoch 30 | Loss = 229.77016117993523\n",
      "Epoch 31 | Loss = 231.37680429570815\n",
      "Epoch 32 | Loss = 229.0789623821483\n",
      "Epoch 33 | Loss = 232.87838055105772\n",
      "Epoch 34 | Loss = 229.68694243711585\n",
      "Epoch 35 | Loss = 230.88499192630545\n",
      "Epoch 36 | Loss = 230.97064938264734\n",
      "Epoch 37 | Loss = 231.12393014571245\n",
      "Epoch 38 | Loss = 226.2418399698594\n",
      "Epoch 39 | Loss = 226.52488798253677\n",
      "Epoch 40 | Loss = 231.291111721712\n",
      "Epoch 41 | Loss = 230.92120636210723\n",
      "Epoch 42 | Loss = 232.04663265452666\n",
      "Epoch 43 | Loss = 229.39207598742317\n",
      "Epoch 44 | Loss = 230.28465012943045\n",
      "Epoch 45 | Loss = 230.93620311512666\n",
      "Epoch 46 | Loss = 228.61843440111946\n",
      "Epoch 47 | Loss = 229.2144926856546\n",
      "Epoch 48 | Loss = 229.10850614659927\n",
      "Epoch 49 | Loss = 230.4556484783397\n",
      "Epoch 50 | Loss = 228.46289084939394\n",
      "Epoch 51 | Loss = 228.19422306733972\n",
      "Epoch 52 | Loss = 231.2333740346572\n",
      "Epoch 53 | Loss = 229.45072387246523\n",
      "Epoch 54 | Loss = 229.5821854647468\n",
      "Epoch 55 | Loss = 231.37430679096894\n",
      "Epoch 56 | Loss = 227.6020892087151\n",
      "Epoch 57 | Loss = 229.43972441729377\n",
      "Epoch 58 | Loss = 226.83262841841753\n",
      "Epoch 59 | Loss = 229.67917672325584\n",
      "Epoch 60 | Loss = 227.8483959646786\n",
      "Epoch 61 | Loss = 225.8258170520558\n",
      "Epoch 62 | Loss = 225.26421086928423\n",
      "Epoch 63 | Loss = 227.48238681344424\n",
      "Epoch 64 | Loss = 225.68678973702822\n",
      "Epoch 65 | Loss = 229.5503225887523\n",
      "Epoch 66 | Loss = 230.026642406688\n",
      "Epoch 68 | Loss = 227.5415023354923\n",
      "Epoch 69 | Loss = 225.88097583546357\n",
      "Epoch 70 | Loss = 227.48662011763628\n",
      "Epoch 71 | Loss = 226.55670272602754\n",
      "Epoch 72 | Loss = 224.7644702125998\n",
      "Epoch 73 | Loss = 225.75350312625662\n",
      "Epoch 74 | Loss = 227.56559860005098\n",
      "Epoch 75 | Loss = 223.1072235107422\n",
      "Epoch 76 | Loss = 227.83194609249338\n",
      "Epoch 77 | Loss = 226.31940684599036\n",
      "Epoch 78 | Loss = 225.72879639793845\n",
      "Epoch 79 | Loss = 226.1198101604686\n",
      "Epoch 80 | Loss = 225.67551029429717\n",
      "Epoch 81 | Loss = 226.05053020926084\n",
      "Epoch 82 | Loss = 224.92295467152314\n",
      "Epoch 83 | Loss = 226.44934177398682\n",
      "Epoch 84 | Loss = 225.69443730747\n",
      "Epoch 85 | Loss = 223.245125097387\n",
      "Epoch 86 | Loss = 225.54007726557114\n",
      "Epoch 87 | Loss = 227.03639922422522\n",
      "Epoch 88 | Loss = 226.5607531491448\n",
      "Epoch 89 | Loss = 228.14306090859804\n",
      "Epoch 90 | Loss = 226.5253682417028\n",
      "Epoch 91 | Loss = 224.7579751295202\n",
      "Epoch 92 | Loss = 228.62936092825498\n",
      "Epoch 93 | Loss = 225.0052080154419\n",
      "Epoch 94 | Loss = 221.20815686618582\n",
      "Epoch 95 | Loss = 222.95456185060388\n",
      "Epoch 96 | Loss = 225.31620345396155\n",
      "Epoch 97 | Loss = 226.05454500983743\n",
      "Epoch 98 | Loss = 224.71106433868408\n",
      "Epoch 99 | Loss = 222.63012712142046\n",
      "Epoch 100 | Loss = 224.0122116874246\n",
      "Epoch 101 | Loss = 223.81775811139275\n",
      "Epoch 102 | Loss = 226.4957825716804\n",
      "Epoch 103 | Loss = 224.63685809864717\n",
      "Epoch 104 | Loss = 225.5516969456392\n",
      "Epoch 105 | Loss = 224.35743859234978\n",
      "Epoch 106 | Loss = 223.9277333652272\n",
      "Epoch 107 | Loss = 221.65380281560562\n",
      "Epoch 108 | Loss = 224.58766179926255\n",
      "Epoch 109 | Loss = 222.3481533387128\n",
      "Epoch 110 | Loss = 222.8433437908397\n",
      "Epoch 111 | Loss = 220.4556552101584\n",
      "Epoch 112 | Loss = 225.831028769998\n",
      "Epoch 113 | Loss = 223.59909063227036\n",
      "Epoch 114 | Loss = 224.925015000736\n",
      "Epoch 115 | Loss = 223.29333137063418\n",
      "Epoch 116 | Loss = 222.08391599094168\n",
      "Epoch 117 | Loss = 224.16112097571877\n",
      "Epoch 118 | Loss = 228.62649900772993\n",
      "Epoch 119 | Loss = 223.6225139393526\n",
      "Epoch 120 | Loss = 224.68466265061323\n",
      "Epoch 121 | Loss = 221.8030388775994\n",
      "Epoch 122 | Loss = 224.40162232342888\n",
      "Epoch 123 | Loss = 224.45951826432173\n",
      "Epoch 124 | Loss = 222.44935770595774\n",
      "Epoch 125 | Loss = 222.84019935832305\n",
      "Epoch 126 | Loss = 223.7933428708245\n",
      "Epoch 127 | Loss = 224.8126298680025\n",
      "Epoch 128 | Loss = 222.36057612475227\n",
      "Epoch 129 | Loss = 222.0768719841452\n",
      "Epoch 130 | Loss = 231.2365853926715\n",
      "Epoch 131 | Loss = 227.04765448850745\n",
      "Epoch 132 | Loss = 224.90716765908633\n",
      "Epoch 133 | Loss = 224.52699812720803\n",
      "Epoch 134 | Loss = 225.36803049199722\n",
      "Epoch 135 | Loss = 221.4274990418378\n",
      "Epoch 136 | Loss = 223.16483817381018\n",
      "Epoch 137 | Loss = 225.17758548960967\n",
      "Epoch 138 | Loss = 223.70024686701157\n",
      "Epoch 139 | Loss = 223.72461060916677\n",
      "Epoch 140 | Loss = 223.52166686338538\n",
      "Epoch 141 | Loss = 221.21444242140825\n",
      "Epoch 142 | Loss = 226.13580490561094\n",
      "Epoch 143 | Loss = 223.26908049863928\n",
      "Epoch 144 | Loss = 224.19891183516557\n",
      "Epoch 145 | Loss = 224.17959981806138\n",
      "Epoch 146 | Loss = 219.83926660874312\n",
      "Epoch 147 | Loss = 225.90034456814035\n",
      "Epoch 148 | Loss = 224.27617308672737\n",
      "Epoch 149 | Loss = 222.68954383625703\n",
      "Epoch 150 | Loss = 222.37866289475386\n",
      "Epoch 151 | Loss = 224.29580856772031\n",
      "Epoch 152 | Loss = 218.49282758376177\n",
      "Epoch 153 | Loss = 222.42466724620147\n",
      "Epoch 154 | Loss = 224.38598083047304\n",
      "Epoch 155 | Loss = 223.11531874712776\n",
      "Epoch 156 | Loss = 222.7826467962826\n",
      "Epoch 157 | Loss = 224.6888293098001\n",
      "Epoch 158 | Loss = 221.4951581954956\n",
      "Epoch 159 | Loss = 221.69637853959028\n",
      "Epoch 160 | Loss = 221.9510113211239\n",
      "Epoch 161 | Loss = 223.34081341238584\n",
      "Epoch 162 | Loss = 223.56597900390625\n",
      "Epoch 163 | Loss = 222.49385003482595\n",
      "Epoch 164 | Loss = 222.8202018737793\n",
      "Epoch 165 | Loss = 221.65723929685706\n",
      "Epoch 166 | Loss = 217.2428690405453\n",
      "Epoch 167 | Loss = 221.1840837142047\n",
      "Epoch 168 | Loss = 223.62797350042007\n",
      "Epoch 169 | Loss = 221.35669758740593\n",
      "Epoch 170 | Loss = 219.26335273069495\n",
      "Epoch 171 | Loss = 221.2927158580107\n",
      "Epoch 172 | Loss = 223.63129110897287\n",
      "Epoch 173 | Loss = 223.14936643488267\n",
      "Epoch 174 | Loss = 226.93996047973633\n",
      "Epoch 175 | Loss = 221.13587385065415\n",
      "Epoch 176 | Loss = 221.29292813469382\n",
      "Epoch 177 | Loss = 219.45974714615767\n",
      "Epoch 178 | Loss = 221.08002208260928\n",
      "Epoch 179 | Loss = 223.32820056466494\n",
      "Epoch 180 | Loss = 221.53508337806252\n",
      "Epoch 181 | Loss = 224.67839437372544\n",
      "Epoch 182 | Loss = 220.87329084732954\n",
      "Epoch 183 | Loss = 222.1789048138787\n",
      "Epoch 184 | Loss = 223.24218794878792\n",
      "Epoch 185 | Loss = 218.71035486109116\n",
      "Epoch 186 | Loss = 223.35874518226174\n",
      "Epoch 187 | Loss = 222.33520171221565\n",
      "Epoch 188 | Loss = 220.23492139928481\n",
      "Epoch 189 | Loss = 221.40391540527344\n",
      "Epoch 190 | Loss = 223.07718989428352\n",
      "Epoch 191 | Loss = 218.1840139837826\n",
      "Epoch 192 | Loss = 219.52644062042236\n",
      "Epoch 193 | Loss = 224.93595016703887\n",
      "Epoch 194 | Loss = 220.62906641118667\n",
      "Epoch 195 | Loss = 218.96054974724265\n",
      "Epoch 196 | Loss = 220.65945148468018\n",
      "Epoch 197 | Loss = 222.85873385036692\n",
      "Epoch 198 | Loss = 220.3019176931942\n",
      "Epoch 199 | Loss = 219.4915115693036\n",
      "Epoch 200 | Loss = 222.2642391990213\n",
      "+-----------------------------------------------------------------------------+\n",
      "Training Time: 23166.800474882126 seconds.\n",
      "Get Embedding...\n",
      "(118, 128) (118,)\n",
      "SVC Accuracy: [0.7446969696969699, 0.7484848484848485]\n",
      "+-----------------------------------------------------------------------------+\n",
      "Embedding Time: 13.470596313476562 seconds.\n",
      "Experient 26\n",
      "20211225-081352 :Log the record!\n",
      "+-----------------------------------------------------------------------------+\n",
      "Experient 27\n",
      "!!!\n",
      "89\n",
      "!!!\n",
      "[553, 217, 346, 850, 1139, 72, 280, 861, 916, 589, 50, 182, 307, 1067, 172, 583, 831, 791, 1014, 246, 18, 1013, 59, 1023, 984, 69, 755, 65, 71, 742, 1153, 256, 856, 266, 427, 956, 679, 826, 541, 147, 910, 0, 922, 683, 131, 103, 91, 1001, 166, 558, 796, 1078, 811, 852, 351, 1002, 383, 145, 197, 1072, 789, 17, 606, 9, 608, 1111, 664, 414, 154, 290, 954, 667, 959, 1097, 652, 339, 184, 368, 327, 506, 933, 370, 36, 768, 67, 1158, 93, 1123, 47, 624, 311, 883, 484, 702, 379, 316, 794, 822, 818, 55, 375, 1008, 605, 584, 920, 575, 986, 675, 82, 468, 1104, 202, 428, 566, 85, 550, 914, 1142, 545, 1145, 870, 1149, 332, 732, 185, 401, 359, 540, 714, 277, 785, 56, 45, 330, 1137, 564, 941, 703, 157, 1056, 1025, 325, 394, 360, 1077, 611, 1058, 1131, 830, 1081, 857, 1105, 364, 57, 233, 291, 1065, 926, 181, 231, 230, 808, 731, 678, 588, 929, 226, 537, 680, 38, 746, 595, 733, 244, 138, 1039, 441, 216, 762, 220, 284, 570, 1166, 1103, 117, 476, 261, 312, 232, 424, 149, 1110, 183, 651, 776, 522, 898, 381, 1100, 1088, 221, 1043, 596, 1084, 719, 669, 253, 715, 498, 35, 112, 495, 524, 548, 369, 735, 574, 580, 391, 167, 293, 942, 793, 273, 204, 1049, 474, 292, 973, 129, 1006, 402, 444, 975, 137, 1071, 647, 656, 1080, 1045, 333, 648, 555, 1127, 576, 486, 602, 989, 399, 3, 1030, 1016, 782, 153, 962, 421, 620, 1036, 264, 944, 848, 736, 717, 26, 1121, 978, 338, 398, 449, 46, 778, 877, 773, 633, 287, 465, 957, 828, 462, 800, 419, 902, 709, 953, 549, 997, 724, 78, 779, 1019, 2, 1050, 546, 1048, 195, 1061, 995, 1032, 10, 25, 450, 357, 1135, 254, 559, 612, 912, 965, 179, 94, 739, 470, 616, 298, 900, 224, 598, 301, 436, 1099, 234, 1060, 1053, 839, 815, 158, 804, 945, 89, 31, 210, 365, 993, 413, 225, 115, 618, 42, 567, 320, 1125, 644, 532, 337, 513, 168, 609, 243, 494, 186, 734, 1159, 193, 439, 305, 641, 628, 300, 521, 636, 948, 198, 994, 556, 96, 1130, 568, 22, 557, 1021, 132, 938, 707, 463, 723, 1152, 915, 711, 156, 631, 931, 457, 1066, 433, 666, 464, 459, 597, 816, 471, 725, 614, 1063, 397, 716, 684, 176, 227, 577, 1004, 801, 637, 34, 761, 302, 404, 1090, 980, 1028, 262, 161, 426, 751, 617, 1000, 39, 345, 504, 880, 907, 409, 173, 515, 249, 1069, 285, 1156, 730, 350, 260, 150, 281, 923, 361, 1062, 918, 1052, 904, 282, 1095, 219, 503, 738, 88, 668, 563, 710, 946, 207, 146, 60, 340, 760, 691, 106, 827, 265, 571, 15, 1132, 1015, 985, 825, 970, 758, 223, 406, 921, 1044, 772, 120, 247, 4, 542, 101, 859, 467, 1150, 12, 871, 268, 1012, 630, 1009, 336, 322, 621, 491, 1026, 187, 629, 432, 385, 992, 809, 1024, 756, 366, 62, 188, 53, 899, 950, 939, 1175, 988, 13, 105, 692, 659, 160, 960, 134, 875, 492, 107, 505, 757, 932, 653, 533, 175, 342, 314, 1165, 936, 1114, 252, 639, 8, 695, 838, 539, 824, 458, 408, 508, 248, 148, 352, 1162, 663, 911, 578, 318, 1116, 108, 1106, 235, 844, 54, 889, 645, 586, 1068, 239, 326, 763, 689, 104, 81, 323, 48, 1031, 908, 1041, 851, 1124, 452, 961, 122, 127, 1108, 388, 215, 155, 73, 887, 475, 77, 348, 485, 84, 1148, 650, 509, 75, 299, 331, 144, 241, 288, 727, 619, 775, 625, 206, 977, 455, 934, 697, 777, 865, 245, 835, 585, 133, 70, 655, 1161, 1092, 892, 190, 7, 32, 807, 110, 693, 1134, 1064, 295, 19, 872, 729, 1154, 289, 1176, 657, 142, 890, 445, 952, 275, 866, 587, 927, 603, 943, 211, 1118, 728, 593, 701, 477, 222, 196, 440, 205, 531, 981, 354, 478, 165, 242, 958, 1115, 1075, 1074, 1007, 24, 967, 913, 49, 744, 11, 554, 201, 321, 437, 517, 999, 814, 529, 681, 236, 180, 750, 780, 135, 868, 516, 879, 820, 315, 1128, 560, 218, 845, 297, 949, 841, 1059, 309, 1167, 1155, 665, 425, 1172, 461, 812, 79, 143, 622, 983, 937, 438, 353, 832, 6, 901, 795, 990, 759, 869, 1120, 276, 686, 810, 199, 371, 174, 677, 627, 116, 1133, 435, 519, 113, 496, 1029, 706, 255, 764, 130, 1005, 525, 500, 453, 367, 720, 171, 278, 483, 1122, 754, 698, 1051, 599, 1126, 74, 867, 726, 885, 250, 191, 1027, 229, 996, 310, 209, 884, 111, 456, 1082, 76, 358, 343, 66, 356, 213, 874, 1164, 362, 813, 1138, 5, 1055, 324, 888, 480, 849, 1033, 228, 267, 410, 786, 1035, 139, 873, 972, 740, 303, 951, 304, 1140, 543, 682, 382, 393, 896, 488, 718, 447, 1054, 417, 643, 43, 964, 14, 237, 189, 662, 29, 306, 1144, 119, 654, 86, 1107, 783, 511, 876, 1136, 696, 1146, 1086, 507, 341, 649, 771, 169, 274, 37, 376, 317, 279, 395, 840, 966, 251, 687, 878, 787, 159, 600, 1087, 905, 319, 1160, 378, 1094, 263, 52, 987, 344, 308, 363, 416, 396, 924, 769, 688, 33, 615, 460, 979, 906, 121, 895, 1171, 102, 579, 526, 882, 1168, 177, 523, 380, 1076, 864, 1085, 125, 674, 83, 781, 64, 124, 1047, 51, 20, 661, 767, 258, 151, 833, 638, 1109, 590, 635, 1169, 930, 372, 448, 982, 991, 846, 538, 374, 792, 446, 708, 528, 283, 819, 1089, 858, 1038, 806, 466, 493, 1174, 347, 114, 136, 128, 863, 935, 1093, 415, 269, 510, 1129, 834, 1, 389, 68, 749, 963, 552, 487, 430, 212, 971, 847, 745, 194, 162, 713, 296, 1011, 592, 582, 917, 594, 429, 853, 1073, 784, 520, 499, 469, 451, 1079, 572, 862, 753, 737, 27, 1117, 547, 947, 123, 894, 1112, 855, 384, 1034, 422, 490, 712, 843, 192, 1173, 203, 634, 514, 928, 694, 423, 141, 420, 329, 1017, 1119, 377, 1046, 527, 998, 349, 272, 454, 1057, 501, 178, 392, 405, 774, 472, 238, 1042, 534, 699, 482, 1018, 569, 842, 164, 660, 1113, 536, 817, 95, 240, 387, 854, 126, 561, 743, 431, 672, 802, 208, 766, 690, 390, 1101, 591, 604, 109, 909, 803, 28, 286, 765, 163, 601, 1163, 722, 90, 1177, 1037, 152, 1098, 805, 334, 294, 829, 700, 974, 21]\n",
      "Number of Graphs (dataset_train): 1060\n",
      "Number of Graphs (dataset_test): 118\n",
      "+-----------------------------------------------------------------------------+\n",
      "Learning Rate: 0.001\n",
      "Epochs: 200\n",
      "Batch Size: 64\n",
      "Hidden Dimension 512\n",
      "Number of Layer in Encoder: 2\n",
      "Opimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device: cuda\n",
      "Model:\n",
      "SimCLR(\n",
      "  (encoder): Encoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=89, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      ))\n",
      "      (1): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      ))\n",
      "    )\n",
      "    (bns): ModuleList(\n",
      "      (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Projection_MLP(\n",
      "    (layer1): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (unilayer): Sequential(\n",
      "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Start Training...\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss = 242.7542112574858\n",
      "Epoch 2 | Loss = 248.05309761271758\n",
      "Epoch 3 | Loss = 250.84299704607795\n",
      "Epoch 4 | Loss = 248.04947348201975\n",
      "Epoch 5 | Loss = 244.91003917245303\n",
      "Epoch 6 | Loss = 242.87850660436294\n",
      "Epoch 7 | Loss = 242.85060231825884\n",
      "Epoch 8 | Loss = 246.48118260327507\n",
      "Epoch 9 | Loss = 243.8472624386058\n",
      "Epoch 10 | Loss = 242.1112278769998\n",
      "Epoch 11 | Loss = 245.20868918474983\n",
      "Epoch 12 | Loss = 239.31035866456872\n",
      "Epoch 13 | Loss = 244.46562407998476\n",
      "Epoch 14 | Loss = 243.01919404198142\n",
      "Epoch 15 | Loss = 234.43751537098603\n",
      "Epoch 16 | Loss = 241.89988231658936\n",
      "Epoch 17 | Loss = 242.08766129437615\n",
      "Epoch 18 | Loss = 240.05304067275102\n",
      "Epoch 19 | Loss = 240.9844233007992\n",
      "Epoch 20 | Loss = 232.5845251644359\n",
      "Epoch 21 | Loss = 236.73050386765425\n",
      "Epoch 22 | Loss = 236.15784790936638\n",
      "Epoch 23 | Loss = 232.58294481389663\n",
      "Epoch 24 | Loss = 235.8281832302318\n",
      "Epoch 25 | Loss = 239.44704207252053\n",
      "Epoch 26 | Loss = 237.63546651952407\n",
      "Epoch 27 | Loss = 233.92824313219856\n",
      "Epoch 28 | Loss = 233.06504468356863\n",
      "Epoch 29 | Loss = 236.2708091174855\n",
      "Epoch 30 | Loss = 232.01869476542754\n",
      "Epoch 31 | Loss = 235.70096745210535\n",
      "Epoch 32 | Loss = 236.53092266531553\n",
      "Epoch 33 | Loss = 232.40691785251394\n",
      "Epoch 34 | Loss = 233.27126446892234\n",
      "Epoch 35 | Loss = 235.78836620555205\n",
      "Epoch 36 | Loss = 227.34997362249038\n",
      "Epoch 37 | Loss = 229.48015936683205\n",
      "Epoch 38 | Loss = 228.4807809380924\n",
      "Epoch 39 | Loss = 229.0395305857939\n",
      "Epoch 40 | Loss = 226.2229201372932\n",
      "Epoch 41 | Loss = 231.74696697908288\n",
      "Epoch 42 | Loss = 230.24311654707964\n",
      "Epoch 43 | Loss = 230.45431782217588\n",
      "Epoch 44 | Loss = 231.47289893206428\n",
      "Epoch 45 | Loss = 227.78561451855828\n",
      "Epoch 46 | Loss = 227.7491169536815\n",
      "Epoch 47 | Loss = 225.29602140538833\n",
      "Epoch 48 | Loss = 228.44715258654426\n",
      "Epoch 49 | Loss = 230.08760037141687\n",
      "Epoch 50 | Loss = 226.69089109757368\n",
      "Epoch 51 | Loss = 233.11675290500418\n",
      "Epoch 52 | Loss = 223.41894441492417\n",
      "Epoch 53 | Loss = 227.1351883271161\n",
      "Epoch 54 | Loss = 234.80991801093606\n",
      "Epoch 55 | Loss = 228.40212328293745\n",
      "Epoch 56 | Loss = 228.53493707320268\n",
      "Epoch 57 | Loss = 231.42562681085923\n",
      "Epoch 58 | Loss = 231.71536625132842\n",
      "Epoch 59 | Loss = 228.54164291830625\n",
      "Epoch 60 | Loss = 224.3838741639081\n",
      "Epoch 61 | Loss = 223.82856795367073\n",
      "Epoch 62 | Loss = 227.56603712194106\n",
      "Epoch 63 | Loss = 227.22553382200354\n",
      "Epoch 64 | Loss = 231.33815479278564\n",
      "Epoch 65 | Loss = 228.91996428545784\n",
      "Epoch 66 | Loss = 225.85694515003877\n",
      "Epoch 67 | Loss = 225.72316809261545\n",
      "Epoch 68 | Loss = 222.3807284411262\n",
      "Epoch 69 | Loss = 229.35324315463797\n",
      "Epoch 70 | Loss = 224.0372393551995\n",
      "Epoch 71 | Loss = 225.95435383740593\n",
      "Epoch 72 | Loss = 223.92174025142893\n",
      "Epoch 73 | Loss = 226.7005001516903\n",
      "Epoch 74 | Loss = 221.3621752121869\n",
      "Epoch 75 | Loss = 225.56483094832475\n",
      "Epoch 76 | Loss = 219.825379596037\n",
      "Epoch 77 | Loss = 224.09368071836585\n",
      "Epoch 78 | Loss = 222.3094969917746\n",
      "Epoch 79 | Loss = 226.2911038679235\n",
      "Epoch 80 | Loss = 226.9989611120785\n",
      "Epoch 81 | Loss = 223.59022477093865\n",
      "Epoch 82 | Loss = 225.77282686794504\n",
      "Epoch 83 | Loss = 227.49625144285315\n",
      "Epoch 84 | Loss = 226.56233849244958\n",
      "Epoch 85 | Loss = 227.72292967403635\n",
      "Epoch 86 | Loss = 225.37955620709587\n",
      "Epoch 87 | Loss = 222.27787943447336\n",
      "Epoch 88 | Loss = 224.16676482032327\n",
      "Epoch 89 | Loss = 222.56144164590273\n",
      "Epoch 90 | Loss = 225.16717226365034\n",
      "Epoch 91 | Loss = 222.8850455564611\n",
      "Epoch 92 | Loss = 221.7145600599401\n",
      "Epoch 93 | Loss = 222.43212464276482\n",
      "Epoch 94 | Loss = 220.7922981486601\n",
      "Epoch 95 | Loss = 224.35512408088235\n",
      "Epoch 96 | Loss = 221.29514206157012\n",
      "Epoch 97 | Loss = 228.23169107998118\n",
      "Epoch 98 | Loss = 222.74758658689612\n",
      "Epoch 99 | Loss = 227.6198268217199\n",
      "Epoch 100 | Loss = 220.46579832189224\n",
      "Epoch 101 | Loss = 225.74931357888613\n",
      "Epoch 102 | Loss = 224.70890000287224\n",
      "Epoch 103 | Loss = 226.2695342793184\n",
      "Epoch 104 | Loss = 222.08661707709817\n",
      "Epoch 105 | Loss = 225.86689337562112\n",
      "Epoch 106 | Loss = 223.90612074908088\n",
      "Epoch 107 | Loss = 222.56277493869558\n",
      "Epoch 108 | Loss = 223.75131876328413\n",
      "Epoch 109 | Loss = 223.93008860419778\n",
      "Epoch 110 | Loss = 221.40225589976592\n",
      "Epoch 111 | Loss = 221.92600749520693\n",
      "Epoch 112 | Loss = 224.81580644495347\n",
      "Epoch 113 | Loss = 223.2456376131843\n",
      "Epoch 114 | Loss = 225.83408754012163\n",
      "Epoch 115 | Loss = 224.86352735407212\n",
      "Epoch 116 | Loss = 223.47376750497256\n",
      "Epoch 117 | Loss = 224.96294251610252\n",
      "Epoch 118 | Loss = 223.1512314852546\n",
      "Epoch 119 | Loss = 225.29048549427705\n",
      "Epoch 120 | Loss = 225.18843734965606\n",
      "Epoch 121 | Loss = 223.02700553220862\n",
      "Epoch 122 | Loss = 224.88122738108916\n",
      "Epoch 123 | Loss = 224.82205144096824\n",
      "Epoch 124 | Loss = 225.5862766153672\n",
      "Epoch 125 | Loss = 225.52820873260498\n",
      "Epoch 126 | Loss = 221.74515370761648\n",
      "Epoch 127 | Loss = 223.02599587159997\n",
      "Epoch 128 | Loss = 220.17572273927576\n",
      "Epoch 129 | Loss = 222.1742187387803\n",
      "Epoch 130 | Loss = 222.48826223261216\n",
      "Epoch 131 | Loss = 223.03453983980066\n",
      "Epoch 132 | Loss = 218.00448782303755\n",
      "Epoch 133 | Loss = 221.5275804856244\n",
      "Epoch 134 | Loss = 220.56345496458167\n",
      "Epoch 135 | Loss = 221.48721986658433\n",
      "Epoch 136 | Loss = 223.8316588682287\n",
      "Epoch 137 | Loss = 220.99701023101807\n",
      "Epoch 138 | Loss = 224.7965264600866\n",
      "Epoch 139 | Loss = 223.79305205625647\n",
      "Epoch 140 | Loss = 218.88113235024844\n",
      "Epoch 141 | Loss = 218.4796303580789\n",
      "Epoch 142 | Loss = 222.7288209690767\n",
      "Epoch 143 | Loss = 223.08029651641846\n",
      "Epoch 144 | Loss = 222.7326224270989\n",
      "Epoch 145 | Loss = 216.07174082363352\n",
      "Epoch 146 | Loss = 219.12073393429026\n",
      "Epoch 147 | Loss = 225.172480134403\n",
      "Epoch 148 | Loss = 221.5867317985086\n",
      "Epoch 149 | Loss = 221.9496747746187\n",
      "Epoch 150 | Loss = 223.04342662586885\n",
      "Epoch 151 | Loss = 220.7843360339894\n",
      "Epoch 152 | Loss = 219.87345011094038\n",
      "Epoch 153 | Loss = 218.48152054057402\n",
      "Epoch 154 | Loss = 217.8390095093671\n",
      "Epoch 155 | Loss = 221.40503159691306\n",
      "Epoch 156 | Loss = 220.16739963082705\n",
      "Epoch 157 | Loss = 223.32855572420007\n",
      "Epoch 158 | Loss = 222.11568927764893\n",
      "Epoch 159 | Loss = 221.55668830871582\n",
      "Epoch 160 | Loss = 221.34974272110884\n",
      "Epoch 161 | Loss = 219.33673196680405\n",
      "Epoch 162 | Loss = 220.88454846774832\n",
      "Epoch 163 | Loss = 224.64153317844168\n",
      "Epoch 164 | Loss = 221.74211827446433\n",
      "Epoch 165 | Loss = 222.8578939998851\n",
      "Epoch 166 | Loss = 220.26774872050567\n",
      "Epoch 167 | Loss = 220.88930371228386\n",
      "Epoch 168 | Loss = 221.9511870215921\n",
      "Epoch 169 | Loss = 223.1157174390905\n",
      "Epoch 170 | Loss = 217.54305413190056\n",
      "Epoch 171 | Loss = 223.38180794435388\n",
      "Epoch 172 | Loss = 219.791553329019\n",
      "Epoch 173 | Loss = 220.15620764564065\n",
      "Epoch 174 | Loss = 217.84062924104578\n",
      "Epoch 175 | Loss = 220.5203130385455\n",
      "Epoch 176 | Loss = 219.29472721324248\n",
      "Epoch 177 | Loss = 218.65673283969656\n",
      "Epoch 178 | Loss = 223.81757663278017\n",
      "Epoch 179 | Loss = 219.76907539367676\n",
      "Epoch 180 | Loss = 221.81357063966638\n",
      "Epoch 181 | Loss = 221.03287831474753\n",
      "Epoch 182 | Loss = 223.87591457366943\n",
      "Epoch 183 | Loss = 223.21911284502815\n",
      "Epoch 184 | Loss = 223.1598648183486\n",
      "Epoch 185 | Loss = 215.83626965915457\n",
      "Epoch 186 | Loss = 221.35490709192612\n",
      "Epoch 187 | Loss = 220.28979374380674\n",
      "Epoch 188 | Loss = 219.7808390785666\n",
      "Epoch 189 | Loss = 220.22178470387178\n",
      "Epoch 190 | Loss = 222.30024214351877\n",
      "Epoch 191 | Loss = 221.3393627615536\n",
      "Epoch 192 | Loss = 223.3094736548031\n",
      "Epoch 193 | Loss = 222.07151429793413\n",
      "Epoch 194 | Loss = 216.70133977777817\n",
      "Epoch 195 | Loss = 217.45479673497817\n",
      "Epoch 196 | Loss = 218.87795061223648\n",
      "Epoch 197 | Loss = 223.2275129767025\n",
      "Epoch 198 | Loss = 220.28221029393814\n",
      "Epoch 199 | Loss = 219.79354420830222\n",
      "Epoch 200 | Loss = 218.4572613099042\n",
      "+-----------------------------------------------------------------------------+\n",
      "Training Time: 23169.605678081512 seconds.\n",
      "Get Embedding...\n",
      "(118, 1024) (118,)\n",
      "SVC Accuracy: [0.7712121212121212, 0.6530303030303031]\n",
      "+-----------------------------------------------------------------------------+\n",
      "Embedding Time: 14.382814645767212 seconds.\n",
      "Experient 27\n",
      "20211225-144017 :Log the record!\n",
      "+-----------------------------------------------------------------------------+\n",
      "Experient 28\n",
      "!!!\n",
      "89\n",
      "!!!\n",
      "[553, 217, 346, 850, 1139, 72, 280, 861, 916, 589, 50, 182, 307, 1067, 172, 583, 831, 791, 1014, 246, 18, 1013, 59, 1023, 984, 69, 755, 65, 71, 742, 1153, 256, 856, 266, 427, 956, 679, 826, 541, 147, 910, 0, 922, 683, 131, 103, 91, 1001, 166, 558, 796, 1078, 811, 852, 351, 1002, 383, 145, 197, 1072, 789, 17, 606, 9, 608, 1111, 664, 414, 154, 290, 954, 667, 959, 1097, 652, 339, 184, 368, 327, 506, 933, 370, 36, 768, 67, 1158, 93, 1123, 47, 624, 311, 883, 484, 702, 379, 316, 794, 822, 818, 55, 375, 1008, 605, 584, 920, 575, 986, 675, 82, 468, 1104, 202, 428, 566, 85, 550, 914, 1142, 545, 1145, 870, 1149, 332, 732, 185, 401, 359, 540, 714, 277, 785, 56, 45, 330, 1137, 564, 941, 703, 157, 1056, 1025, 325, 394, 360, 1077, 611, 1058, 1131, 830, 1081, 857, 1105, 364, 57, 233, 291, 1065, 926, 181, 231, 230, 808, 731, 678, 588, 929, 226, 537, 680, 38, 746, 595, 733, 244, 138, 1039, 441, 216, 762, 220, 284, 570, 1166, 1103, 117, 476, 261, 312, 232, 424, 149, 1110, 183, 651, 776, 522, 898, 381, 1100, 1088, 221, 1043, 596, 1084, 719, 669, 253, 715, 498, 35, 112, 495, 524, 548, 369, 735, 574, 580, 391, 167, 293, 942, 793, 273, 204, 1049, 474, 292, 973, 129, 1006, 402, 444, 975, 137, 1071, 647, 656, 1080, 1045, 333, 648, 555, 1127, 576, 486, 602, 989, 399, 3, 1030, 1016, 782, 153, 962, 421, 620, 1036, 264, 944, 848, 736, 717, 26, 1121, 978, 338, 398, 449, 46, 778, 877, 773, 633, 287, 465, 957, 828, 462, 800, 419, 902, 709, 953, 549, 997, 724, 78, 779, 1019, 2, 1050, 546, 1048, 195, 1061, 995, 1032, 10, 25, 450, 357, 1135, 254, 559, 612, 912, 965, 179, 94, 739, 470, 616, 298, 900, 224, 598, 301, 436, 1099, 234, 1060, 1053, 839, 815, 158, 804, 945, 89, 31, 210, 365, 993, 413, 225, 115, 618, 42, 567, 320, 1125, 644, 532, 337, 513, 168, 609, 243, 494, 186, 734, 1159, 193, 439, 305, 641, 628, 300, 521, 636, 948, 198, 994, 556, 96, 1130, 568, 22, 557, 1021, 132, 938, 707, 463, 723, 1152, 915, 711, 156, 631, 931, 457, 1066, 433, 666, 464, 459, 597, 816, 471, 725, 614, 1063, 397, 716, 684, 176, 227, 577, 1004, 801, 637, 34, 761, 302, 404, 1090, 980, 1028, 262, 161, 426, 751, 617, 1000, 39, 345, 504, 880, 907, 409, 173, 515, 249, 1069, 285, 1156, 730, 350, 260, 150, 281, 923, 361, 1062, 918, 1052, 904, 282, 1095, 219, 503, 738, 88, 668, 563, 710, 946, 207, 146, 60, 340, 760, 691, 106, 827, 265, 571, 15, 1132, 1015, 985, 825, 970, 758, 223, 406, 921, 1044, 772, 120, 247, 4, 542, 101, 859, 467, 1150, 12, 871, 268, 1012, 630, 1009, 336, 322, 621, 491, 1026, 187, 629, 432, 385, 992, 809, 1024, 756, 366, 62, 188, 53, 899, 950, 939, 1175, 988, 13, 105, 692, 659, 160, 960, 134, 875, 492, 107, 505, 757, 932, 653, 533, 175, 342, 314, 1165, 936, 1114, 252, 639, 8, 695, 838, 539, 824, 458, 408, 508, 248, 148, 352, 1162, 663, 911, 578, 318, 1116, 108, 1106, 235, 844, 54, 889, 645, 586, 1068, 239, 326, 763, 689, 104, 81, 323, 48, 1031, 908, 1041, 851, 1124, 452, 961, 122, 127, 1108, 388, 215, 155, 73, 887, 475, 77, 348, 485, 84, 1148, 650, 509, 75, 299, 331, 144, 241, 288, 727, 619, 775, 625, 206, 977, 455, 934, 697, 777, 865, 245, 835, 585, 133, 70, 655, 1161, 1092, 892, 190, 7, 32, 807, 110, 693, 1134, 1064, 295, 19, 872, 729, 1154, 289, 1176, 657, 142, 890, 445, 952, 275, 866, 587, 927, 603, 943, 211, 1118, 728, 593, 701, 477, 222, 196, 440, 205, 531, 981, 354, 478, 165, 242, 958, 1115, 1075, 1074, 1007, 24, 967, 913, 49, 744, 11, 554, 201, 321, 437, 517, 999, 814, 529, 681, 236, 180, 750, 780, 135, 868, 516, 879, 820, 315, 1128, 560, 218, 845, 297, 949, 841, 1059, 309, 1167, 1155, 665, 425, 1172, 461, 812, 79, 143, 622, 983, 937, 438, 353, 832, 6, 901, 795, 990, 759, 869, 1120, 276, 686, 810, 199, 371, 174, 677, 627, 116, 1133, 435, 519, 113, 496, 1029, 706, 255, 764, 130, 1005, 525, 500, 453, 367, 720, 171, 278, 483, 1122, 754, 698, 1051, 599, 1126, 74, 867, 726, 885, 250, 191, 1027, 229, 996, 310, 209, 884, 111, 456, 1082, 76, 358, 343, 66, 356, 213, 874, 1164, 362, 813, 1138, 5, 1055, 324, 888, 480, 849, 1033, 228, 267, 410, 786, 1035, 139, 873, 972, 740, 303, 951, 304, 1140, 543, 682, 382, 393, 896, 488, 718, 447, 1054, 417, 643, 43, 964, 14, 237, 189, 662, 29, 306, 1144, 119, 654, 86, 1107, 783, 511, 876, 1136, 696, 1146, 1086, 507, 341, 649, 771, 169, 274, 37, 376, 317, 279, 395, 840, 966, 251, 687, 878, 787, 159, 600, 1087, 905, 319, 1160, 378, 1094, 263, 52, 987, 344, 308, 363, 416, 396, 924, 769, 688, 33, 615, 460, 979, 906, 121, 895, 1171, 102, 579, 526, 882, 1168, 177, 523, 380, 1076, 864, 1085, 125, 674, 83, 781, 64, 124, 1047, 51, 20, 661, 767, 258, 151, 833, 638, 1109, 590, 635, 1169, 930, 372, 448, 982, 991, 846, 538, 374, 792, 446, 708, 528, 283, 819, 1089, 858, 1038, 806, 466, 493, 1174, 347, 114, 136, 128, 863, 935, 1093, 415, 269, 510, 1129, 834, 1, 389, 68, 749, 963, 552, 487, 430, 212, 971, 847, 745, 194, 162, 713, 296, 1011, 592, 582, 917, 594, 429, 853, 1073, 784, 520, 499, 469, 451, 1079, 572, 862, 753, 737, 27, 1117, 547, 947, 123, 894, 1112, 855, 384, 1034, 422, 490, 712, 843, 192, 1173, 203, 634, 514, 928, 694, 423, 141, 420, 329, 1017, 1119, 377, 1046, 527, 998, 349, 272, 454, 1057, 501, 178, 392, 405, 774, 472, 238, 1042, 534, 699, 482, 1018, 569, 842, 164, 660, 1113, 536, 817, 95, 240, 387, 854, 126, 561, 743, 431, 672, 802, 208, 766, 690, 390, 1101, 591, 604, 109, 909, 803, 28, 286, 765, 163, 601, 1163, 722, 90, 1177, 1037, 152, 1098, 805, 334, 294, 829, 700, 974, 21]\n",
      "Number of Graphs (dataset_train): 1060\n",
      "Number of Graphs (dataset_test): 118\n",
      "+-----------------------------------------------------------------------------+\n",
      "Learning Rate: 0.001\n",
      "Epochs: 200\n",
      "Batch Size: 64\n",
      "Hidden Dimension 64\n",
      "Number of Layer in Encoder: 3\n",
      "Opimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device: cuda\n",
      "Model:\n",
      "SimCLR(\n",
      "  (encoder): Encoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=89, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      ))\n",
      "      (1): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      ))\n",
      "      (2): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      ))\n",
      "    )\n",
      "    (bns): ModuleList(\n",
      "      (0): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Projection_MLP(\n",
      "    (layer1): Sequential(\n",
      "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (1): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (1): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (1): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (unilayer): Sequential(\n",
      "      (0): Linear(in_features=192, out_features=192, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=192, out_features=192, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Start Training...\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss = 249.48301292868223\n",
      "Epoch 2 | Loss = 244.71859185835893\n",
      "Epoch 3 | Loss = 244.77207026762122\n",
      "Epoch 4 | Loss = 242.03783321380615\n",
      "Epoch 5 | Loss = 242.90766631855683\n",
      "Epoch 6 | Loss = 238.325932895436\n",
      "Epoch 7 | Loss = 237.82067068885354\n",
      "Epoch 8 | Loss = 242.8002835442038\n",
      "Epoch 9 | Loss = 238.31339763192568\n",
      "Epoch 10 | Loss = 241.1108351875754\n",
      "Epoch 11 | Loss = 236.31346035003662\n",
      "Epoch 12 | Loss = 238.04544723735137\n",
      "Epoch 13 | Loss = 234.32114735771628\n",
      "Epoch 14 | Loss = 238.0154987223008\n",
      "Epoch 15 | Loss = 238.2483449823716\n",
      "Epoch 16 | Loss = 236.53743210960837\n",
      "Epoch 17 | Loss = 238.39635119718665\n",
      "Epoch 18 | Loss = 235.34369193806367\n",
      "Epoch 19 | Loss = 239.4831667507396\n",
      "Epoch 20 | Loss = 236.22711551890654\n",
      "Epoch 21 | Loss = 235.33859898062315\n",
      "Epoch 22 | Loss = 237.19217732373406\n",
      "Epoch 23 | Loss = 236.12289439930635\n",
      "Epoch 24 | Loss = 235.14200479844038\n",
      "Epoch 25 | Loss = 230.50735574610093\n",
      "Epoch 26 | Loss = 233.8124855265898\n",
      "Epoch 27 | Loss = 231.75601224338308\n",
      "Epoch 28 | Loss = 236.4212376089657\n",
      "Epoch 29 | Loss = 235.2658929824829\n",
      "Epoch 30 | Loss = 232.09738849191103\n",
      "Epoch 31 | Loss = 235.06000760022332\n",
      "Epoch 32 | Loss = 234.0001682954676\n",
      "Epoch 33 | Loss = 229.9684148676255\n",
      "Epoch 34 | Loss = 230.56624749127556\n",
      "Epoch 35 | Loss = 232.7123378865859\n",
      "Epoch 36 | Loss = 234.75321057263542\n",
      "Epoch 37 | Loss = 232.96638657065\n",
      "Epoch 38 | Loss = 232.8529545840095\n",
      "Epoch 39 | Loss = 232.92550558202407\n",
      "Epoch 40 | Loss = 234.032883644104\n",
      "Epoch 41 | Loss = 230.03596956589644\n",
      "Epoch 42 | Loss = 234.65153929766487\n",
      "Epoch 43 | Loss = 234.5766591464772\n",
      "Epoch 44 | Loss = 228.05475481818704\n",
      "Epoch 45 | Loss = 233.17860261131736\n",
      "Epoch 46 | Loss = 231.56703359940474\n",
      "Epoch 47 | Loss = 232.20279817020193\n",
      "Epoch 48 | Loss = 228.60131460077622\n",
      "Epoch 49 | Loss = 231.42497887330896\n",
      "Epoch 50 | Loss = 228.34094266330496\n",
      "Epoch 51 | Loss = 227.0962635489071\n",
      "Epoch 52 | Loss = 227.72409169814165\n",
      "Epoch 53 | Loss = 230.15648073308608\n",
      "Epoch 54 | Loss = 223.6299934948192\n",
      "Epoch 55 | Loss = 230.7010061600629\n",
      "Epoch 56 | Loss = 229.03760674420525\n",
      "Epoch 57 | Loss = 225.49497828764075\n",
      "Epoch 58 | Loss = 227.8236229279462\n",
      "Epoch 59 | Loss = 227.20247594047996\n",
      "Epoch 60 | Loss = 227.13346313027773\n",
      "Epoch 61 | Loss = 227.19857799305635\n",
      "Epoch 62 | Loss = 227.4366543152753\n",
      "Epoch 63 | Loss = 231.00211193982292\n",
      "Epoch 64 | Loss = 227.01604091419892\n",
      "Epoch 65 | Loss = 226.22291817384607\n",
      "Epoch 66 | Loss = 229.7503931382123\n",
      "Epoch 67 | Loss = 226.64078129039092\n",
      "Epoch 68 | Loss = 227.24851894378662\n",
      "Epoch 69 | Loss = 229.46081268086152\n",
      "Epoch 70 | Loss = 229.45850164750044\n",
      "Epoch 71 | Loss = 231.00919241063735\n",
      "Epoch 72 | Loss = 231.55780225641587\n",
      "Epoch 73 | Loss = 229.43262212416704\n",
      "Epoch 74 | Loss = 227.61840270547304\n",
      "Epoch 76 | Loss = 226.13351967755486\n",
      "Epoch 77 | Loss = 224.99726188884063\n",
      "Epoch 78 | Loss = 226.19761029411765\n",
      "Epoch 79 | Loss = 224.91904746784883\n",
      "Epoch 80 | Loss = 225.13320485283347\n",
      "Epoch 81 | Loss = 226.99166045469397\n",
      "Epoch 82 | Loss = 226.30852373908547\n",
      "Epoch 83 | Loss = 228.87442386851592\n",
      "Epoch 84 | Loss = 226.44117170221665\n",
      "Epoch 85 | Loss = 226.81549818375532\n",
      "Epoch 86 | Loss = 224.0785405215095\n",
      "Epoch 87 | Loss = 225.8770642561071\n",
      "Epoch 88 | Loss = 224.45248665529138\n",
      "Epoch 89 | Loss = 227.24203496820786\n",
      "Epoch 90 | Loss = 223.55650329589844\n",
      "Epoch 91 | Loss = 225.1397039750043\n",
      "Epoch 92 | Loss = 226.42607329873476\n",
      "Epoch 93 | Loss = 226.50978817659265\n",
      "Epoch 94 | Loss = 221.85949830447927\n",
      "Epoch 95 | Loss = 228.31006768170525\n",
      "Epoch 96 | Loss = 224.29542591992546\n",
      "Epoch 97 | Loss = 221.88360343259924\n",
      "Epoch 98 | Loss = 226.42530901291792\n",
      "Epoch 99 | Loss = 225.29388629688935\n",
      "Epoch 100 | Loss = 218.10365620781394\n",
      "Epoch 101 | Loss = 224.46869564056396\n",
      "Epoch 102 | Loss = 225.03995289522058\n",
      "Epoch 103 | Loss = 226.75913844389075\n",
      "Epoch 104 | Loss = 226.9254794962266\n",
      "Epoch 105 | Loss = 222.89747103522805\n",
      "Epoch 106 | Loss = 226.3875337488511\n",
      "Epoch 107 | Loss = 224.55728519664092\n",
      "Epoch 108 | Loss = 227.8855062933529\n",
      "Epoch 109 | Loss = 223.30963869655832\n",
      "Epoch 110 | Loss = 221.73136464287253\n",
      "Epoch 111 | Loss = 227.6524046168608\n",
      "Epoch 112 | Loss = 223.2422898797428\n",
      "Epoch 113 | Loss = 226.85695064769072\n",
      "Epoch 114 | Loss = 225.42828279383042\n",
      "Epoch 115 | Loss = 227.15871620178223\n",
      "Epoch 116 | Loss = 224.9420352823594\n",
      "Epoch 117 | Loss = 223.53014328900505\n",
      "Epoch 118 | Loss = 220.86938762664795\n",
      "Epoch 119 | Loss = 222.8540750952328\n",
      "Epoch 120 | Loss = 224.46812107983757\n",
      "Epoch 121 | Loss = 223.5793809890747\n",
      "Epoch 122 | Loss = 223.56869815377627\n",
      "Epoch 123 | Loss = 229.55270307204302\n",
      "Epoch 124 | Loss = 220.9819496940164\n",
      "Epoch 125 | Loss = 222.26897469688865\n",
      "Epoch 126 | Loss = 227.92384927413042\n",
      "Epoch 127 | Loss = 225.78074690874885\n",
      "Epoch 128 | Loss = 223.87151987412398\n",
      "Epoch 129 | Loss = 221.8731040393605\n",
      "Epoch 130 | Loss = 224.1472749710083\n",
      "Epoch 131 | Loss = 220.4836328169879\n",
      "Epoch 132 | Loss = 221.11085869284238\n",
      "Epoch 133 | Loss = 223.04645594428567\n",
      "Epoch 134 | Loss = 222.67008169959573\n",
      "Epoch 135 | Loss = 219.8966668072869\n",
      "Epoch 136 | Loss = 224.9901606054867\n",
      "Epoch 137 | Loss = 223.11236308602724\n",
      "Epoch 138 | Loss = 220.66113410276526\n",
      "Epoch 139 | Loss = 221.36832383099724\n",
      "Epoch 140 | Loss = 225.4502753650441\n",
      "Epoch 141 | Loss = 225.3197302537806\n",
      "Epoch 142 | Loss = 224.89027606739717\n",
      "Epoch 143 | Loss = 222.2505142548505\n",
      "Epoch 144 | Loss = 219.26542332593132\n",
      "Epoch 145 | Loss = 219.11694246179917\n",
      "Epoch 146 | Loss = 222.9503318562227\n",
      "Epoch 147 | Loss = 222.09651890922996\n",
      "Epoch 148 | Loss = 223.54948066262637\n",
      "Epoch 149 | Loss = 219.65265055263743\n",
      "Epoch 150 | Loss = 221.23202946606804\n",
      "Epoch 151 | Loss = 222.3438637116376\n",
      "Epoch 152 | Loss = 219.4718054603128\n",
      "Epoch 153 | Loss = 221.3507355521707\n",
      "Epoch 154 | Loss = 223.9719897999483\n",
      "Epoch 155 | Loss = 223.01923235724954\n",
      "Epoch 156 | Loss = 221.8574275409474\n",
      "Epoch 157 | Loss = 221.52467138626997\n",
      "Epoch 158 | Loss = 223.85185516581817\n",
      "Epoch 159 | Loss = 222.26900010950425\n",
      "Epoch 160 | Loss = 221.4769536186667\n",
      "Epoch 161 | Loss = 221.93908786773682\n",
      "Epoch 162 | Loss = 221.63812424154844\n",
      "Epoch 163 | Loss = 218.16188930062685\n",
      "Epoch 164 | Loss = 223.41579375547522\n",
      "Epoch 165 | Loss = 220.3495449739344\n",
      "Epoch 166 | Loss = 222.52038428362678\n",
      "Epoch 167 | Loss = 223.91297564787024\n",
      "Epoch 168 | Loss = 221.4132406571332\n",
      "Epoch 169 | Loss = 222.09219377181108\n",
      "Epoch 170 | Loss = 219.3261521283318\n",
      "Epoch 171 | Loss = 221.92346600925222\n",
      "Epoch 172 | Loss = 222.50923538208008\n",
      "Epoch 173 | Loss = 224.36246238035315\n",
      "Epoch 174 | Loss = 220.14138793945312\n",
      "Epoch 175 | Loss = 220.52430685828713\n",
      "Epoch 176 | Loss = 219.75185399896958\n",
      "Epoch 177 | Loss = 220.98120543536018\n",
      "Epoch 178 | Loss = 222.9916971992044\n",
      "Epoch 179 | Loss = 220.4948876324822\n",
      "Epoch 180 | Loss = 220.84325745526482\n",
      "Epoch 181 | Loss = 221.98980196784524\n",
      "Epoch 182 | Loss = 217.03964878531065\n",
      "Epoch 183 | Loss = 222.60376947066362\n",
      "Epoch 184 | Loss = 220.8113810034359\n",
      "Epoch 185 | Loss = 221.36443934721106\n",
      "Epoch 186 | Loss = 221.68495716768152\n",
      "Epoch 187 | Loss = 224.16761678807876\n",
      "Epoch 188 | Loss = 218.66717703202193\n",
      "Epoch 189 | Loss = 218.7795571719899\n",
      "Epoch 190 | Loss = 221.7101734946756\n",
      "Epoch 191 | Loss = 221.87695026397705\n",
      "Epoch 192 | Loss = 222.7485933864818\n",
      "Epoch 193 | Loss = 221.80017476923325\n",
      "Epoch 194 | Loss = 222.94561818066765\n",
      "Epoch 195 | Loss = 221.90739149205825\n",
      "Epoch 196 | Loss = 224.68262722912957\n",
      "Epoch 197 | Loss = 219.33840162613814\n",
      "Epoch 198 | Loss = 217.37997099932502\n",
      "Epoch 199 | Loss = 221.06927967071533\n",
      "Epoch 200 | Loss = 220.50688962375418\n",
      "+-----------------------------------------------------------------------------+\n",
      "Training Time: 23146.028915166855 seconds.\n",
      "Get Embedding...\n",
      "(118, 192) (118,)\n",
      "SVC Accuracy: [0.7530303030303032, 0.7136363636363636]\n",
      "+-----------------------------------------------------------------------------+\n",
      "Embedding Time: 13.55564284324646 seconds.\n",
      "Experient 28\n",
      "20211225-210617 :Log the record!\n",
      "+-----------------------------------------------------------------------------+\n",
      "Experient 29\n",
      "!!!\n",
      "89\n",
      "!!!\n",
      "[553, 217, 346, 850, 1139, 72, 280, 861, 916, 589, 50, 182, 307, 1067, 172, 583, 831, 791, 1014, 246, 18, 1013, 59, 1023, 984, 69, 755, 65, 71, 742, 1153, 256, 856, 266, 427, 956, 679, 826, 541, 147, 910, 0, 922, 683, 131, 103, 91, 1001, 166, 558, 796, 1078, 811, 852, 351, 1002, 383, 145, 197, 1072, 789, 17, 606, 9, 608, 1111, 664, 414, 154, 290, 954, 667, 959, 1097, 652, 339, 184, 368, 327, 506, 933, 370, 36, 768, 67, 1158, 93, 1123, 47, 624, 311, 883, 484, 702, 379, 316, 794, 822, 818, 55, 375, 1008, 605, 584, 920, 575, 986, 675, 82, 468, 1104, 202, 428, 566, 85, 550, 914, 1142, 545, 1145, 870, 1149, 332, 732, 185, 401, 359, 540, 714, 277, 785, 56, 45, 330, 1137, 564, 941, 703, 157, 1056, 1025, 325, 394, 360, 1077, 611, 1058, 1131, 830, 1081, 857, 1105, 364, 57, 233, 291, 1065, 926, 181, 231, 230, 808, 731, 678, 588, 929, 226, 537, 680, 38, 746, 595, 733, 244, 138, 1039, 441, 216, 762, 220, 284, 570, 1166, 1103, 117, 476, 261, 312, 232, 424, 149, 1110, 183, 651, 776, 522, 898, 381, 1100, 1088, 221, 1043, 596, 1084, 719, 669, 253, 715, 498, 35, 112, 495, 524, 548, 369, 735, 574, 580, 391, 167, 293, 942, 793, 273, 204, 1049, 474, 292, 973, 129, 1006, 402, 444, 975, 137, 1071, 647, 656, 1080, 1045, 333, 648, 555, 1127, 576, 486, 602, 989, 399, 3, 1030, 1016, 782, 153, 962, 421, 620, 1036, 264, 944, 848, 736, 717, 26, 1121, 978, 338, 398, 449, 46, 778, 877, 773, 633, 287, 465, 957, 828, 462, 800, 419, 902, 709, 953, 549, 997, 724, 78, 779, 1019, 2, 1050, 546, 1048, 195, 1061, 995, 1032, 10, 25, 450, 357, 1135, 254, 559, 612, 912, 965, 179, 94, 739, 470, 616, 298, 900, 224, 598, 301, 436, 1099, 234, 1060, 1053, 839, 815, 158, 804, 945, 89, 31, 210, 365, 993, 413, 225, 115, 618, 42, 567, 320, 1125, 644, 532, 337, 513, 168, 609, 243, 494, 186, 734, 1159, 193, 439, 305, 641, 628, 300, 521, 636, 948, 198, 994, 556, 96, 1130, 568, 22, 557, 1021, 132, 938, 707, 463, 723, 1152, 915, 711, 156, 631, 931, 457, 1066, 433, 666, 464, 459, 597, 816, 471, 725, 614, 1063, 397, 716, 684, 176, 227, 577, 1004, 801, 637, 34, 761, 302, 404, 1090, 980, 1028, 262, 161, 426, 751, 617, 1000, 39, 345, 504, 880, 907, 409, 173, 515, 249, 1069, 285, 1156, 730, 350, 260, 150, 281, 923, 361, 1062, 918, 1052, 904, 282, 1095, 219, 503, 738, 88, 668, 563, 710, 946, 207, 146, 60, 340, 760, 691, 106, 827, 265, 571, 15, 1132, 1015, 985, 825, 970, 758, 223, 406, 921, 1044, 772, 120, 247, 4, 542, 101, 859, 467, 1150, 12, 871, 268, 1012, 630, 1009, 336, 322, 621, 491, 1026, 187, 629, 432, 385, 992, 809, 1024, 756, 366, 62, 188, 53, 899, 950, 939, 1175, 988, 13, 105, 692, 659, 160, 960, 134, 875, 492, 107, 505, 757, 932, 653, 533, 175, 342, 314, 1165, 936, 1114, 252, 639, 8, 695, 838, 539, 824, 458, 408, 508, 248, 148, 352, 1162, 663, 911, 578, 318, 1116, 108, 1106, 235, 844, 54, 889, 645, 586, 1068, 239, 326, 763, 689, 104, 81, 323, 48, 1031, 908, 1041, 851, 1124, 452, 961, 122, 127, 1108, 388, 215, 155, 73, 887, 475, 77, 348, 485, 84, 1148, 650, 509, 75, 299, 331, 144, 241, 288, 727, 619, 775, 625, 206, 977, 455, 934, 697, 777, 865, 245, 835, 585, 133, 70, 655, 1161, 1092, 892, 190, 7, 32, 807, 110, 693, 1134, 1064, 295, 19, 872, 729, 1154, 289, 1176, 657, 142, 890, 445, 952, 275, 866, 587, 927, 603, 943, 211, 1118, 728, 593, 701, 477, 222, 196, 440, 205, 531, 981, 354, 478, 165, 242, 958, 1115, 1075, 1074, 1007, 24, 967, 913, 49, 744, 11, 554, 201, 321, 437, 517, 999, 814, 529, 681, 236, 180, 750, 780, 135, 868, 516, 879, 820, 315, 1128, 560, 218, 845, 297, 949, 841, 1059, 309, 1167, 1155, 665, 425, 1172, 461, 812, 79, 143, 622, 983, 937, 438, 353, 832, 6, 901, 795, 990, 759, 869, 1120, 276, 686, 810, 199, 371, 174, 677, 627, 116, 1133, 435, 519, 113, 496, 1029, 706, 255, 764, 130, 1005, 525, 500, 453, 367, 720, 171, 278, 483, 1122, 754, 698, 1051, 599, 1126, 74, 867, 726, 885, 250, 191, 1027, 229, 996, 310, 209, 884, 111, 456, 1082, 76, 358, 343, 66, 356, 213, 874, 1164, 362, 813, 1138, 5, 1055, 324, 888, 480, 849, 1033, 228, 267, 410, 786, 1035, 139, 873, 972, 740, 303, 951, 304, 1140, 543, 682, 382, 393, 896, 488, 718, 447, 1054, 417, 643, 43, 964, 14, 237, 189, 662, 29, 306, 1144, 119, 654, 86, 1107, 783, 511, 876, 1136, 696, 1146, 1086, 507, 341, 649, 771, 169, 274, 37, 376, 317, 279, 395, 840, 966, 251, 687, 878, 787, 159, 600, 1087, 905, 319, 1160, 378, 1094, 263, 52, 987, 344, 308, 363, 416, 396, 924, 769, 688, 33, 615, 460, 979, 906, 121, 895, 1171, 102, 579, 526, 882, 1168, 177, 523, 380, 1076, 864, 1085, 125, 674, 83, 781, 64, 124, 1047, 51, 20, 661, 767, 258, 151, 833, 638, 1109, 590, 635, 1169, 930, 372, 448, 982, 991, 846, 538, 374, 792, 446, 708, 528, 283, 819, 1089, 858, 1038, 806, 466, 493, 1174, 347, 114, 136, 128, 863, 935, 1093, 415, 269, 510, 1129, 834, 1, 389, 68, 749, 963, 552, 487, 430, 212, 971, 847, 745, 194, 162, 713, 296, 1011, 592, 582, 917, 594, 429, 853, 1073, 784, 520, 499, 469, 451, 1079, 572, 862, 753, 737, 27, 1117, 547, 947, 123, 894, 1112, 855, 384, 1034, 422, 490, 712, 843, 192, 1173, 203, 634, 514, 928, 694, 423, 141, 420, 329, 1017, 1119, 377, 1046, 527, 998, 349, 272, 454, 1057, 501, 178, 392, 405, 774, 472, 238, 1042, 534, 699, 482, 1018, 569, 842, 164, 660, 1113, 536, 817, 95, 240, 387, 854, 126, 561, 743, 431, 672, 802, 208, 766, 690, 390, 1101, 591, 604, 109, 909, 803, 28, 286, 765, 163, 601, 1163, 722, 90, 1177, 1037, 152, 1098, 805, 334, 294, 829, 700, 974, 21]\n",
      "Number of Graphs (dataset_train): 1060\n",
      "Number of Graphs (dataset_test): 118\n",
      "+-----------------------------------------------------------------------------+\n",
      "Learning Rate: 0.001\n",
      "Epochs: 200\n",
      "Batch Size: 64\n",
      "Hidden Dimension 512\n",
      "Number of Layer in Encoder: 3\n",
      "Opimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Device: cuda\n",
      "Model:\n",
      "SimCLR(\n",
      "  (encoder): Encoder(\n",
      "    (convs): ModuleList(\n",
      "      (0): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=89, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      ))\n",
      "      (1): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      ))\n",
      "      (2): GINConv(nn=Sequential(\n",
      "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      ))\n",
      "    )\n",
      "    (bns): ModuleList(\n",
      "      (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (projector): Projection_MLP(\n",
      "    (layer1): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (unilayer): Sequential(\n",
      "      (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "+-----------------------------------------------------------------------------+\n",
      "Start Training...\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss = 254.45417813693777\n",
      "Epoch 2 | Loss = 257.3828233270084\n",
      "Epoch 3 | Loss = 255.51210050021902\n",
      "Epoch 4 | Loss = 255.87279454399558\n",
      "Epoch 5 | Loss = 260.4177159141092\n",
      "Epoch 6 | Loss = 248.35234445684097\n",
      "Epoch 7 | Loss = 254.48030477411606\n",
      "Epoch 8 | Loss = 251.49771505243638\n",
      "Epoch 9 | Loss = 252.49707827848547\n",
      "Epoch 10 | Loss = 244.82803541071274\n",
      "Epoch 11 | Loss = 247.8336627062629\n",
      "Epoch 12 | Loss = 253.3652462678797\n",
      "Epoch 13 | Loss = 254.14025906955496\n",
      "Epoch 14 | Loss = 250.37598391140207\n",
      "Epoch 15 | Loss = 250.87509811625762\n",
      "Epoch 16 | Loss = 240.66060464522417\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "for model_name in model_ls:\n",
    "    for augmentation in aug_ls:\n",
    "        for batch_size in batch_size_ls:\n",
    "            for enc_layer in enc_layers:\n",
    "                for hidden_dim in hid_dim_ls:\n",
    "                    if idx<0:\n",
    "                        pass\n",
    "                    else:\n",
    "                        print(\"+-----------------------------------------------------------------------------+\")\n",
    "                        print(\"Experient\", idx)\n",
    "                        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                        acc, val_acc, cost_time, emb_time = main_train(idx, train_idx, augmentation, batch_size, hidden_dim, enc_layer, proj_layer, model_name)\n",
    "                        print(\"Experient\", idx)\n",
    "\n",
    "                        now = datetime.now()\n",
    "                        dt = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "                        with open(log_path+\"log_\"+dataset_name+\".txt\", mode=\"a\") as f:\n",
    "\n",
    "                            f.write(\"{},{},{},{},{},{},{},{},{},{},{},{}\".format(idx, data_prop, model_name, augmentation, batch_size, enc_layer, hidden_dim, acc, val_acc, cost_time, dt, emb_time))\n",
    "                            f.write(\"\\n\")\n",
    "                            print(dt, \":Log the record!\")\n",
    "\n",
    "                        if idx%1==0:\n",
    "    #                         tz = timezone(timedelta(hours=+8))\n",
    "    #                         now = datetime.now(tz)\n",
    "    #                         dt = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "                            msg = \"[Sinica 0] \"+ dataset_name + \" | Exp \" + str(idx) +\" | \" \n",
    "                            lineNotifyMessage(token, msg)\n",
    "\n",
    "                    idx = idx + 1                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "print(\"+-----------------------------------------------------------------------------+\")                    \n",
    "print(\"+-----------------------------------------------------------------------------+\")\n",
    "print(\"DONE!!!\")\n",
    "print(dt)\n",
    "print(\"+-----------------------------------------------------------------------------+\")\n",
    "print(\"+-----------------------------------------------------------------------------+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3QBNAq8hkmR_"
   },
   "source": [
    "## Plot and Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "3QBNAq8hkmR_"
   ],
   "machine_shape": "hm",
   "name": "[v5]SSL on Graph Classification",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
