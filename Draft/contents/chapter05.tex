% !TeX root = ../main.tex

\section{Conclusion}\label{sec:5}

We have systematically investigated three self-supervised learning methods by employing them to train DNNs on graph data. We have also discussed the simulation experiments in detail, providing explanations for the experiment results so that subsequent researchers may use them to propose better methods and ideas.

First, increasing the batch size in the training stage when the dataset is medium or small size is not helpful, but deepening the layer of the encoder seems to be a favorable means to improve model performance. Second, graphs have their special structure. Applying previous procedures used to improve the performance of models trained on image datasets, e.g., increasing the hidden dimension size, to every similar scenario may not be inappropriate. Finally, because of the characteristic differences between molecular and bioinformatics data, different optimal hyperparameters should be used when training models on these two data types.

At present, we only focus on small-scale datasets containing less than ten thousand samples. However, real-world data generated by commercial systems may have millions of nodes or 10 billion edges, which are orders of magnitude larger than the datasets used in this thesis. Owing to resource constraints and computational limitations, we cannot provide a comprehensive review of larger graph datasets. 


There is abundant space for advancement in large-scale datasets \cite{choi2022triangular} and other categories, such as social networks or recommendation systems \cite{mahcl}. Furthermore, future research should explore new data augmentation methods \cite{yang2022contrastive, shen2022improving}, as well as other key factors in or approaches \cite{hou2022graphmae} to self-supervised learning, to find a robust and efficient method for improving model performance.


